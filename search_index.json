[
["index.html", "Tutorium zur Inferenzstatistik Vorbemerkungen Session Info", " Tutorium zur Inferenzstatistik Maik Thalmann maik.thalmann@gmail.com Wintersemester 2019/20 Version: 18. April, 2020 Vorbemerkungen Dieses Buch ist zeitlich nach den Folien entstanden, die ich zum Unterrichten meiner Veranstaltung verwenden werde. Um gewährleisten zu können, dass der Inhalt der Folien und dieser Onlineresource deckungsgleich bleibt, werden beide auf Basis derselben R Markdown-Dateien erstellt – genauer im darauf basierenden bookdown-Format. Das hat einige Konsequenzen, die ästhetisch wenig zufriedenstellend sind, aber von mir billigend in Kauf genommen werden (müssen). Ein offensichtlicher dieser Punkte betrifft die Abschnittsgliederung: Da Abschnitte im Präsentationsformat die Foliengrenzen markieren, ist dieses Buch leider vielerorts etwas unglücklich gegliedert (insbesondere in den Übungssektionen). Bis ich eine Lösung gefunden habe, die mir weiterhin erlaubt, in nur einem Dokument/Format zu arbeiten und mehrere Dateien generiert zu bekommen, muss das allerdings so bleiben. Obwohl diese Resource eine Einfürung in R ist, muss ich an dieser Stelle ein paar Voreinstellungen für die nachfolgenden Kapitel (insbesondere zu ladende Pakete und Schriftarteinstellungen für Plots) vornehmen. Anfänger brauchen sich mit den folgenden Codezeilen nicht zu beschäftigen; ich wollte lediglich transparent vorgehen und offenlegen, welche externen Pakete ich verwende. # load packages Packages &lt;- c(&quot;tidyverse&quot;, &quot;gridExtra&quot;, &quot;reshape2&quot;, &quot;psych&quot;, &quot;stringr&quot;, &quot;afex&quot;, &quot;ez&quot;, &quot;car&quot;, &quot;emmeans&quot;, # not really necessary &quot;ggthemes&quot;, &quot;ggpubr&quot;, &quot;scico&quot;, &quot;janitor&quot;, &quot;magick&quot;, &quot;gganimate&quot;, &quot;hrbrthemes&quot;) xfun::pkg_attach(Packages, install = TRUE) # base size for plots made with ggplot2 theme_set(theme_grey(base_size = 11)) # disable scientific number notation options(scipen = 999) Session Info ## R version 3.6.1 (2019-07-05) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS Catalina 10.15.2 ## ## Locale: en_US.UTF-8 / en_US.UTF-8 / en_US.UTF-8 / C / en_US.UTF-8 / en_US.UTF-8 ## ## Package version: ## hrbrthemes_0.7.2 plotly_4.9.1 MASS_7.3-51.4 ## broom_0.5.2 ggsci_2.9 qqplotr_0.0.3 ## gganimate_1.0.4 magick_2.2 janitor_1.2.0 ## scico_1.1.0 ggpubr_0.2.4 magrittr_1.5 ## ggthemes_4.2.0 car_3.0-5 carData_3.0-3 ## ez_4.4-0 afex_0.25-1 lme4_1.1-21 ## Matrix_1.2-18 psych_1.8.12 reshape2_1.4.3 ## gridExtra_2.3 forcats_0.4.0 stringr_1.4.0 ## dplyr_0.8.3 purrr_0.3.3 readr_1.3.1 ## tidyr_1.0.0 tibble_2.1.3 ggplot2_3.2.1 ## tidyverse_1.3.0 minqa_1.2.4 colorspace_1.4-1 ## ggsignif_0.6.0 rio_0.5.16 estimability_1.3 ## fs_1.3.1 rstudioapi_0.10 farver_2.0.1 ## mvtnorm_1.0-11 lubridate_1.7.4 xml2_1.2.2 ## codetools_0.2-16 splines_3.6.1 extrafont_0.17 ## mnormt_1.5-5 robustbase_0.93-5 knitr_1.26 ## zeallot_0.1.0 jsonlite_1.6 nloptr_1.2.1 ## Rttf2pt1_1.3.7 dbplyr_1.4.2 png_0.1-7 ## compiler_3.6.1 httr_1.4.1 emmeans_1.4.3.01 ## backports_1.1.5 assertthat_0.2.1 lazyeval_0.2.2 ## cli_1.1.0 tweenr_1.0.1 htmltools_0.4.0 ## prettyunits_1.0.2 tools_3.6.1 lmerTest_3.1-0 ## coda_0.19-3 gtable_0.3.0 glue_1.3.1 ## Rcpp_1.0.3 styler_1.2.0 cellranger_1.1.0 ## vctrs_0.2.0 nlme_3.1-142 extrafontdb_1.0 ## xfun_0.11 openxlsx_4.1.3 rvest_0.3.5 ## lifecycle_0.1.0 DEoptimR_1.0-8 scales_1.1.0 ## hms_0.5.2 parallel_3.6.1 rematch2_2.1.0 ## yaml_2.2.0 curl_4.3 gdtools_0.2.1 ## stringi_1.4.3 gifski_0.8.6 boot_1.3-23 ## zip_2.0.4 systemfonts_0.1.1 rlang_0.4.2 ## pkgconfig_2.0.3 evaluate_0.14 lattice_0.20-38 ## htmlwidgets_1.5.1 tidyselect_0.2.5 plyr_1.8.4 ## bookdown_0.16 R6_2.4.1 generics_0.0.2 ## DBI_1.0.0 pillar_1.4.2 haven_2.2.0 ## foreign_0.8-72 withr_2.1.2 mgcv_1.8-31 ## abind_1.4-5 modelr_0.1.5 crayon_1.3.4 ## rmarkdown_1.18 progress_1.2.2 grid_3.6.1 ## readxl_1.3.1 data.table_1.12.6 reprex_0.3.0 ## digest_0.6.23 xtable_1.8-4 numDeriv_2016.8-1.1 ## munsell_0.5.0 viridisLite_0.3.0 "],
["sitzung-1.html", "Sitzung 1 Allgemeines Was kann man mit R alles anstellen? RStudio-Optionen", " Sitzung 1 Allgemeines Ein paar Worte zum Tutorium anfangs kaum Statistik, sondern eine Einführung in R ich lade meine Foliensätze, die R-Scripte und die externen Dateien, die wir verwenden, bei StudIP hoch damit ihr was vom Tutorium habt, wäre es super, wenn ihr zu jeder Sitzung einen Computer mitbringen würdet (oder euch zusammenschließt und zu zweit einen Computer benutzt) wenn ihr keinen Computer zur Verfügung habt, gibt es auch Apps für R, die sind aber kein guter Ersatz; vor allem, wenn wir in der dritten Sitzung anfangen, mit richtigen Daten zu arbeiten IOS: https://apps.apple.com/de/app/r-programming-compiler/id1158038782 (nicht getestet) Android ??? Wie ihr R auf euren Computer bekommt, zeige ich euch gleich R: Ein kurzer Überblick 1992 entwickelt, angelehnt an die Programmiersprache S Fokus liegt auf statistischen Berechnungen und Grafiken aller Art weit verbreitet, Anwendung vor allem in der Wissenschaft fortlaufende Weiterentwicklung durch die Nutzergemeinschaft (v.a. in Form von Paketen) kostenlos verwendbar Ein Beispielplot Installation Am wichtigsten: R selbst https://cran.r-project.org Wir werden zwar hauptsächlich mit R arbeiten, aber da die Oberfläche wenig benutzerfreundlich ist, verwende ich außerdem Rstudio https://www.rstudio.com Bitte alle beides installieren und zum Laufen bringen! Tada! Und so sieht das Ganze dann ungefähr aus Hilfestellung https://r4ds.had.co.nz / https://www.degruyter.com/view/product/203826 Mehr Hilfe Mehr kostenlose Bücher und Resourcen: https://committedtotape.shinyapps.io/freeR/ Allgemeines Wir werden hauptsächlich mit Zahlen arbeiten, darum ist das Folgende wichtig: - R verwendet den amerikanischen Dezimaltrenner (0.1666667 statt 0,1666667) 1 / 6 ## [1] 0.1666667 Um Hilfe mit Funktionen zu bekommen: - ?Funktionsname - help(Funktionsname) Gebt mal Folgendes ein: ?mean help(mean) Was kann man mit R alles anstellen? Skripte R ist eine Programmiersprache, die, wie alle anderen auch, textbasiert ist. D.h. man arbeitet in der Regel mit Skripten, in denen man seinen Code schreibt und diesen dann ausführt. Hier ein Beispiel für ein (zugegeben sehr einfaches) Skript: Und so sieht das Ganze dann ungefähr aus Skripte Zwar könntet ihr auch einfach in der Konsole in Rstudio euren Code schreiben, aber dann geht er euch ziemlich schnell verloren und ihr könnt nicht einfach dort wieder anfangen, wo ihr aufgehört habt. Übung 1 erstellt einen Order auf dem Desktop erstellt ein Skript in RStudio und speichert es in diesem Ordner macht eure Noizen am besten in diesem R-Skript Fließtext als Kommentare nach dem #-Zeichen # dies ist ein Kommentar, den R einfach ignoriert print(&quot;Hello World&quot;) # ebenso mit dem hier ## [1] &quot;Hello World&quot; R als Taschenrechner 1 + 2 ## [1] 3 17 - 9 ## [1] 8 5 * 6 ## [1] 30 9 / 3 ## [1] 3 4^3 ## [1] 64 Mathematische Funktionen in R Quadratwurzel: sqrt() Summe: sum() Miminum: min() Maximum: max() Zahlenreihen von x bis y: x:y 1:5 ## [1] 1 2 3 4 5 Übung II addiert 500 und 5 subtrahiert 19 von 3 dividiert 144 durch 12 addiert alle Zahlen von 1 bis 1000 addiert 4 und 5 fünf mal hintereinander Lösungen II 500 + 5 ## [1] 505 19 - 3 ## [1] 16 144 / 12 ## [1] 12 sum(1:1000) ## [1] 500500 4 + 5;4 + 5;4 + 5;4 + 5;4 + 5 ## [1] 9 ## [1] 9 ## [1] 9 ## [1] 9 ## [1] 9 Logische Operatoren Logische Operatoren geben Wahrheitswerte (wie in der Semantik) wieder: entweder TRUE (oder T) oder FALSE (oder F) Vergleichsoperatoren Gleich: == Ungleich: != Kleiner: &lt; Größer: &gt; Kleiner gleich: &lt;= Größer gleich: &gt;= Logische Operatoren Und/Konjuntion: &amp; Oder/Disjunktion: | Negation: ! Beispiele: Logische Operatoren 4 == 4 ## [1] TRUE 4 != 5 ## [1] TRUE 17 / 9 &gt;= 2 + 1 ## [1] FALSE TRUE &amp; FALSE == FALSE ## [1] TRUE TRUE | FALSE == TRUE ## [1] TRUE TRUE | FALSE == FALSE ## [1] TRUE R ist objektorientiert In R ist es oftmals wichtig, Berechnungen für die spätere Wiederverwertung zu speichern das geschieht, indem man ihnen einen diese Objekte erscheinen dann in der Arbeitsumgebung (eng. Enviromnent) a &lt;- 9 b &lt;- 81 Arbeitsumgebung Beispiele a &lt;- 4 + 5 a ## [1] 9 a * 2 ## [1] 18 b &lt;- a^2 b ## [1] 81 (c &lt;- a + b) ## [1] 90 Mehr zu Objekten Wir können uns Objekte (oft auch Varablen genannt) als Mikrospeicherorte in unserem Makrospeicher (dem Skript selbst) vorstellen. Aus diesem Grund ist es wichtig, dass ihr Berechnungen und anderes, das ihr wiederverwenden wollt, als Objekt (mit sinnigem Namen) speichert meinname &lt;- &quot;Maik Thalmann&quot; meineuni &lt;- &quot;Uni Göttingen&quot; paste(meinname, &quot;studiert an der&quot;, meineuni) ## [1] &quot;Maik Thalmann studiert an der Uni Göttingen&quot; Wenn ihr euch die Arbeitsumgebung anzeigen lassen möchtet, verwendet ls(), löschen könnt ihr sie mit rm(list = ls()) ls() ## [1] &quot;a&quot; &quot;b&quot; &quot;c&quot; &quot;meineuni&quot; &quot;meinname&quot; Datentypen in R Ihr kennt bisher drei Datentypen: numeric: Zahlen integer double character: Zeichenketten logical: Wahrheitswerte mode(a) ## [1] &quot;numeric&quot; typeof(a) ## [1] &quot;double&quot; typeof(FALSE) ## [1] &quot;logical&quot; typeof(meinname) ## [1] &quot;character&quot; RStudio-Optionen Optionen, die das Leben erleichtern Bitte stellt folgende Optionen in RStudio ein: Tools \\(\\rightarrow\\) Global Options \\(\\rightarrow\\) Code \\(\\rightarrow\\) Editing Soft-Wrap Source Files Saving Default Text Encoding: UTF-8 Display Highlight Selected Line Show syntax highlighting in console input Das war’s! Danke fürs mitmachen und bis nächste Woche! "],
["sitzung-2.html", "Sitzung 2 Wiederholung Vektoren Zwei-Dimensionale Objekte", " Sitzung 2 Wiederholung Das Wichtigste vom letzten Mal R ist eine Programmiersprache für Statistik kostenlos nicht immer benutzerfreundlich, deshalb RStudio es gibt einiges zu beachten: Ihr solltet immer mit einem Skipt arbeiten, nur in Ausnahmefällen in der Konsole Hilfe bekommt ihr durch ?Funktion oder help(Funktion) Wenn ihr Berechnungen macht und Daten aufzeichnet, bzw. einlest, und auf diese später wieder zugreifen möchtet, solltet ihr sie Objekten zuweisen die Verschiedenen Datentypen: numeric: Zahlen charachter: Buchstabenfolgen in Anführungszeichen logical: Wahrheitswerte Code vom letzten Mal a &lt;- 1:17 b &lt;- sum(a) b ## [1] 153 TRUE != FALSE ## [1] TRUE (b &amp; 3) &gt;= 1 / 9 ## [1] TRUE b &lt;- &quot;überschreiben&quot; b == 153 ## [1] FALSE typeof(b) ## [1] &quot;character&quot; Vektoren Moment mal Gerade haben wir folgenden Code gesehen: a &lt;- 1:17 Hatten wir beim letzten Mal nicht gesagt, dass x:y in R “von x bis y” bedeutet? Schaut mal, was in der Berechnung unten passiert. 1:3 + 10 ## [1] 11 12 13 a &lt;- 1:3 + 10 a ## [1] 11 12 13 Was passiert hier? Genau, wir können nicht nur einzelne Werte einem Objekt zuordnen, sondern auch eine Sequenz aus mehreren Werten unter einem Objektnamen zusammenfassen. Vektoren Wir haben nun gesehen, dass wir Elemente kombinieren können – diese komplexen Elemente nennt man Vektoren und sie funktionieren in etwa so geordnete Tupel in der Mengentheorie. Zahlenreihen können wir mit dem Doppelpunktoperator verbinden, bei anderen Datentypen verwenden wir c() (für “concatenate” – verketten) a &lt;- 7:14 b &lt;- c(7, 8, 9, 10, 11, 12, 13, 14) a == b ## [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE c &lt;- c(&quot;Als&quot;, &quot;Gregor&quot;, &quot;Samsa&quot;, &quot;eines&quot;, &quot;Morgens&quot;) Subsetting Wenn wir die Vekoten erstellt haben, können einzelne Elemente können per Subsetting herausgepickt werden. Dazu werden eckige Klammern mit positionalen Idizes verwendet … c[5] # zeigt das fünfte Element an ## [1] &quot;Morgens&quot; (subset &lt;- c[2:3]) # Elemente mit den Indizes von 2 bis 3 ## [1] &quot;Gregor&quot; &quot;Samsa&quot; … oder Bedigungen für das Enthaltensein in der Teilmenge: b[b &gt;= 10] ## [1] 10 11 12 13 14 Dimensionen von Vektoren Bei solch Komplexen Objekten interessiert uns oft die Anzahl der enthaltenen Elemente. Diese können wir uns mithilfe von length() ausgeben lassen: a ## [1] 7 8 9 10 11 12 13 14 length(a) ## [1] 8 length(c) ## [1] 5 length(c(a, c)) ## [1] 13 Nochmal Subsetting Wenn wir Subsetting mit der length-Funktion verbinden, erhalten wir das letzte Element des Vektors a[length(a)] ## [1] 14 a[length(a) - 1] # vorletztes Element ## [1] 13 Übungen addiere zu jeder zahl von 1 bis 700 die zahl 13 und speichere das ergebnis in einem vektor ab. speichere das 345. element dieses vektors in einem neuen vektor und füge zum diesem vektor außerdem das 666. Element des ersten Vektors hinzu lass dir die letzten 22 elemente des langen vektors ausgeben lass dir dasjenige element anzeigen, das die mitte des langen vektors bildet addiere alle elemente im langen vektor auf Lösungen addiere zu jeder zahl von 1 bis 1000 die zahl 13 und speichere das ergebnis in einem vektor ab. x &lt;- 1:700 y &lt;- x + 13 y ## [1] 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 ## [19] 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 ## [37] 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 ## [55] 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 ## [73] 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 ## [91] 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 ## [109] 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 ## [127] 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 ## [145] 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 ## [163] 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 ## [181] 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 ## [199] 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 ## [217] 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 ## [235] 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 ## [253] 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 ## [271] 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 ## [289] 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 ## [307] 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 ## [325] 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 ## [343] 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 ## [361] 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 ## [379] 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 ## [397] 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 ## [415] 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 ## [433] 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 ## [451] 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 ## [469] 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 ## [487] 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 ## [505] 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 ## [523] 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 ## [541] 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 ## [559] 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 ## [577] 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 ## [595] 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 ## [613] 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 ## [631] 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 ## [649] 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 ## [667] 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 ## [685] 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 Lösungen speichere das 345. element dieses vektors in einem neuen vektor und füge zum diesem vektor außerdem das 666. Element des ersten Vektors hinzu z &lt;- y[345] z &lt;- c(z, y[666]) z ## [1] 358 679 lass dir die letzten 22 elemente des langen vektors ausgeben y[(length(y) - 21):length(y)] ## [1] 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 ## [20] 711 712 713 y[length(y):(length(y) - 21)] ## [1] 713 712 711 710 709 708 707 706 705 704 703 702 701 700 699 698 697 696 695 ## [20] 694 693 692 Lösungen lass dir dasjenige element anzeigen, das die mitte des langen vektors bildet y[length(y) / 2] ## [1] 363 addiere alle elemente im langen vektor auf sum(y) ## [1] 254450 Heads und Tails Oft ist es praktisch, sich die ersten und letzten Elemente eines Objektes anzeigen zu lassen. Wir haben verschiedene Möglichkeiten gesehen, wie man das erreichen kann. Glücklicherweise hat R gesonderte Funktionen für genau solche Operationen und erspart uns somit Tipparbeit y[1:6] ## [1] 14 15 16 17 18 19 head(y) ## [1] 14 15 16 17 18 19 y[(length(y) - 5):length(y)] ## [1] 708 709 710 711 712 713 tail(y) ## [1] 708 709 710 711 712 713 Listen Manchmal ist es praktisch, wenn die Elemente eines Vekors selbst komplex sind und somit Elemente einer bestimmten Art enthalten. Dazu verwendet man list kind &lt;- c(&quot;Fara&quot;, &quot;Lula&quot;, &quot;Fu&quot;) note &lt;- c(2, 1, 5) (klausur &lt;- list(kind, note)) ## [[1]] ## [1] &quot;Fara&quot; &quot;Lula&quot; &quot;Fu&quot; ## ## [[2]] ## [1] 2 1 5 Subsetting von Listen Bei Objekten mit komplexen Elementen funktioniert das Subsetting etwas anders: klausur[2] # gibt eine Liste aus ## [[1]] ## [1] 2 1 5 klausur[[2]] # gibt einen Vektor aus ## [1] 2 1 5 klausur[[1]][3] # gibt das dritte Element des ersten Vektors aus ## [1] &quot;Fu&quot; klausur[1][3] ## [[1]] ## NULL Zwei-Dimensionale Objekte Über Vekoten hinaus Wir haben gesehen, dass eindimensionale Objekte erstellen können, die sich Vektoren (oder Listen) nennen. Was ist nun mit Objekten, die sowohl eine Länge wie auch eine Breite haben (read: Tabellen)? kind ## [1] &quot;Fara&quot; &quot;Lula&quot; &quot;Fu&quot; note ## [1] 2 1 5 (klausur &lt;- cbind(kind, note)) ## kind note ## [1,] &quot;Fara&quot; &quot;2&quot; ## [2,] &quot;Lula&quot; &quot;1&quot; ## [3,] &quot;Fu&quot; &quot;5&quot; is.matrix(klausur) ## [1] TRUE Die Dimensionen von Tabellen Die length-Funktion ist sehr irreführend bei Matritzen. Es ist besser, die dim-Funktion zu verwenden. length(klausur) ## [1] 6 dim(klausur) ## [1] 3 2 klausur[1, ] # erste Zeile ## kind note ## &quot;Fara&quot; &quot;2&quot; klausur[, 1] # erste Spalte ## [1] &quot;Fara&quot; &quot;Lula&quot; &quot;Fu&quot; klausur[3, 2] # dritte Zeile, zweites Element ## note ## &quot;5&quot; Subsetting von Matritzen klausur[1, ] # erste Zeile ## kind note ## &quot;Fara&quot; &quot;2&quot; klausur[, 1] # erste Spalte ## [1] &quot;Fara&quot; &quot;Lula&quot; &quot;Fu&quot; klausur[3, 2] # dritte Zeile, zweites Element ## note ## &quot;5&quot; Datenblätter Zwar sind Matritzen geeignet, um Daten in Tabellenform zu repräsentieren. Eine wesentlich komfortablere Möglichkeit (und die, die wir im Verlaufe des Semesters fast ausschließlich verwenden werden) bieten data frames. (klausur &lt;- data.frame(kind, note)) ## kind note ## 1 Fara 2 ## 2 Lula 1 ## 3 Fu 5 is.data.frame(klausur) ## [1] TRUE dim(klausur) ## [1] 3 2 length(klausur) # gibt Spaltenanzahl aus ## [1] 2 Subsetting von Datenblättern klausur$note ## [1] 2 1 5 klausur[1, ] ## kind note ## 1 Fara 2 klausur[, 2] ## [1] 2 1 5 (klausur2 &lt;- subset(klausur, note &gt; 3)) ## kind note ## 3 Fu 5 Datenblätter erweitern geschlecht &lt;- c(&quot;w&quot;, &quot;w&quot;, &quot;m&quot;) note2 &lt;- c(4, 3, 2) (klausur &lt;- cbind(klausur, note2, geschlecht)) ## kind note note2 geschlecht ## 1 Fara 2 4 w ## 2 Lula 1 3 w ## 3 Fu 5 2 m dim(klausur) ## [1] 3 4 klausur$schnitt &lt;- (klausur$note + klausur$note2) / 2 klausur ## kind note note2 geschlecht schnitt ## 1 Fara 2 4 w 3.0 ## 2 Lula 1 3 w 2.0 ## 3 Fu 5 2 m 3.5 Übungen II Fragt eure Sitznachbarn (links und rechts) nach Vorname Lieblingsessen Geburtsmonat und erstellt für alle drei Faktoren Vektoren mit den jeweiligen Infos für Nachbar_in 1 Nachbar_in 2 und euch selbst Erstellt aus diesen Vektoren einen data frame Lasst euch das zweite Element der dritten Zeile ausgeben Lasst euch die Zeile mit den Informationen über die Person links von euch anzeigen Speichert die Spalte mit dem Lieblingsessen in einem neuen Vektor “mealprep” ab Lösungen II Erstellt aus diesen Vektoren einen data frame vorname &lt;- c(&quot;Marta&quot;, &quot;Thomas&quot;, &quot;Maik&quot;) essen &lt;- c(&quot;Schnitzel&quot;, &quot;Auflauf&quot;, &quot;Bier&quot;) monat &lt;- c(6, 8, 12) (people &lt;- data.frame(vorname, essen, monat)) ## vorname essen monat ## 1 Marta Schnitzel 6 ## 2 Thomas Auflauf 8 ## 3 Maik Bier 12 Lasst euch das zweite Element der dritten Zeile ausgeben people[3, 2] ## [1] Bier ## Levels: Auflauf Bier Schnitzel Lösungen II Lasst euch die Zeile mit den Informationen über die Person links von euch anzeigen people[1, ] ## vorname essen monat ## 1 Marta Schnitzel 6 Speichert die Spalte mit dem Lieblingsessen in einem neuen Vektor “mealprep” ab (mealprep &lt;- people$essen) ## [1] Schnitzel Auflauf Bier ## Levels: Auflauf Bier Schnitzel Tüdelü Bis zum nächsten Mal! "],
["sitzung-3.html", "Sitzung 3 Wiederholung Mehr zu Datenblättern Daten exportieren Daten einlesen", " Sitzung 3 Wiederholung Das Wichtigste vom letzten Mal Vektoren sind eindimensionale Tupel von Elementen # vektoren erstellen a &lt;- 1:19 b &lt;- a + 19 c &lt;- c(&quot;svo&quot;, &quot;sov&quot;, &quot;ovs&quot;) # vektoren ausgeben lassen c ## [1] &quot;svo&quot; &quot;sov&quot; &quot;ovs&quot; length(b) ## [1] 19 Das Wichtigste vom letzten Mal Dataframes sind Tabellen (und damit 2-dimensional) df &lt;- data.frame(a, b) dim(df) ## [1] 19 2 head(df, 1) # eine statt sechs zeilen als optionales argument ## a b ## 1 1 20 tail(df, 2) ## a b ## 18 18 37 ## 19 19 38 df$c &lt;- a + b head(df, 1) ## a b c ## 1 1 20 21 Das Wichtigste vom letzten Mal head(df[, 1]) ## [1] 1 2 3 4 5 6 df[df$a == 19, ] ## a b c ## 19 19 38 57 df[df$a + df$b &gt; 50, ] ## a b c ## 16 16 35 51 ## 17 17 36 53 ## 18 18 37 55 ## 19 19 38 57 subset(df, b &gt; 35) # alle Reihen, auf die die Bedingung zutrifft ## a b c ## 17 17 36 53 ## 18 18 37 55 ## 19 19 38 57 Mehr zu Datenblättern Spalten umbenennen names(df) &lt;- c(&quot;low&quot;, &quot;medium&quot;, &quot;high&quot;) head(df) ## low medium high ## 1 1 20 21 ## 2 2 21 23 ## 3 3 22 25 ## 4 4 23 27 ## 5 5 24 29 ## 6 6 25 31 Mehr Subsetting logische Bedingungen (beachtet hier das Komma! Wir arbeiten jetzt mit 2-dimensionalen Daten und müssen mit angeben, ob wir uns auf Zeilen oder Spalten beziehen) df[df$low &gt;= 17, ] ## low medium high ## 17 17 36 53 ## 18 18 37 55 ## 19 19 38 57 df[df$low + df$medium &gt; 53, ] ## low medium high ## 18 18 37 55 ## 19 19 38 57 df[df$low %% 2 == 0 &amp; df$high &gt; 38, ] # modulo division -&gt; rest = 0 ## low medium high ## 10 10 29 39 ## 12 12 31 43 ## 14 14 33 47 ## 16 16 35 51 ## 18 18 37 55 Noch mehr Subsetting Mengen: %in% fungiert wie \\(\\in\\) aus der Mengentheorie df[df$low %in% c(5, 6, 7), ] ## low medium high ## 5 5 24 29 ## 6 6 25 31 ## 7 7 26 33 df[df$low %in% c(5, 6, 7) | df$high %in% c(23, 25), ] ## low medium high ## 2 2 21 23 ## 3 3 22 25 ## 5 5 24 29 ## 6 6 25 31 ## 7 7 26 33 Daten exportieren Workspace Wenn wir mir externen Daten (also solchen, die wir nicht in R generiert und/oder nicht nur in R verwenden wollen) arbeiten, müssen wir uns für einen Speicherort entscheiden. Ganz allgemein wird R ein Standardverezeichnis festlegen. Schaut mal eures nach. getwd() ## [1] &quot;/Users/Maik/Library/Mobile Documents/com~apple~CloudDocs/Studium/Arbeit am SDP/Tut Inferenzstatistik&quot; Workspace verändern Wenn ihr das Arbeitsverzeichnis verändern wollt, könnt ihr dies mit setwd() machen. Wie genau ihr die Pfade festlegt, könnt ihr entweder im Internet nachschauen (das variiert ziemlich zwischen Windows und Mac) oder per Rstudio über den Tab “Files” RStudio zum Workspace festlegen Workspace verändern Selbst wenn ihr RStudio verwendet, um das Arbeitsverzeichnis zu verändern, solltet ihr die Codezeile aus der Konsole in eurer Skript kopieren, damit ihr das beim nächsten Mal nicht nochmal erledigen müsst, sondern einfach den Code ausführen könnt. setwd(&quot;~/Desktop&quot;) getwd() ## [1] &quot;/Users/Maik/Desktop&quot; write.csv/write.table write.csv(ObjektNameInR, &quot;PFAD&quot;, WeitereParameter) weitere Parameter Zeilennummerierung übernehmen: row.names=TRUE/FALSE Die Zeilennummerierung wird als erste Spalte übernommen. Die Spaltenbezeichnungen werden dementsprechend nach links verschoben, sodass die col.names manuell angepasst werden müssen. Dezimalstellentrennungszeichen bestimmen: dec=„Trennungszeichen“ Separatoren definieren: sep = „Separator“ Strings in Anführungszeichen einschließen: quote = TRUE/FALSE Übersicht # Zeilennummerierung von R wird nicht in die neue Tabelle übernommen write.table(d, &quot;tab.csv&quot;, row.names = F) # Erstellt eine Tabelle, die Tabstops als Zellentrenner benutzt write.table(d, &quot;tab.csv&quot;, sep = &quot;\\t&quot;) # Erstellt eine Tabelle mit Kommas als Dezimalstellentrenner write.table(d, &quot;tab.csv&quot;, dec = &quot;,&quot;) # Erstellt eine Tabelle mit Semikolons als Separatoren write.table(d, &quot;tab.csv&quot;, sep = &quot;;&quot;) # NAs werden als &quot;Nicht vorhanden&quot; bezeichnet write.table(d, &quot;tab.csv&quot;, na = &quot;Nicht vorhanden&quot;) Datenblatt speichern Als nächstes können wir unseren Dataframe im Arbeitsverzeichnis unter dem angegebenen Namen abspeichern. Falls ihr andere Dateiformate verwenden wollt, schaut euch mal write.table an. head(df) ## low medium high ## 1 1 20 21 ## 2 2 21 23 ## 3 3 22 25 ## 4 4 23 27 ## 5 5 24 29 ## 6 6 25 31 write.csv(df, &quot;df_test.csv&quot;, row.names = F, quote = F) Daten einlesen Interlude: Dateiformate Dateiformate Jede Datei wird in einem Dateiformat abgespeichert Endungen bestimmen die Art (z.B. ein Programm), mit welcher der Computer eine Datei öffnet Gägige Dateiendungen für unsere Zwecke -Comma-separated values: .csv Tab-separated values: .tsv Text-Datei: .txt Unbestimmte Datei: .dat Excel: xls R kann xls nur mit bestimmten Paketen lesen read.table() Voraussetzung: einzulesende Datei befindet sich im Arbeitsverzeichnis bzw. in einem im Arbeitsverzeichnis liegenden Unterordner wichtige optionale Argumente von read.table/read.csv header = TRUE/FALSE sep = “Separator für Einträge” oft verwendet: Komma, Semikolon, Leerzeichen oder Tabstopps dec = “Dezimalzahltrennungszeichen” beachte: R verwendet den Punkt und nicht das Komma als Dezimaltrenner na.strings = “Zeichenkette, die als NA definiert werden soll” (oder Vektor mit mehreren Strings) skip = Zeile, bis zu der übersprungen werden soll blank.lines.skip = TRUE comment.char = “Kommentarzeichen” Ein Beispiel Ich habe hier ein einfaches Beispiel, bei dem man keine weiteren Parameter angeben muss, damit ihr sehen könnt, wie es funktioniert. Bei Gelingen sieht das dann so aus: d &lt;- read.csv(&quot;docs/data/tut3/example.csv&quot;) head(d) ## id item judgment tester years months ## 1 3536fed0235c7b34a33ddf06fb91b390 schaffen4_at 5 mailin 5 1 ## 2 3536fed0235c7b34a33ddf06fb91b390 schaffen1_non 5 mailin 5 1 ## 3 3536fed0235c7b34a33ddf06fb91b390 cleft2_non 5 mailin 5 1 ## 4 3536fed0235c7b34a33ddf06fb91b390 appRel3_non 5 mailin 5 1 ## 5 3536fed0235c7b34a33ddf06fb91b390 appRel9_non 4 mailin 5 1 ## 6 3536fed0235c7b34a33ddf06fb91b390 gewinnen2_non 4 mailin 5 1 ## gender issue trigger itemid lex ## 1 w at-issue soft schaffen4 schaffen ## 2 w non-at-issue soft schaffen1 schaffen ## 3 w non-at-issue hard cleft2 cleft ## 4 w non-at-issue appositive RC appRel3 appRel ## 5 w non-at-issue appositive RC appRel9 appRel ## 6 w non-at-issue soft gewinnen2 gewinnen Vorsicht Beachtet man die genannten Parameter nicht, bekommt man schnell unbrauchbaren Datensalat: head(read.csv(&quot;docs/data/tut3/raw.txt&quot;)) ## blablablablabla.wejfuwejfnqj.qfnwejfn ## 1 1\\t1\\ta\\tdass der Peter den Hans mag. ## 2 1\\t1\\tb\\tdass den Hans der Peter mag. ## 3 1\\t1\\tc\\tdass der Peter sicherlich den Hans mag. ## 4 1\\t1\\td\\tdass den Hans sicherlich der Peter mag. ## 5 1\\t2\\ta\\tdass die Frau den Bäcker liebt. ## 6 1\\t2\\tb\\tdass den Bäcker die Frau liebt. Übungen Lese die folgenden Dateien in R ein (Objektzuweisung nicht vergessen!): alldata.dat Beispiel.csv Tutorium_dat3.txt NHIS_2007_data4.csv lungen.txt Tutorium_4.dat raw.txt raw2.txt master.txt Lösungen # &quot;alldata.dat&quot; d1 &lt;- read.table(&quot;docs/data/tut3/alldata.dat&quot;, header = T) head(d1) ## id geo subject variant experiment item condition judgement ## 1 m-1987-EMusik bla 1 1 9 4 a 6 ## 2 m-1987-EMusik bla 1 1 8 2 b 7 ## 3 m-1987-EMusik bla 1 1 7 10 b 7 ## 4 m-1987-EMusik bla 1 1 9 3 d 1 ## 5 m-1987-EMusik bla 1 1 1 8 d NA ## 6 m-1987-EMusik bla 1 1 8 9 a 6 # &quot;Beispiel.csv&quot; d2 &lt;- read.csv(&quot;docs/data/tut3/Beispiel.csv&quot;, sep = &quot;;&quot;, header = T) head(d2) ## subject item condition judgement F1 F2 ## 1 1 10 b 3 0 1 ## 2 1 4 d 4 1 1 ## 3 1 15 c 7 1 0 ## 4 1 9 a 2 0 0 ## 5 1 16 d 1 1 1 ## 6 1 13 a 7 0 0 Lösungen # &quot;Tutorium_dat3.txt&quot; d3 &lt;- read.table(&quot;docs/data/tut3/Tutorium_dat3.txt&quot;, header = T, sep = &quot;\\t&quot;) head(d3) ## ProbandIn Geschlecht Treatment Zeit1 Zeit2 Zeit3 ## 1 1 f 1 19.7 19.7 20.5 ## 2 2 f 2 14.2 16.9 17.3 ## 3 3 f 3 11.2 14.2 17.2 ## 4 4 f 1 18.8 20.3 28.5 ## 5 5 f 2 14.8 25.3 30.2 ## 6 6 f 3 11.1 12.1 16.2 # &quot;NHIS_2007_data4.csv&quot; d4 &lt;- read.csv(&quot;docs/data/tut3/NHIS_2007_data4.csv&quot;, header = T, sep = &quot;;&quot;, dec = &quot;,&quot;) head(d4) ## HHX FMX FPX SEX BMI SLEEP educ height weight ## 1 16 1 2 1 33.36 8 16 74 260 ## 2 20 1 1 1 26.54 7 14 70 185 ## 3 69 1 2 2 32.13 7 9 61 170 ## 4 87 1 1 1 26.62 8 14 68 175 ## 5 88 1 1 2 27.13 8 13 66 168 ## 6 99 1 1 2 99.99 98 12 98 998 Lösungen # &quot;lungen.txt&quot; d5 &lt;- read.table(&quot;docs/data/tut3/lungen.txt&quot;, header = T, sep = &quot;|&quot;, dec = &quot;,&quot;) head(d5) ## Lungenkapazität Alter Größe Raucher Geschlecht Kaiserschnitt ## 1 6.475 6 62.1 no male no ## 2 11.125 14 71.0 no male no ## 3 4.800 5 56.9 no male no ## 4 7.325 11 70.4 no male no ## 5 8.875 15 70.5 no male no ## 6 6.800 11 59.2 no male no # &quot;Tutorium_4.dat&quot; d6 &lt;- read.table(&quot;docs/data/tut3/Tutorium_4.dat&quot;, sep = &quot;;&quot;, dec = &quot;,&quot;, header = F, na.strings = c(&quot;umgeknickt&quot;, &quot;verdorrt&quot;)) head(d6) ## V1 V2 V3 V4 V5 ## 1 1 1 19.7 19.7 20.5 ## 2 1 2 14.2 18.9 27.3 ## 3 1 3 11.2 24.2 37.2 ## 4 2 1 18.8 20.3 28.5 ## 5 1 2 14.8 25.3 30.2 ## 6 1 3 11.1 12.1 16.2 Lösungen # &quot;raw2.txt&quot; d8 &lt;- read.table(&quot;docs/data/tut3/raw2.txt&quot;, sep = &quot;\\t&quot;, skip = 1, comment.char = &quot;%&quot;) head(d8) ## V1 V2 V3 V4 ## 1 1 1 a dass der Peter den Hans mag. ## 2 1 1 b dass den Hans der Peter mag. ## 3 1 1 c dass der Peter sicherlich den Hans mag. ## 4 1 1 d dass den Hans sicherlich der Peter mag. ## 5 1 2 a dass die Frau den Bäcker liebt. ## 6 1 2 b dass den Bäcker die Frau liebt. # &quot;raw.txt&quot; d7 &lt;- read.table(&quot;docs/data/tut3/raw.txt&quot;, sep = &quot;\\t&quot;, skip = 1, header = F) head(d7) ## V1 V2 V3 V4 ## 1 1 1 a dass der Peter den Hans mag. ## 2 1 1 b dass den Hans der Peter mag. ## 3 1 1 c dass der Peter sicherlich den Hans mag. ## 4 1 1 d dass den Hans sicherlich der Peter mag. ## 5 1 2 a dass die Frau den Bäcker liebt. ## 6 1 2 b dass den Bäcker die Frau liebt. Lösungen # &quot;master.txt&quot; d9 &lt;- read.table(&quot;docs/data/tut3/master.txt&quot;, sep = &quot;&amp;&quot;, comment.char = &quot;$&quot;, dec = &quot;,&quot;, header = T) head(d9) ## korinon povalon tertiron anbolon velbidon ## 1 korinon 1.230 tertiron ja 0 ## 2 korinon 4.230 tertiron nein 0 ## 3 korinon 1.124 tertiron ja 0 ## 4 korinon 8.340 tertiron nein 1 ## 5 korinon 0.900 tertiron nein 1 ## 6 korinon 6.740 tertiron nein 0 Daten einlesen mit GUI Und Tschüss Bis nächste Woche! "],
["sitzung-4.html", "Sitzung 4 Wiederholung Übung Tabellenüberblick Erste statistische Berechnungen Nur Kurz: Plots", " Sitzung 4 Wiederholung Das Wichtigste vom letzten Mal Wir haben verschiedene Methoden besprochen, nur Teile einer Tabelle zu verwenden (“Subsetting”) Was bedeuten die unteren Befehle? d[2:5, ] d[d$wert == 14 | d$wert &lt;= 2, ] d[d$blub %in% c(&quot;bla&quot;, &quot;bli&quot;), ] Wir können Daten in R einlesen und aus R exportieren und so auf dem Computer speichern: read.csv() read.table() write.csv() write.table() Übung Übung Lest die Datei “workwithme.csv” als Objekt d ein und lasst euch die ersten 10 Zeilen anzeigen Erstellt eine neue Spalte mit dem Namen “sum”, die die Zeilensumme der Spalten “aldi”, “lidl” und “penny” enthält Erstellt eine neue Spalte “sumrank”: Wenn in der “sum”-Spalte ein Wert steht, der größer als 140 ist, dann enthält sie den Wert “high”, von 140 bis 125 “medium” und darunter “low” Erstellt eine Spalte, die den Mittelwert der Werte aus den Spalten “aldi”, “lidl” und “penny” enthält Erstellt eine Spalte, die den Monat des Jahres enthält. Benutzt dafür die substring-Funktion Benennt die Spalten um Speichert das Datenblatt als csv-Datei auf eurem Computer, benutzt die folgenden Parameter: Spaltentrenner = |, keine Anführungszeichen um Text, keine Zeilennamen. Benutzt die Hilfefunktion, um die relevanten Argumente der write.csv-Funktion herauszufinden Lösung Lest die Datei “workwithme.csv” als Objekt d ein und lasst euch die ersten 10 Zeilen anzeigen d &lt;- read.csv(&quot;docs/data/tut4/workwithme.csv&quot;) head(d, 10) # alternative: d[1:10,] ## time aldi lidl penny source coeff ## 1 2018-01-01 49.41064 42.10573 44.69301 indirect 2.0289111 ## 2 2018-01-02 51.51267 38.47846 55.16647 indirect 0.8333160 ## 3 2018-01-03 44.75134 39.28813 46.71183 direct 2.1071974 ## 4 2018-01-04 45.09636 42.36057 54.62832 indirect 2.1303495 ## 5 2018-01-05 52.19682 36.08796 49.56332 indirect 0.7593176 ## 6 2018-01-06 48.85863 39.54701 44.47574 direct 1.4346635 ## 7 2018-01-07 44.31124 42.58359 49.07205 unknown 1.7395597 ## 8 2018-01-08 49.72772 38.21966 37.36114 unknown 0.4596969 ## 9 2018-01-09 49.90116 40.69519 64.80204 indirect 0.6560386 ## 10 2018-01-10 51.66494 38.04120 68.73857 indirect 1.1486464 Lösung Erstellt eine neue Spalte mit dem Namen “sum”, die die Zeilensumme der Spalten “aldi”, “lidl” und “penny” enthält d$sum &lt;- d$aldi + d$lidl + d$penny head(d) ## time aldi lidl penny source coeff sum ## 1 2018-01-01 49.41064 42.10573 44.69301 indirect 2.0289111 136.2094 ## 2 2018-01-02 51.51267 38.47846 55.16647 indirect 0.8333160 145.1576 ## 3 2018-01-03 44.75134 39.28813 46.71183 direct 2.1071974 130.7513 ## 4 2018-01-04 45.09636 42.36057 54.62832 indirect 2.1303495 142.0853 ## 5 2018-01-05 52.19682 36.08796 49.56332 indirect 0.7593176 137.8481 ## 6 2018-01-06 48.85863 39.54701 44.47574 direct 1.4346635 132.8814 Lösung Erstellt eine neue Spalte “sumrank”: Wenn in der “sum”-Spalte ein Wert ist, der größer als 140 ist, dann enthält sie den Wert “high”, von 140 bis 125 “medium” und darunter “low” d$sumrank[d$sum &gt; 140] &lt;- &quot;high&quot; d$sumrank[d$sum &lt;= 140] &lt;- &quot;medium&quot; d$sumrank[d$sum &lt;= 125] &lt;- &quot;low&quot; head(d[, 7:8]) ## sum sumrank ## 1 136.2094 medium ## 2 145.1576 high ## 3 130.7513 medium ## 4 142.0853 high ## 5 137.8481 medium ## 6 132.8814 medium Lösung Erstellt eine Spalte, die den Mittelwert der Werte aus den Spalten “aldi”, “lidl” und “penny” enthält \\[\\bar{x}= {\\frac {1}{n}}\\sum _{i=1}^{n}a_{i}={\\frac {a_{1}+a_{2}+\\cdots +a_{n}}{n}}\\] d$compmean &lt;- (d$aldi + d$lidl + d$penny) / 3 head(d[, c(2, 3, 4, 7, 8, 9)]) ## aldi lidl penny sum sumrank compmean ## 1 49.41064 42.10573 44.69301 136.2094 medium 45.40313 ## 2 51.51267 38.47846 55.16647 145.1576 high 48.38587 ## 3 44.75134 39.28813 46.71183 130.7513 medium 43.58377 ## 4 45.09636 42.36057 54.62832 142.0853 high 47.36175 ## 5 52.19682 36.08796 49.56332 137.8481 medium 45.94937 ## 6 48.85863 39.54701 44.47574 132.8814 medium 44.29379 Lösung Erstellt eine Spalte, die den Monat des Jahres enthält. Benutzt dafür die substring-Funktion d$month &lt;- substring(d$time, 6, 7) head(d[, c(1, 2, 3, 4, 7, 10)]) ## time aldi lidl penny sum month ## 1 2018-01-01 49.41064 42.10573 44.69301 136.2094 01 ## 2 2018-01-02 51.51267 38.47846 55.16647 145.1576 01 ## 3 2018-01-03 44.75134 39.28813 46.71183 130.7513 01 ## 4 2018-01-04 45.09636 42.36057 54.62832 142.0853 01 ## 5 2018-01-05 52.19682 36.08796 49.56332 137.8481 01 ## 6 2018-01-06 48.85863 39.54701 44.47574 132.8814 01 Lösung Benennt die Spalten um names(d) &lt;- c(&quot;date&quot;, &quot;markt1&quot;, &quot;markt2&quot;, &quot;markt3&quot;, &quot;origin&quot;, &quot;coef&quot;, &quot;all&quot;, &quot;level&quot;, &quot;marktmean&quot;, &quot;month&quot;, &quot;coefmed&quot;, &quot;quarter&quot;) Speichert das Datenblatt als csv-Datei auf eurem Computer, benutzt die folgenden Parameter: Spaltentrenner = |, keine Anführungszeichen um Text, keine Zeilennamen. write.csv(d, &quot;supermarktBS.csv&quot;, sep = &quot;|&quot;, row.names = F, quote = F) Tabellenüberblick Struktur dim(d) # alternativ: nrow(d), ncol(d) ## [1] 365 10 str(d) ## &#39;data.frame&#39;: 365 obs. of 10 variables: ## $ time : Factor w/ 365 levels &quot;2018-01-01&quot;,&quot;2018-01-02&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... ## $ aldi : num 49.4 51.5 44.8 45.1 52.2 ... ## $ lidl : num 42.1 38.5 39.3 42.4 36.1 ... ## $ penny : num 44.7 55.2 46.7 54.6 49.6 ... ## $ source : Factor w/ 3 levels &quot;direct&quot;,&quot;indirect&quot;,..: 2 2 1 2 2 1 3 3 2 2 ... ## $ coeff : num 2.029 0.833 2.107 2.13 0.759 ... ## $ sum : num 136 145 131 142 138 ... ## $ sumrank : chr &quot;medium&quot; &quot;high&quot; &quot;medium&quot; &quot;high&quot; ... ## $ compmean: num 45.4 48.4 43.6 47.4 45.9 ... ## $ month : chr &quot;01&quot; &quot;01&quot; &quot;01&quot; &quot;01&quot; ... überblick über Zahlen summary(d) ## time aldi lidl penny ## 2018-01-01: 1 Min. :35.02 Min. :33.93 Min. :20.21 ## 2018-01-02: 1 1st Qu.:46.57 1st Qu.:38.70 1st Qu.:38.29 ## 2018-01-03: 1 Median :49.74 Median :40.04 Median :45.88 ## 2018-01-04: 1 Mean :49.80 Mean :40.10 Mean :45.49 ## 2018-01-05: 1 3rd Qu.:52.78 3rd Qu.:41.33 3rd Qu.:51.93 ## 2018-01-06: 1 Max. :63.52 Max. :47.83 Max. :81.56 ## (Other) :359 ## source coeff sum sumrank ## direct :106 Min. :-0.4144 Min. :103.3 Length:365 ## indirect:210 1st Qu.: 0.6830 1st Qu.:127.4 Class :character ## unknown : 49 Median : 0.9468 Median :135.4 Mode :character ## Mean : 0.9746 Mean :135.4 ## 3rd Qu.: 1.2882 3rd Qu.:142.7 ## Max. : 2.1701 Max. :171.2 ## ## compmean month ## Min. :34.44 Length:365 ## 1st Qu.:42.48 Class :character ## Median :45.15 Mode :character ## Mean :45.13 ## 3rd Qu.:47.56 ## Max. :57.07 ## Häufigkeiten table(d$sumrank) ## ## high low medium ## 122 65 178 # verteilungen von summenwerten auf monate abgebildet overview &lt;- table(d$sumrank, d$month) overview ## ## 01 02 03 04 05 06 07 08 09 10 11 12 ## high 7 11 10 10 10 7 14 10 15 12 9 7 ## low 6 8 7 2 5 7 2 4 4 7 6 7 ## medium 18 9 14 18 16 16 15 17 11 12 15 17 (overview2 &lt;- addmargins(overview)) ## ## 01 02 03 04 05 06 07 08 09 10 11 12 Sum ## high 7 11 10 10 10 7 14 10 15 12 9 7 122 ## low 6 8 7 2 5 7 2 4 4 7 6 7 65 ## medium 18 9 14 18 16 16 15 17 11 12 15 17 178 ## Sum 31 28 31 30 31 30 31 31 30 31 30 31 365 Relative Häufigkeiten overview3 &lt;- prop.table(overview) * 100 overview3 ## ## 01 02 03 04 05 06 07 ## high 1.9178082 3.0136986 2.7397260 2.7397260 2.7397260 1.9178082 3.8356164 ## low 1.6438356 2.1917808 1.9178082 0.5479452 1.3698630 1.9178082 0.5479452 ## medium 4.9315068 2.4657534 3.8356164 4.9315068 4.3835616 4.3835616 4.1095890 ## ## 08 09 10 11 12 ## high 2.7397260 4.1095890 3.2876712 2.4657534 1.9178082 ## low 1.0958904 1.0958904 1.9178082 1.6438356 1.9178082 ## medium 4.6575342 3.0136986 3.2876712 4.1095890 4.6575342 round(prop.table(overview) * 100, digits = 2) ## ## 01 02 03 04 05 06 07 08 09 10 11 12 ## high 1.92 3.01 2.74 2.74 2.74 1.92 3.84 2.74 4.11 3.29 2.47 1.92 ## low 1.64 2.19 1.92 0.55 1.37 1.92 0.55 1.10 1.10 1.92 1.64 1.92 ## medium 4.93 2.47 3.84 4.93 4.38 4.38 4.11 4.66 3.01 3.29 4.11 4.66 Erste statistische Berechnungen Maße der zentralen Tendenz Die wichtigsten statistischen Operationen sind als Funktionen in R enthalten. So auch das arithmetische Mittel und der Median \\[\\bar{x}= {\\frac {1}{n}}\\sum _{i=1}^{n}a_{i}={\\frac {a_{1}+a_{2}+\\cdots +a_{n}}{n}}\\] \\[\\tilde{x}=\\begin{cases} x_{\\frac{n+1}{2}}&amp;n\\text{ odd}\\\\ \\frac{1}{2}\\left(x_{\\frac{n}{2}} + x_{\\frac{n}{2} + 1}\\right)&amp;n \\text{ even}\\\\ \\end{cases}\\] sample &lt;- c(1, 1, 2, 2, 4, 6, 9) mean(sample) ## [1] 3.571429 median(sample) ## [1] 2 Meme My Stats Maße der Streuung Gleiches gilt für die Streuungsmaße Standardabweichung und Median der absoluten Abweichung \\[s= \\sqrt{s^2} = {\\sqrt{\\frac {\\sum _{i=1}^{n}(x_{i}-{\\overline {x}})^{2}}{n-1}}}\\] \\[MAD = median|x_i - \\tilde{x}|\\] sample &lt;- c(1, 1, 2, 2, 4, 6, 9) sd(sample) ## [1] 2.992053 mad(sample, constant = 1) ## [1] 1 Übung Gibt es Werte in der “lidl”-Spalte, die mehr als 2 Standardabweichungen vom Mittelwert abweichen? Erstellt eine Tabelle mit relativen Häufigkeiten für “source” abgebildet auf “sumrank” Legt eine neue Spalte “coeffmedian” an, die den Median der Spalte “coeff” enthält Legt eine neue Spalte an, die Informationen über das Jahresquartal enthält. Um die Monate (01, 02, 03, …) in Zahlen (1, 2, 3, …) umzuwandeln, könnt ihr as.numeric verwenden. Lösung Gibt es Werte in der “lidl”-Spalte, die mehr als 2 Standardabweichungen vom Mittelwert abweichen? d$lidl[d$lidl &gt;= mean(d$lidl) + 2 * sd(d$lidl) | d$lidl &lt;= mean(d$lidl) - 2 * sd(d$lidl)] ## [1] 44.35339 35.54213 44.55232 35.88545 35.28163 46.54198 44.44375 35.81564 ## [9] 33.93010 44.88954 44.88695 35.88933 44.45852 35.95163 44.34326 44.81850 ## [17] 45.40808 47.83175 35.70551 35.95866 35.74292 Lösung Erstellt eine Tabelle mit relativen Häufigkeiten für “source” abgebildet auf “sumrank” tab &lt;- table(d$source, d$sumrank) (perctab &lt;- prop.table(tab) * 100) ## ## high low medium ## direct 11.506849 4.383562 13.150685 ## indirect 16.164384 10.684932 30.684932 ## unknown 5.753425 2.739726 4.931507 Legt eine neue Spalte “coeffmedian” an, die den Median der Spalte “coeff” enthält d$coeffmedian &lt;- median(d$coeff) tail(d[, c(6, 11)]) ## coeff coeffmedian ## 360 1.3716517 0.9467506 ## 361 0.1725648 0.9467506 ## 362 0.9713443 0.9467506 ## 363 0.8421736 0.9467506 ## 364 0.6721796 0.9467506 ## 365 1.2695549 0.9467506 Lösung Legt eine neue Spalte an, die Informationen über das Jahresquartal enthält. Um die Monate (01, 02, 03, …) in Zahlen (1, 2, 3, …) umzuwandeln, könnt ihr as.numeric verwenden. q &lt;- c(1, 2, 3) d$quarter[as.numeric(d$month) %in% q] &lt;- 1 d$quarter[as.numeric(d$month) %in% (q + 3)] &lt;- 2 d$quarter[as.numeric(d$month) %in% (q + 6)] &lt;- 3 d$quarter[as.numeric(d$month) %in% (q + 9)] &lt;- 4 # head(d[,c(10,length(d))], 3) # tail(d[,c(10,length(d))], 3) table(d$quarter) ## ## 1 2 3 4 ## 90 91 92 92 Nur Kurz: Plots Ein Plot zum Abschluss d$time &lt;- as.ts(d$time) # convert to time series plot(d$time, d$compmean, type = &quot;l&quot;, xlab = &quot;Tag&quot;, ylab = &quot;CompMean&quot;, main = &quot;Line Plot&quot;) Ein Plot zum Abschluss library(ggplot2) d$time &lt;- as.ts(d$time) # convert to time series ggplot(d, aes(x = time, y = compmean)) + geom_line() + labs(x = &quot;Tag&quot;, y = &quot;CompMean&quot;, title = &quot;Line Plot&quot;) session no longer in session Schönes Wochenende! "],
["sitzung-5.html", "Sitzung 5 Wiederholung Tabellen miteinander verbinden Fehlende Werte Faktoren Nur Kurz: Plots", " Sitzung 5 Wiederholung Das Wichtigste vom letzten Mal Wir haben uns verschiedene Funktionen angesehen, die in der Statistik benutzt werden # zentrale Tendenz mean() median() # streuung sd() mad() Das Wichtigste vom letzten Mal Wir haben gelernt, Häufigkeiten zählen zu lassen table() prop.table() Das Wichtigste vom letzten Mal Wir haben uns die Struktur von Datenblättern angesehen head(diamonds, 3) ## carat cut color clarity depth table price x y z ## 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 ## 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 ## 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 str(diamonds) ## &#39;data.frame&#39;: 53940 obs. of 10 variables: ## $ carat : num 0.23 0.21 0.23 0.29 0.31 0.24 0.24 0.26 0.22 0.23 ... ## $ cut : Ord.factor w/ 5 levels &quot;Fair&quot;&lt;&quot;Good&quot;&lt;..: 5 4 2 4 2 3 3 3 1 3 ... ## $ color : Ord.factor w/ 7 levels &quot;D&quot;&lt;&quot;E&quot;&lt;&quot;F&quot;&lt;&quot;G&quot;&lt;..: 2 2 2 6 7 7 6 5 2 5 ... ## $ clarity: Ord.factor w/ 8 levels &quot;I1&quot;&lt;&quot;SI2&quot;&lt;&quot;SI1&quot;&lt;..: 2 3 5 4 2 6 7 3 4 5 ... ## $ depth : num 61.5 59.8 56.9 62.4 63.3 62.8 62.3 61.9 65.1 59.4 ... ## $ table : num 55 61 65 58 58 57 57 55 61 61 ... ## $ price : int 326 326 327 334 335 336 336 337 337 338 ... ## $ x : num 3.95 3.89 4.05 4.2 4.34 3.94 3.95 4.07 3.87 4 ... ## $ y : num 3.98 3.84 4.07 4.23 4.35 3.96 3.98 4.11 3.78 4.05 ... ## $ z : num 2.43 2.31 2.31 2.63 2.75 2.48 2.47 2.53 2.49 2.39 ... Das Wichtigste vom letzten Mal summary(diamonds) ## carat cut color clarity depth ## Min. :0.2000 Fair : 1610 D: 6775 SI1 :13065 Min. :43.00 ## 1st Qu.:0.4000 Good : 4906 E: 9797 VS2 :12258 1st Qu.:61.00 ## Median :0.7000 Very Good:12082 F: 9542 SI2 : 9194 Median :61.80 ## Mean :0.7979 Premium :13791 G:11292 VS1 : 8171 Mean :61.75 ## 3rd Qu.:1.0400 Ideal :21551 H: 8304 VVS2 : 5066 3rd Qu.:62.50 ## Max. :5.0100 I: 5422 VVS1 : 3655 Max. :79.00 ## J: 2808 (Other): 2531 ## table price x y ## Min. :43.00 Min. : 326 Min. : 0.000 Min. : 0.000 ## 1st Qu.:56.00 1st Qu.: 950 1st Qu.: 4.710 1st Qu.: 4.720 ## Median :57.00 Median : 2401 Median : 5.700 Median : 5.710 ## Mean :57.46 Mean : 3933 Mean : 5.731 Mean : 5.735 ## 3rd Qu.:59.00 3rd Qu.: 5324 3rd Qu.: 6.540 3rd Qu.: 6.540 ## Max. :95.00 Max. :18823 Max. :10.740 Max. :58.900 ## ## z ## Min. : 0.000 ## 1st Qu.: 2.910 ## Median : 3.530 ## Mean : 3.539 ## 3rd Qu.: 4.040 ## Max. :31.800 ## Tabellen miteinander verbinden Verbinden von zwei Tabellen Tabellen untereinander zusammenfügen: rbind(Tabelle1, Tabelle2) Tabellen nebeneinander zusammenfügen: cbind(Tabelle1, Tabelle2) Übung Lest die folgenden Tabellen in R ein Briefe.csv Briefe2.csv Tagebuecher.csv Tagebuecher2.csv Verbindet alle Tabellen sinnvoll miteinander Lösung Lest die Tabellen in R ein d1 &lt;- read.csv(&quot;docs/data/tut5/Briefe.csv&quot;, sep = &quot;;&quot;, dec = &quot;,&quot;) d2 &lt;- read.csv(&quot;docs/data/tut5/Briefe2.csv&quot;, sep = &quot;;&quot;, dec = &quot;,&quot;) d3 &lt;- read.csv(&quot;docs/data/tut5/Tagebuecher.csv&quot;, sep = &quot;;&quot;, dec = &quot;,&quot;) d4 &lt;- read.csv(&quot;docs/data/tut5/Tagebuecher2.csv&quot;, sep = &quot;;&quot;, dec = &quot;,&quot;) head(d1, 3) ## Filename Segment WC WPS Sixltr Pronoun I We ## 1 m-Rollet_Alexander-BR-1844_1906.txt 1 500 19.23 32.4 4.0 0.0 0.4 ## 2 m-Rollet_Alexander-BR-1844_1906.txt 2 500 18.52 32.8 7.6 3.6 0.6 ## 3 m-Rollet_Alexander-BR-1844_1906.txt 3 500 41.67 28.8 9.2 0.0 5.6 ## Self You ## 1 0.4 2.2 ## 2 4.2 1.6 ## 3 5.6 0.8 head(d2, 3) ## Other Preps Affect Posemo Negemo Past Present Future ## 1 3.6 9.4 3.2 2.4 0.8 1.2 1.6 0.2 ## 2 3.2 7.2 3.6 3.2 0.2 2.2 2.2 0.2 ## 3 3.2 7.2 2.0 2.0 0.0 2.4 2.8 1.4 Lösung Verbindet alle Tabellen sinnvoll miteinander dneu1 &lt;- cbind(d1, d2) # schreibt die Tabellen nebeneinander dneu2 &lt;- cbind(d3, d4) # schreibt die Tabellen nebeneinander dneuges &lt;- rbind(dneu1, dneu2) # schreibt die Tabellen untereinander head(dneuges) ## Filename Segment WC WPS Sixltr Pronoun I We ## 1 m-Rollet_Alexander-BR-1844_1906.txt 1 500 19.23 32.4 4.0 0.0 0.4 ## 2 m-Rollet_Alexander-BR-1844_1906.txt 2 500 18.52 32.8 7.6 3.6 0.6 ## 3 m-Rollet_Alexander-BR-1844_1906.txt 3 500 41.67 28.8 9.2 0.0 5.6 ## 4 m-Rollet_Alexander-BR-1844_1906.txt 4 500 31.25 36.2 5.8 0.0 1.2 ## 5 m-Rollet_Alexander-BR-1844_1906.txt 5 500 23.81 34.8 5.0 0.0 0.6 ## 6 m-Rollet_Alexander-BR-1844_1906.txt 6 500 25.00 36.0 8.4 4.8 0.6 ## Self You Other Preps Affect Posemo Negemo Past Present Future ## 1 0.4 2.2 3.6 9.4 3.2 2.4 0.8 1.2 1.6 0.2 ## 2 4.2 1.6 3.2 7.2 3.6 3.2 0.2 2.2 2.2 0.2 ## 3 5.6 0.8 3.2 7.2 2.0 2.0 0.0 2.4 2.8 1.4 ## 4 1.2 1.8 4.4 7.2 3.8 3.4 0.4 2.4 3.4 2.4 ## 5 0.6 2.2 3.4 6.6 3.4 3.4 0.0 1.6 2.2 0.6 ## 6 5.4 1.6 2.6 11.4 4.6 3.8 0.8 1.8 3.6 0.2 Fehlende Werte NAs Fehlende Werte führen dazu, dass Berechnungen ins Leere laufen NA + 8 ## [1] NA sum(c(1, NA, 3, 5)) ## [1] NA Zwar ist dies bei intern erstellten Daten kein Problem, aber was ist, wenn Werte in einem Datenset fehlen wie hier: d &lt;- read.table(&quot;docs/data/tut5/alldata.dat&quot;, header = T) head(d) ## id geo subject variant experiment item condition judgement ## 1 m-1987-EMusik bla 1 1 9 4 a 6 ## 2 m-1987-EMusik bla 1 1 8 2 b 7 ## 3 m-1987-EMusik bla 1 1 7 10 b 7 ## 4 m-1987-EMusik bla 1 1 9 3 d 1 ## 5 m-1987-EMusik bla 1 1 1 8 d NA ## 6 m-1987-EMusik bla 1 1 8 9 a 6 mean(d$judgement) ## [1] NA NAs finden anyNA gibt an, ob fehlende Werte enthalten sind: anyNA(c(1, NA, 3, 5)) ## [1] TRUE anyNA(d$judgement) ## [1] TRUE is.na geht das ganze Objekt durch, durchsucht es nach NAs und gibt deren Präsenz über Wahrheitswerte an: is.na(c(1, NA, 3, 5)) ## [1] FALSE TRUE FALSE FALSE Da Tabellen in der Regel sehr viele Werte enthalten, macht es hier Sinn, sich das Ergebnis der Suche als Tabelle ausgeben zu lassen: table(is.na(d$judgement)) # summary statt table auch möglich ## ## FALSE TRUE ## 3836 4 alle Zeilen mit NAs d[!complete.cases(d), ] ## id geo subject variant experiment item condition judgement ## 5 m-1987-EMusik bla 1 1 1 8 d NA ## 205 m-1983-EnglSport bla 3 1 7 12 d NA ## 2431 m-86-psych ??? 31 6 7 12 c NA ## 3284 m-1987-geschengl ??? 42 7 8 22 d NA Prozent NA Zeigt für jede Spalte an, wie viel Prozent der Daten NAs sind (apply(is.na(d), 2, sum) / length(d[, 1])) * 100 ## id geo subject variant experiment item condition ## 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 ## judgement ## 0.1041667 NAs überspringen Möglichkeit 1: mean(d$judgement[is.na(d$judgement) == F ]) ## [1] 4.64025 Möglichkeit 2: na.rm ist ein logisches Argument, das in vielen Funktionen zusätzlich angegeben werden kann, um fehlende Werte bei der Berechnung auszusparen mean(d$judgement, na.rm = T) ## [1] 4.64025 sd(d$judgement, na.rm = T) ## [1] 2.435801 NAs entfernen Alternativ kann man alle Zeilen, die fehlende Werte aufweisen, aus dem Datensatz entfernen d &lt;- na.omit(d) mean(d$judgement) ## [1] 4.64025 Faktoren Vektoren mit Faktorstufen Vor allem wichtig in der statistischen Berechnung und bei Plots, weil Faktoren in R der Datentyp von kategorialen und ordinalen Variablen sind (für letzteres eigentlich ordered = TRUE, kann aber regelmäßig weggelassen werden) calories &lt;- c(&quot;high&quot;, &quot;high&quot;, &quot;low&quot;, &quot;medium&quot;, &quot;high&quot;, &quot;medium&quot;) levels(calories) ## NULL calories &lt;- factor(calories) levels(calories) ## [1] &quot;high&quot; &quot;low&quot; &quot;medium&quot; calories &lt;- factor(calories, levels = c(&quot;low&quot;, &quot;medium&quot;, &quot;high&quot;)) levels(calories) ## [1] &quot;low&quot; &quot;medium&quot; &quot;high&quot; Plots # funktioniert nicht! # plot(as.character(calories)) plot(calories) Level bleiben erhalten Selbst wenn alle Instanzen eines Levels wegfallen, bleibt das Level im Faktor enthalten: calories &lt;- calories[calories %in% c(&quot;low&quot;, &quot;medium&quot;)] calories ## [1] low medium medium ## Levels: low medium high Dies kann mithilfe von einer erneuten Anwendung von factor behoben werden. Alternativ kann auch die droplevels-Funktion verwendet werden. calories &lt;- factor(calories) calories ## [1] low medium medium ## Levels: low medium Konvertierungen weitere wichtige Konvertierungen: - as.numeric() - as.logical() - as.data.frame() str(calories) ## Factor w/ 2 levels &quot;low&quot;,&quot;medium&quot;: 1 2 2 calories &lt;- as.character(calories) str(calories) ## chr [1:3] &quot;low&quot; &quot;medium&quot; &quot;medium&quot; calories &lt;- as.factor(calories) str(calories) ## Factor w/ 2 levels &quot;low&quot;,&quot;medium&quot;: 1 2 2 summary() calories &lt;- factor(c(&quot;high&quot;, &quot;high&quot;, &quot;low&quot;, &quot;medium&quot;, &quot;high&quot;, &quot;medium&quot;), levels = c(&quot;low&quot;, &quot;medium&quot;, &quot;high&quot;)) summary(calories) # funktioniert wie table() ## low medium high ## 1 2 3 summary(as.character(calories)) ## Length Class Mode ## 6 character character Fakoren in eingelesenen Daten R konvertiert strings, also alphabetische Zeichenketten, automatisch zu Faktoren, wenn sie in eingelesenen Daten vorkommen. Um dies zu verhindern, verwendet man stringsAsFactors = F read.csv(&quot;data.csv&quot;, stringsAsFactors = F) Übungen II Lese die Datei “titanic.csv” in R ein und verschaffe dir einen Überblick über die Daten Erstelle ein neues Objekt, das keine NAs enthält Ersetze im Originalobjekt alle NAs durch den Mittelwert der jeweiligen Spalte Lösungen II Lese die Datei “titanic.csv” in R ein und verschaffe dir einen Überblick über die Daten titanic &lt;- read.csv(&quot;docs/data/tut5/titanic.csv&quot;) head(titanic, 3) ## PassengerId Pclass Name Sex Age SibSp Parch ## 1 892 3 Kelly, Mr. James male 34.5 0 0 ## 2 893 3 Wilkes, Mrs. James (Ellen Needs) female 47.0 1 0 ## 3 894 2 Myles, Mr. Thomas Francis male 62.0 0 0 ## Ticket Fare Cabin Embarked ## 1 330911 7.8292 Q ## 2 363272 7.0000 S ## 3 240276 9.6875 Q str(titanic) ## &#39;data.frame&#39;: 418 obs. of 11 variables: ## $ PassengerId: int 892 893 894 895 896 897 898 899 900 901 ... ## $ Pclass : int 3 3 2 3 3 3 3 2 3 3 ... ## $ Name : Factor w/ 418 levels &quot;Abbott, Master. Eugene Joseph&quot;,..: 210 409 273 414 182 370 85 58 5 104 ... ## $ Sex : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 2 1 2 2 1 2 1 2 1 2 ... ## $ Age : num 34.5 47 62 27 22 14 30 26 18 21 ... ## $ SibSp : int 0 1 0 0 1 0 0 1 0 2 ... ## $ Parch : int 0 0 0 0 1 0 0 1 0 0 ... ## $ Ticket : Factor w/ 363 levels &quot;110469&quot;,&quot;110489&quot;,..: 153 222 74 148 139 262 159 85 101 270 ... ## $ Fare : num 7.83 7 9.69 8.66 12.29 ... ## $ Cabin : Factor w/ 77 levels &quot;&quot;,&quot;A11&quot;,&quot;A18&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ Embarked : Factor w/ 3 levels &quot;C&quot;,&quot;Q&quot;,&quot;S&quot;: 2 3 2 3 3 3 2 3 1 3 ... Lösungen II Erstelle ein neues Objekt, das keine NAs enthält table(is.na(titanic)) ## ## FALSE TRUE ## 4511 87 table(is.na(titanic$Age)) ## ## FALSE TRUE ## 332 86 titanicsub &lt;- na.omit(titanic) dim(titanic) # or: nrow() ## [1] 418 11 dim(titanicsub) ## [1] 331 11 Lösungen II Ersetze im Originalobjekt alle NAs durch den Mittelwert der jeweiligen Spalte # durchschauen mit: # titanic[!complete.cases(titanic),] titanic[c(11, 23, 30, 34, 37, 153), ] ## PassengerId Pclass Name Sex ## 11 902 3 Ilieff, Mr. Ylio male ## 23 914 1 Flegenheim, Mrs. Alfred (Antoinette) female ## 30 921 3 Samaan, Mr. Elias male ## 34 925 3 Johnston, Mrs. Andrew G (Elizabeth Lily&quot; Watson)&quot; female ## 37 928 3 Roth, Miss. Sarah A female ## 153 1044 3 Storey, Mr. Thomas male ## Age SibSp Parch Ticket Fare Cabin Embarked ## 11 NA 0 0 349220 7.8958 S ## 23 NA 0 0 PC 17598 31.6833 S ## 30 NA 2 0 2662 21.6792 C ## 34 NA 1 2 W./C. 6607 23.4500 S ## 37 NA 0 0 342712 8.0500 S ## 153 60.5 0 0 3701 NA S titanic$Age[is.na(titanic$Age)] &lt;- mean(titanic$Age, na.rm = T) titanic$Fare[is.na(titanic$Fare)] &lt;- mean(titanic$Fare, na.rm = T) anyNA(titanic) ## [1] FALSE Nur Kurz: Plots Ein Plot zum Abschluss hist(titanic$Age, main = &quot;Titanic-Datensatz&quot;, xlab = &quot;Passagieralter&quot;, ylab = &quot;Häufigkeit&quot;) Ein Plot zum Abschluss library(ggplot2) ggplot(titanic, aes(x = Age)) + geom_histogram() + labs(title = &quot;Titanic-Datensatz&quot;, x = &quot;Passagieralter&quot;, y = &quot;Häufigkeit&quot;) Friday Noon Fever Bis in einer Woche! "],
["sitzung-6.html", "Sitzung 6 Wiederholung Pakete und mehr Statistik Eigene Funktionen schreiben Tabellen transponieren Nur Kurz: Plots", " Sitzung 6 Wiederholung Das Wichtigste vom letzten Mal Tabellen verknüpfen cbind() rbind() Faktoren und NAs a &lt;- c(1, 2, 3, 4, 5, 4, 3, 5, NA, 4, 2, 1, 2, 3, 3, 6, 4) b &lt;- factor(a) b ## [1] 1 2 3 4 5 4 3 5 &lt;NA&gt; 4 2 1 2 3 3 ## [16] 6 4 ## Levels: 1 2 3 4 5 6 anyNA(b) ## [1] TRUE b &lt;- na.omit(b) length(b) == length(a) - 1 ## [1] TRUE Pakete und mehr Statistik Viel Tipparbeit length(a) ## [1] 16 mean(a) ## [1] 3.25 sd(a) ## [1] 1.437591 median(a) ## [1] 3 mad(a) ## [1] 1.4826 min(a) ## [1] 1 max(a) ## [1] 6 range(a) ## [1] 1 6 Wenig Tipparbeit (mit Paketen) describe(a) ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 16 3.25 1.44 3 3.21 1.48 1 6 5 0.09 -1.01 0.36 Pakete installieren Pakete installieren (muss nur einmal gemacht werden): install.packages(&quot;psych&quot;) Pakete einbinden, damit die darin enthaltenen Funktionen verwendet werden können (muss in jeder neuen R-Session gemacht werden; am besten an den Anfang des Skripts schreiben) library(psych) describe(a) ## vars n mean sd median trimmed mad min max range skew kurtosis se ## X1 1 16 3.25 1.44 3 3.21 1.48 1 6 5 0.09 -1.01 0.36 Interlude: Mein Skiptanfang Meine Skipte fangen in der Regel so an wie unten. Einige der Pakete werdet ihr im Laufe dieses Semesters kennenlernen; andere sind für euch entweder nicht wichtig, oder die enthaltenen Funktionen übersteigen den (statistischen) Seminarstoff. ### %#################################################################################%### # # ###### &lt;&lt;Titel&gt;&gt; ###### # # ### %#################################################################################%### rm(list = ls()) # empty environment options(scipen = 999) # no scientific number notation library(ez) library(tidyverse) library(psych) # describeBy library(gridExtra) # grid.arrange for plots library(ggpubr) # theme_pubr(margin = F) + labs_pubr() library(data.table) library(ordinal) # used for the glmm library(lme4) library(ggthemes) library(tikzDevice) # ggplot-latex library(xtable) # table-latex describeBy() Gibt die wichtigsten deskriptiv-statistischen Berechnungen (in unserem Fall idR für eine abhängige Variable) wieder. Da man es jedoch oft mit Experimenten zu tun hat, in denen mindestens zwei Bedingungen einer unanhängigen Variable Einfluss auf die Messung nehmen, gibt es außerdem describeBy describeBy(diamonds$price, diamonds$cut) ## ## Descriptive statistics by group ## group: Fair ## vars n mean sd median trimmed mad min max range skew ## X1 1 1610 4358.76 3560.39 3282 3695.65 2183.13 337 18574 18237 1.78 ## kurtosis se ## X1 3.07 88.73 ## ------------------------------------------------------------ ## group: Good ## vars n mean sd median trimmed mad min max range skew ## X1 1 4906 3928.86 3681.59 3050.5 3251.51 2853.26 327 18788 18461 1.72 ## kurtosis se ## X1 3.04 52.56 ## ------------------------------------------------------------ ## group: Very Good ## vars n mean sd median trimmed mad min max range skew ## X1 1 12082 3981.76 3935.86 2648 3243.22 2855.49 336 18818 18482 1.6 ## kurtosis se ## X1 2.24 35.81 ## ------------------------------------------------------------ ## group: Premium ## vars n mean sd median trimmed mad min max range skew ## X1 1 13791 4584.26 4349.2 3185 3822.23 3371.43 326 18823 18497 1.33 ## kurtosis se ## X1 1.07 37.03 ## ------------------------------------------------------------ ## group: Ideal ## vars n mean sd median trimmed mad min max range skew ## X1 1 21551 3457.54 3808.4 1810 2656.14 1630.86 326 18806 18480 1.84 ## kurtosis se ## X1 2.98 25.94 describeBy() Um den Output ein bisschen besser lesbar zu machen, eignen sich diese beiden optionalen Parameter: Anzahl der Nachkommastellen bestimmten: digits = Zahl Ouput der Funktion als Matrix: mat = TRUE describeBy(diamonds$price, diamonds$cut, digits = 2, mat = T) ## item group1 vars n mean sd median trimmed mad min max ## X11 1 Fair 1 1610 4358.76 3560.39 3282.0 3695.65 2183.13 337 18574 ## X12 2 Good 1 4906 3928.86 3681.59 3050.5 3251.51 2853.26 327 18788 ## X13 3 Very Good 1 12082 3981.76 3935.86 2648.0 3243.22 2855.49 336 18818 ## X14 4 Premium 1 13791 4584.26 4349.20 3185.0 3822.23 3371.43 326 18823 ## X15 5 Ideal 1 21551 3457.54 3808.40 1810.0 2656.14 1630.86 326 18806 ## range skew kurtosis se ## X11 18237 1.78 3.07 88.73 ## X12 18461 1.72 3.04 52.56 ## X13 18482 1.60 2.24 35.81 ## X14 18497 1.33 1.07 37.03 ## X15 18480 1.84 2.98 25.94 describeBy() Falls mehr als nur ein Faktor mit einbezogen werden sollen, muss das zweite Argument der Funktion eine list sein, die sämtliche dieser Faktoren enthält describeBy(diamonds$price, list(diamonds$cut, diamonds$color), digits = 2, mat = T) ## item group1 group2 vars n mean sd median trimmed mad min ## X11 1 Fair D 1 163 4291.06 3286.11 3730.0 3703.96 2043.02 536 ## X12 2 Good D 1 662 3405.38 3175.15 2728.5 2859.30 2674.61 361 ## X13 3 Very Good D 1 1513 3470.47 3523.75 2310.0 2774.01 2373.64 357 ## X14 4 Premium D 1 1603 3631.29 3711.63 2009.0 2881.00 1893.28 367 ## X15 5 Ideal D 1 2834 2629.09 3001.07 1576.0 1929.83 1184.60 367 ## X16 6 Fair E 1 224 3682.31 2976.65 2956.0 3164.34 2311.37 337 ## X17 7 Good E 1 933 3423.64 3330.70 2420.0 2799.42 2542.66 327 ## X18 8 Very Good E 1 2400 3214.65 3408.02 1989.5 2514.87 2014.11 352 ## X19 9 Premium E 1 2337 3538.91 3794.99 1928.0 2719.40 1712.40 326 ## X110 10 Ideal E 1 3903 2597.55 2956.01 1437.0 1904.70 1003.72 326 ## X111 11 Fair F 1 312 3827.00 3223.30 3035.0 3255.46 2404.04 496 ## X112 12 Good F 1 909 3495.75 3202.41 2647.0 2932.24 2403.29 357 ## X113 13 Very Good F 1 2164 3778.82 3786.12 2471.0 3052.83 2581.95 357 ## X114 14 Premium F 1 2331 4324.89 4012.02 2841.0 3605.12 2802.11 342 ## X115 15 Ideal F 1 3826 3374.94 3766.64 1775.0 2561.01 1481.12 408 ## X116 16 Fair G 1 314 4239.25 3609.64 3057.0 3537.06 2097.14 369 ## X117 17 Good G 1 871 4123.48 3702.50 3340.0 3473.04 3177.21 394 ## X118 18 Very Good G 1 2299 3872.75 3861.38 2437.0 3166.19 2610.86 354 ## X119 19 Premium G 1 2924 4500.74 4356.57 2745.0 3738.69 2919.24 382 ## X120 20 Ideal G 1 4884 3720.71 4006.26 1857.5 2948.86 1686.46 361 ## X121 21 Fair H 1 303 5135.68 3886.48 3816.0 4489.27 2544.14 659 ## X122 22 Good H 1 702 4276.25 4020.66 3468.5 3546.09 3231.33 368 ## X123 23 Very Good H 1 1824 4535.39 4185.80 3734.0 3820.86 3670.92 337 ## X124 24 Premium H 1 2360 5216.71 4466.19 4511.0 4551.71 4312.88 368 ## X125 25 Ideal H 1 3115 3889.33 4013.38 2278.0 3121.10 2493.73 357 ## X126 26 Fair I 1 175 4685.45 3730.27 3246.0 4015.75 1792.46 735 ## X127 27 Good I 1 522 5078.53 4631.70 3639.5 4353.21 3814.73 351 ## X128 28 Very Good I 1 1204 5255.88 4687.10 3888.0 4519.71 3925.18 336 ## X129 29 Premium I 1 1428 5946.18 5053.75 4640.0 5303.56 5278.06 334 ## X130 30 Ideal I 1 2093 4451.97 4505.15 2659.0 3643.44 3068.98 348 ## X131 31 Fair J 1 119 4975.66 4050.46 3302.0 4280.01 2277.27 416 ## X132 32 Good J 1 307 4574.17 3707.79 3733.0 4039.28 3546.38 335 ## X133 33 Very Good J 1 678 5103.51 4135.65 4113.0 4574.19 3936.30 336 ## X134 34 Premium J 1 808 6294.59 4788.94 5063.0 5833.48 4679.09 363 ## X135 35 Ideal J 1 896 4918.19 4476.21 4096.0 4194.51 4272.85 340 ## max range skew kurtosis se ## X11 16386 15850 1.90 3.81 257.39 ## X12 18468 18107 2.03 5.42 123.41 ## X13 18542 18185 1.91 3.92 90.59 ## X14 18575 18208 1.78 2.96 92.70 ## X15 18693 18326 2.55 6.92 56.37 ## X16 15584 15247 1.88 4.00 198.89 ## X17 18236 17909 1.91 4.09 109.04 ## X18 18731 18379 1.99 4.21 69.57 ## X19 18477 18151 1.92 3.36 78.50 ## X110 18729 18403 2.58 7.18 47.32 ## X111 17995 17499 2.04 4.85 182.48 ## X112 18686 18329 2.07 5.34 106.22 ## X113 18777 18420 1.73 2.88 81.39 ## X114 18791 18449 1.46 1.65 83.10 ## X115 18780 18372 1.92 3.25 60.89 ## X116 18574 18205 1.80 3.01 203.70 ## X117 18788 18394 1.63 2.80 125.45 ## X118 18818 18464 1.56 2.17 80.53 ## X119 18741 18359 1.28 0.89 80.57 ## X120 18806 18445 1.59 1.93 57.33 ## X121 18565 17906 1.46 1.58 223.27 ## X122 18640 18272 1.58 2.25 151.75 ## X123 18803 18466 1.39 1.54 98.01 ## X124 18795 18427 1.13 0.62 91.94 ## X125 18760 18403 1.58 2.19 71.91 ## X126 18242 17507 1.73 2.67 281.98 ## X127 18707 18356 1.16 0.39 202.72 ## X128 18500 18164 1.16 0.43 135.08 ## X129 18823 18489 0.89 -0.26 133.74 ## X130 18779 18431 1.34 0.97 98.47 ## X131 18531 18115 1.54 1.66 371.30 ## X132 18325 17990 1.20 1.16 211.61 ## X133 18430 18094 0.99 0.16 158.83 ## X134 18710 18347 0.74 -0.44 168.47 ## X135 18508 18168 1.20 0.72 149.54 Eigene Funktionen schreiben Standardfehler-Funktion Eine eigene Funktion lässt sich wie folgt schreiben: name &lt;- function(argument){Berechnung} Hier ein Beispiel für den Standardfehler des Stichprobenmittelwerts: \\[s_{\\bar{x}} = \\frac{s}{\\sqrt{n}}\\] std.error &lt;- function(x) { sd(x) / sqrt(length(x)) } sample &lt;- c(1, 1, 2, 2, 4, 6, 9) std.error(sample) == sd(sample) / sqrt(length(sample)) ## [1] TRUE std.error(sample) ## [1] 1.13089 Mehr Beispiele # celsius nach kelvin konvertieren celsius.kelvin &lt;- function(temp) { temp + 273.15 } # fahrenheit nach kelvin konvertieren fahrenheit.celsius &lt;- function(temp) { (temp - 32) * 5 / 9 } # celsius nach fahrenheit konvertieren celsius.fahrenheit &lt;- function(temp) { temp * 9 / 5 + 32 } celsius.kelvin(36) ## [1] 309.15 fahrenheit.celsius(96.8) ## [1] 36 celsius.fahrenheit(36) ## [1] 96.8 Übungen Schreibt eine Funktion, die aus einer gegebenen Zahl ihr Quadrat berechnet Schreibt eine Funktion, die den describeBy-Befehl auf ihre Argumente anwendet und die Matrix als data frame ausgibt Schreibt eine Funktion, die den Mittelwert ihres Arguments berechnet (ohne mean!) \\[\\bar{x}= {\\frac {1}{n}}\\sum _{i=1}^{n}a_{i}={\\frac {a_{1}+a_{2}+\\cdots +a_{n}}{n}}\\] Schreibt eine Funktion, die das erste und das letzte Element eines Vektors ausgibt Schreibt eine Funktion, die den Mittelwert, die Standardabweichung und den Median eines Vektors berechnet und als Vektor ausgibt. Lösungen Schreibt eine funktion, die aus einer gegebenen Zahl ihr Quadrat berechnet my.square &lt;- function(n) { n * n } my.squaretwo &lt;- function(n) { n^2 } my.square(10) ## [1] 100 my.squaretwo(10) ## [1] 100 Lösungen Schreibt eine Funktion, die den describeBy-Befehl auf ihre Argumente anwendet und die Matrix als data frame ausgibt describeByDF &lt;- function(x, y) { d &lt;- describeBy(x, y, mat = T) d &lt;- as.data.frame(d) d } descrip &lt;- describeByDF(diamonds$price, list(diamonds$cut, diamonds$color)) is.data.frame(descrip) ## [1] TRUE head(descrip) ## item group1 group2 vars n mean sd median trimmed mad ## X11 1 Fair D 1 163 4291.061 3286.114 3730.0 3703.962 2043.023 ## X12 2 Good D 1 662 3405.382 3175.149 2728.5 2859.296 2674.610 ## X13 3 Very Good D 1 1513 3470.467 3523.753 2310.0 2774.014 2373.643 ## X14 4 Premium D 1 1603 3631.293 3711.634 2009.0 2880.995 1893.280 ## X15 5 Ideal D 1 2834 2629.095 3001.070 1576.0 1929.827 1184.597 ## X16 6 Fair E 1 224 3682.312 2976.652 2956.0 3164.344 2311.373 ## min max range skew kurtosis se ## X11 536 16386 15850 1.902602 3.810324 257.38833 ## X12 361 18468 18107 2.033159 5.424658 123.40566 ## X13 357 18542 18185 1.914574 3.923597 90.59120 ## X14 367 18575 18208 1.782423 2.964122 92.70398 ## X15 367 18693 18326 2.554160 6.920157 56.37365 ## X16 337 15584 15247 1.876280 4.002687 198.88590 Lösungen Schreibt eine Funktion, die den Mittelwert ihres Arguments berechnet (ohne mean!) mein.mittelwert &lt;- function(x) { sum(x) / length(x) } mein.mittelwert(sample) == mean(sample) ## [1] TRUE mein.mittelwert(sample) ## [1] 3.571429 Lösungen Schreibt eine Funktion, die das erste und das letzte Element eines Vektors ausgibt sample ## [1] 1 1 2 2 4 6 9 my.ends &lt;- function(vector) { answer &lt;- c(vector[1], vector[length(vector)]) answer } my.ends2 &lt;- function(x) { c(head(x, 1), tail(x, 1)) } a &lt;- 1:15 my.ends2(sample) ## [1] 1 9 my.ends(sample) ## [1] 1 9 Lösungen Schreibt eine Funktion, die den Mittelwert, die Standardabweichung und den Median eines Vektors berechnet und als Vektor ausgibt. my.desc &lt;- function(vector) { mean &lt;- mean(vector) sd &lt;- sd(vector) median &lt;- median(vector) desc &lt;- c(mean, sd, median) desc } my.desc(sample) ## [1] 3.571429 2.992053 2.000000 standard &lt;- c(mean(sample), sd(sample), median(sample)) my.desc(sample) == standard ## [1] TRUE TRUE TRUE my.desc2 &lt;- function(vector) { a &lt;- c(mean(vector), sd(vector), median(vector)) a } my.desc2(sample) == my.desc(sample) ## [1] TRUE TRUE TRUE Tabellen transponieren reshape2 install.packages(&quot;reshape2&quot;) library(reshape2) melt() # wide zu long cast() # long zu wide Von Wide zu Long Einlesen der Datei br &lt;- read.table(&quot;docs/data/tut6/Briefe.csv&quot;, header = T, sep = &quot;;&quot;, dec = &quot;,&quot;) head(br) ## Filename Segment WC WPS Sixltr Pronoun I We ## 1 m-Rollet_Alexander-BR-1844_1906.txt 1 500 19.23 32.4 4.0 0.0 0.4 ## 2 m-Rollet_Alexander-BR-1844_1906.txt 2 500 18.52 32.8 7.6 3.6 0.6 ## 3 m-Rollet_Alexander-BR-1844_1906.txt 3 500 41.67 28.8 9.2 0.0 5.6 ## 4 m-Rollet_Alexander-BR-1844_1906.txt 4 500 31.25 36.2 5.8 0.0 1.2 ## 5 m-Rollet_Alexander-BR-1844_1906.txt 5 500 23.81 34.8 5.0 0.0 0.6 ## 6 m-Rollet_Alexander-BR-1844_1906.txt 6 500 25.00 36.0 8.4 4.8 0.6 ## Self You ## 1 0.4 2.2 ## 2 4.2 1.6 ## 3 5.6 0.8 ## 4 1.2 1.8 ## 5 0.6 2.2 ## 6 5.4 1.6 Von Wide zu Long Transponieren der Tabelle ins Long-Format: brlong &lt;- melt(br, id.vars = c(&quot;Filename&quot;, &quot;Segment&quot;)) head(brlong, 15) ## Filename Segment variable value ## 1 m-Rollet_Alexander-BR-1844_1906.txt 1 WC 500 ## 2 m-Rollet_Alexander-BR-1844_1906.txt 2 WC 500 ## 3 m-Rollet_Alexander-BR-1844_1906.txt 3 WC 500 ## 4 m-Rollet_Alexander-BR-1844_1906.txt 4 WC 500 ## 5 m-Rollet_Alexander-BR-1844_1906.txt 5 WC 500 ## 6 m-Rollet_Alexander-BR-1844_1906.txt 6 WC 500 ## 7 m-Rollet_Alexander-BR-1844_1906.txt 7 WC 500 ## 8 m-Rollet_Alexander-BR-1844_1906.txt 8 WC 500 ## 9 m-Rollet_Alexander-BR-1844_1906.txt 9 WC 500 ## 10 m-Rollet_Alexander-BR-1844_1906.txt 10 WC 500 ## 11 m-Rollet_Alexander-BR-1844_1906.txt 11 WC 500 ## 12 m-Rollet_Alexander-BR-1844_1906.txt 12 WC 500 ## 13 m-Rollet_Alexander-BR-1844_1906.txt 13 WC 500 ## 14 m-Rollet_Alexander-BR-1844_1906.txt 14 WC 500 ## 15 m-Rollet_Alexander-BR-1844_1906.txt 15 WC 500 Von Long zu Wide brwide &lt;- dcast(brlong, Filename + Segment ~ variable, value.var = &quot;value&quot;) head(brwide) ## Filename Segment WC WPS Sixltr Pronoun I We ## 1 f-Bachmann_Ingeborg-BR-1948_61.txt 1 500 13.16 14.6 16.2 6.8 0.6 ## 2 f-Bachmann_Ingeborg-BR-1948_61.txt 2 500 17.86 14.4 18.8 8.4 0.6 ## 3 f-Bachmann_Ingeborg-BR-1948_61.txt 3 500 15.15 14.0 20.2 8.2 2.2 ## 4 f-Bachmann_Ingeborg-BR-1948_61.txt 4 500 18.52 15.0 18.4 8.6 0.8 ## 5 f-Bachmann_Ingeborg-BR-1948_61.txt 5 500 19.23 12.8 15.2 8.8 0.8 ## 6 f-Bachmann_Ingeborg-BR-1948_61.txt 6 500 16.13 16.8 17.2 9.8 0.8 ## Self You ## 1 7.4 5.8 ## 2 9.0 5.6 ## 3 10.4 6.4 ## 4 9.4 6.0 ## 5 9.6 3.4 ## 6 10.6 3.8 Übungen II Lest die Datei “implicated_lies.csv” ein Entscheidet, ob die Datensortierung long oder wide ist und transponiert die Daten ins jeweils andere Format Speichert die Daten auf eurem Computer ab Benutzt describeBy, um statistische Berechnungen für jedes Item separat zu bekommen Lösungen II Lest die Datei “implicated_lies.csv” ein dlies &lt;- read.csv(&quot;docs/data/tut6/implicated_lies.csv&quot;, header = T) head(dlies) ## id age stage af1 af2 af3 af4 af5 af6 aw1 aw2 aw3 aw4 ## 1 0kcc8m19m6e46i2ojjomvjre91 53 adult 1 1 1 1 1 1 5 5 5 5 ## 2 4fg5ka3ppabgnnfd1k65ei6ar5 25 adult 1 1 1 1 1 1 5 5 5 5 ## 3 77njs4ij7pstnrbe7d7eao33j5 24 adult 5 1 1 1 1 1 5 5 5 5 ## 4 fauj5blp14kgovr221aac53p64 23 adult 1 1 1 1 1 1 5 5 5 5 ## 5 ubqoiq93qbg3fcarse9e54qmi0 26 adult 1 1 5 1 1 1 5 5 5 5 ## 6 398ovonqku33iadqkoorvju9s0 20 adult 1 1 1 1 1 1 5 5 5 5 ## aw5 aw6 gf1s gf2s gf3s gf4n gf5n gf6n gw1s gw2s gw3s gw4n gw5n gw6n pf1 pf2 ## 1 5 5 2 1 1 5 5 1 5 5 5 5 5 5 5 5 ## 2 5 5 3 5 1 5 1 2 5 5 5 5 5 5 5 1 ## 3 5 5 1 4 4 4 4 1 5 5 5 5 5 5 4 2 ## 4 5 5 1 1 1 1 4 2 5 5 4 5 5 5 1 1 ## 5 5 5 1 1 1 4 2 1 2 5 5 5 5 5 2 4 ## 6 4 5 2 2 2 3 4 3 5 NA 5 5 5 5 3 2 ## pf3 pf4 pf5 pf6 pw1 pw2 pw3 pw4 pw5 pw6 ## 1 5 5 2 5 5 2 5 5 2 2 ## 2 5 1 3 5 5 5 5 5 4 5 ## 3 4 3 2 3 5 5 5 5 4 5 ## 4 1 1 1 4 5 5 5 5 4 4 ## 5 5 1 2 4 5 5 5 3 5 5 ## 6 2 3 3 3 4 5 4 3 4 4 Lösungen II Entscheidet, ob die Datensortierung long oder wide ist und transponiert die Daten ins jeweils andere Format dlieslong &lt;- melt(dlies, id.vars = c(&quot;id&quot;, &quot;stage&quot;, &quot;age&quot;), value.name = &quot;judgment&quot;, variable.name = &quot;item&quot;) # sorting: dlieslong sorted first by the id column and then, # provided id stays constant, by item dlieslong &lt;- dlieslong[order(dlieslong$id, dlieslong$item), ] head(dlieslong, 8) ## id stage age item judgment ## 21 02dfkhe0rsj9q8abs2skfci9h0 adult 31 af1 1 ## 123 02dfkhe0rsj9q8abs2skfci9h0 adult 31 af2 1 ## 225 02dfkhe0rsj9q8abs2skfci9h0 adult 31 af3 1 ## 327 02dfkhe0rsj9q8abs2skfci9h0 adult 31 af4 1 ## 429 02dfkhe0rsj9q8abs2skfci9h0 adult 31 af5 1 ## 531 02dfkhe0rsj9q8abs2skfci9h0 adult 31 af6 1 ## 633 02dfkhe0rsj9q8abs2skfci9h0 adult 31 aw1 5 ## 735 02dfkhe0rsj9q8abs2skfci9h0 adult 31 aw2 5 Lösungen Speichert die Daten auf eurem Computer ab write.csv(dlieslong, file = &quot;implicated_lies_long.csv&quot;, row.names = F, quote = F, na = &quot;keine Angabe&quot;) Lösungen Benutzt describeBy, um statistische Berechnungen für jedes Item separat zu bekommen describeBy(dlieslong$judgment, dlieslong$item, mat = T, digits = 1) ## item group1 vars n mean sd median trimmed mad min max range skew ## X11 1 af1 1 101 2.0 1.5 1 1.7 0.0 1 5 4 1.3 ## X12 2 af2 1 100 1.2 0.8 1 1.0 0.0 1 5 4 4.4 ## X13 3 af3 1 100 1.1 0.6 1 1.0 0.0 1 5 4 6.2 ## X14 4 af4 1 101 1.1 0.4 1 1.0 0.0 1 4 3 5.3 ## X15 5 af5 1 101 1.1 0.6 1 1.0 0.0 1 5 4 5.8 ## X16 6 af6 1 101 1.1 0.3 1 1.0 0.0 1 2 1 3.3 ## X17 7 aw1 1 101 4.8 0.8 5 5.0 0.0 1 5 4 -3.7 ## X18 8 aw2 1 100 4.7 0.9 5 5.0 0.0 1 5 4 -3.3 ## X19 9 aw3 1 101 4.9 0.6 5 5.0 0.0 1 5 4 -5.6 ## X110 10 aw4 1 99 4.8 0.8 5 5.0 0.0 1 5 4 -3.6 ## X111 11 aw5 1 99 4.8 0.6 5 5.0 0.0 1 5 4 -5.0 ## X112 12 aw6 1 101 4.8 0.8 5 5.0 0.0 1 5 4 -3.9 ## X113 13 gf1s 1 33 1.6 1.0 1 1.4 0.0 1 5 4 1.7 ## X114 14 gf2s 1 33 1.8 1.2 1 1.6 0.0 1 5 4 1.4 ## X115 15 gf3s 1 33 2.1 1.3 2 1.9 1.5 1 5 4 0.9 ## X116 16 gf4n 1 33 3.6 1.5 4 3.7 1.5 1 5 4 -0.6 ## X117 17 gf5n 1 33 2.7 1.3 3 2.6 1.5 1 5 4 0.2 ## X118 18 gf6n 1 33 1.9 1.4 1 1.7 0.0 1 5 4 1.2 ## X119 19 gw1s 1 33 4.6 0.8 5 4.7 0.0 2 5 3 -1.7 ## X120 20 gw2s 1 32 5.0 0.2 5 5.0 0.0 4 5 1 -5.1 ## X121 21 gw3s 1 33 4.8 0.4 5 4.8 0.0 4 5 1 -1.1 ## X122 22 gw4n 1 33 5.0 0.0 5 5.0 0.0 5 5 0 NaN ## X123 23 gw5n 1 33 4.8 0.9 5 5.0 0.0 1 5 4 -3.6 ## X124 24 gw6n 1 33 4.7 0.8 5 4.9 0.0 1 5 4 -3.7 ## X125 25 pf1 1 101 2.9 1.5 3 2.9 3.0 1 5 4 0.1 ## X126 26 pf2 1 101 3.2 1.5 3 3.3 3.0 1 5 4 -0.1 ## X127 27 pf3 1 101 2.1 1.3 2 1.9 1.5 1 5 4 1.1 ## X128 28 pf4 1 102 2.6 1.6 2 2.5 1.5 1 5 4 0.5 ## X129 29 pf5 1 99 2.3 1.3 2 2.2 1.5 1 5 4 0.6 ## X130 30 pf6 1 102 3.4 1.4 3 3.5 1.5 1 5 4 -0.3 ## X131 31 pw1 1 100 4.6 0.9 5 4.8 0.0 1 5 4 -2.4 ## X132 32 pw2 1 100 4.9 0.5 5 5.0 0.0 2 5 3 -4.4 ## X133 33 pw3 1 102 4.7 0.8 5 4.9 0.0 1 5 4 -3.5 ## X134 34 pw4 1 99 4.8 0.7 5 4.9 0.0 2 5 3 -3.0 ## X135 35 pw5 1 100 4.5 0.9 5 4.8 0.0 1 5 4 -2.2 ## X136 36 pw6 1 101 4.6 0.8 5 4.8 0.0 1 5 4 -2.7 ## kurtosis se ## X11 0.0 0.2 ## X12 18.4 0.1 ## X13 38.8 0.1 ## X14 30.2 0.0 ## X15 33.5 0.1 ## X16 9.3 0.0 ## X17 13.3 0.1 ## X18 9.6 0.1 ## X19 32.9 0.1 ## X110 12.2 0.1 ## X111 26.3 0.1 ## X112 13.9 0.1 ## X113 2.5 0.2 ## X114 0.8 0.2 ## X115 -0.6 0.2 ## X116 -1.2 0.3 ## X117 -1.3 0.2 ## X118 0.0 0.2 ## X119 1.9 0.1 ## X120 25.2 0.0 ## X121 -0.7 0.1 ## X122 NaN 0.0 ## X123 11.9 0.1 ## X124 14.6 0.1 ## X125 -1.4 0.2 ## X126 -1.5 0.2 ## X127 -0.1 0.1 ## X128 -1.3 0.2 ## X129 -0.8 0.1 ## X130 -1.3 0.1 ## X131 5.3 0.1 ## X132 20.1 0.1 ## X133 12.2 0.1 ## X134 8.6 0.1 ## X135 4.4 0.1 ## X136 7.4 0.1 Nur Kurz: Plots Ein Plot zum Abschluss # boxplot der letzten vier spalten in briefe.csv p1 &lt;- boxplot(br[, 7:length(br)], main = &quot;Briefdatensatz&quot;, xlab = &quot;Spaltenname&quot;, ylab = &quot;Wert&quot;) Ein Plot zum Abschluss library(ggplot2) p2 &lt;- ggplot(br, aes(y = I)) + geom_boxplot() + labs(title = &quot;Briefdatensatz&quot;, x = &quot;I&quot;, y = &quot;Wert&quot;) Ein Plot zum Abschluss p2 F*ck die Uni Schönes Wochenende! "],
["sitzung-7.html", "Sitzung 7 Wiederholung Fehlertypen \\(\\chi^2\\)-Test \\(t\\)-test Nur Kurz: Plots", " Sitzung 7 Wiederholung Das Wichtigste vom letzten Mal Pakete installieren und laden install.packages(&quot;PAKETNAME&quot;) library(PAKETNAME) Deskriptive Statistik library(psych) describeBy(d$judgment, list(d$wordorder, d$quantifier), digits = 2, mat = T) Das Wichtigste vom letzten Mal Funktionen schreiben my.ends &lt;- function(vector) { answer &lt;- c(vector[1], vector[length(vector)]) answer } Daten transponieren library(reshape2) cast(id + age + gender ~ condition, value.var = &quot;judgment&quot;) # long zu wide melt(d, id.vars = c(&quot;id&quot;, &quot;age&quot;, &quot;gender&quot;)) # wide zu long Fehlertypen Fehlertypen: Erinnerungshilfe \\(\\chi^2\\)-Test Daten Übersicht zahlreicher Metriken von Studierenden an irgendeiner Hochschule. Von der Website: d &lt;- read.csv(&quot;docs/data/tut7/studentsmajor.csv&quot;) head(d, 3) ## stud.id name gender age height weight religion nc.score ## 1 833917 Gonzales, Christina Female 19 160 64.8 Muslim 1.91 ## 2 898539 Lozano, T&#39;Hani Female 19 172 73.0 Other 1.56 ## 3 379678 Williams, Hanh Female 22 168 70.6 Protestant 1.24 ## semester major minor score1 score2 ## 1 1st Political Science Social Sciences NA NA ## 2 2nd Social Sciences Mathematics and Statistics NA NA ## 3 3rd Social Sciences Mathematics and Statistics 45 46 ## online.tutorial graduated salary ## 1 0 0 NA ## 2 0 0 NA ## 3 0 0 NA Daten Anforderungen des \\(\\chi^2\\)-Tests: Kategoriale Daten Unabhängige Beobachtungen str(d) ## &#39;data.frame&#39;: 8239 obs. of 16 variables: ## $ stud.id : int 833917 898539 379678 807564 383291 256074 754591 146494 723584 314281 ... ## $ name : Factor w/ 8174 levels &quot;Aarvold, Cindi&quot;,..: 2480 4196 7858 5109 5770 5592 1258 162 7221 5240 ... ## $ gender : Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 1 1 1 2 1 2 1 1 2 1 ... ## $ age : int 19 19 22 19 21 19 21 21 18 18 ... ## $ height : int 160 172 168 183 175 189 156 167 195 165 ... ## $ weight : num 64.8 73 70.6 79.7 71.4 85.8 65.9 65.7 94.4 66 ... ## $ religion : Factor w/ 5 levels &quot;Catholic&quot;,&quot;Muslim&quot;,..: 2 4 5 4 1 1 5 4 4 3 ... ## $ nc.score : num 1.91 1.56 1.24 1.37 1.46 1.34 1.11 2.03 1.29 1.19 ... ## $ semester : Factor w/ 7 levels &quot;&gt;6th&quot;,&quot;1st&quot;,&quot;2nd&quot;,..: 2 3 4 3 2 3 3 4 4 3 ... ## $ major : Factor w/ 6 levels &quot;Biology&quot;,&quot;Economics and Finance&quot;,..: 5 6 6 3 3 5 5 5 2 3 ... ## $ minor : Factor w/ 6 levels &quot;Biology&quot;,&quot;Economics and Finance&quot;,..: 6 4 4 4 4 4 6 2 3 4 ... ## $ score1 : int NA NA 45 NA NA NA NA 58 57 NA ... ## $ score2 : int NA NA 46 NA NA NA NA 62 67 NA ... ## $ online.tutorial: int 0 0 0 0 0 0 0 0 0 0 ... ## $ graduated : int 0 0 0 0 0 0 0 0 0 0 ... ## $ salary : num NA NA NA NA NA NA NA NA NA NA ... Häufigkeitstabelle Kategoriale Variablen aufeinander abbilden tab &lt;- table(d$major, d$gender) tab ## ## Female Male ## Biology 959 638 ## Economics and Finance 461 863 ## Environmental Sciences 745 881 ## Mathematics and Statistics 276 949 ## Political Science 978 477 ## Social Sciences 691 321 Relative Häufigkeiten prop.table(tab, margin = 1) * 100 ## ## Female Male ## Biology 60.05009 39.94991 ## Economics and Finance 34.81873 65.18127 ## Environmental Sciences 45.81796 54.18204 ## Mathematics and Statistics 22.53061 77.46939 ## Political Science 67.21649 32.78351 ## Social Sciences 68.28063 31.71937 Grafische Übersicht plot(tab) \\(\\chi^2\\) berechnen options(scipen = 999) chisq.test(d$gender, d$major) ## ## Pearson&#39;s Chi-squared test ## ## data: d$gender and d$major ## X-squared = 875.44, df = 5, p-value &lt; 0.00000000000000022 P-Werte P-Werte Übungen Erstellt eine Tabelle, die das Studienfach nach Religionszugehörigkeit auszählt (einmal als absolute Häufigkeiten, einmal in Prozent) Plottet die Tabelle Berechnet den dazugehörigen \\(\\chi^2\\)-Test Lösungen Erstellt eine Tabelle, die das Studienfach nach Religionszugehörigkeit auszählt (einmal als absolute Häufigkeiten, einmal in Prozent) tabrelm &lt;- table(d$major, d$religion) prop.table(tabrelm, margin = 1) * 100 ## ## Catholic Muslim Orthodox Other Protestant ## Biology 34.126487 4.320601 6.261741 33.437696 21.853475 ## Economics and Finance 34.969789 3.172205 6.570997 32.477341 22.809668 ## Environmental Sciences 34.255843 4.489545 8.118081 31.303813 21.832718 ## Mathematics and Statistics 35.836735 3.265306 6.775510 31.183673 22.938776 ## Political Science 32.164948 4.536082 7.285223 32.371134 23.642612 ## Social Sciences 32.114625 3.952569 7.608696 35.770751 20.553360 Lösungen Plottet die Tabelle plot(tabrelm) Lösungen Berechnet den dazugehörigen \\(\\chi^2\\)-Test chisq.test(d$religion, d$major) ## ## Pearson&#39;s Chi-squared test ## ## data: d$religion and d$major ## X-squared = 23.881, df = 20, p-value = 0.2476 Übungen II Erstellt ein Subset mit den drei größten Religionen Erstellt eine Tabelle, die die Religionszugehörigkeit nach Geschlecht auszählt (einmal als absolute Häufigkeiten, einmal in Prozent) Plottet die Tabelle Berechnet den dazugehörigen \\(\\chi^2\\)-Test Lösungen II Erstellt ein Subset mit den drei größten Religionen dsub &lt;- subset(d, religion %in% c(&quot;Catholic&quot;, &quot;Other&quot;, &quot;Protestant&quot;)) # alternative_1: dsub &lt;- d[d$religion %in% c(”Catholic”, ”Other”, ”Protestant”), ] # alternative_2: dsub &lt;- d[d$religion == “Catholic” | d$religion == “Other” | d$religion == “Protestant”, ] # alternative: dsub$religion &lt;- factor(dsub$religion ) dsub &lt;- droplevels(dsub) Zur Erinnerung: droplevels a &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;) a &lt;- factor(a) a ## [1] a b c ## Levels: a b c a &lt;- a[1:2] a ## [1] a b ## Levels: a b c a &lt;- droplevels(a) a ## [1] a b ## Levels: a b Lösungen II Erstellt eine Tabelle, die die Religionszugehörigkeit nach Geschlecht auszählt (einmal als absolute Häufigkeiten, einmal in Prozent) tabrelg &lt;- table(dsub$religion, dsub$gender) prop.table(tabrelg, margin = 1) * 100 ## ## Female Male ## Catholic 48.48051 51.51949 ## Other 50.55804 49.44196 ## Protestant 49.48341 50.51659 Lösungen II Plottet die Tabelle plot(tabrelg) Lösungen II Berechnet den dazugehörigen \\(\\chi^2\\)-Test chisq.test(dsub$religion, dsub$gender) ## ## Pearson&#39;s Chi-squared test ## ## data: dsub$religion and dsub$gender ## X-squared = 2.3668, df = 2, p-value = 0.3062 \\(t\\)-test Zwei Arten t.test(WerteGruppe1, WerteGruppe2, paired = TRUE / FALSE, var.equal = TRUE) # var.equal = F = Welch&#39;s Test Alternative: t.test(AV ~ Gruppenvariable, data = d, paired = TRUE / FALSE, var.equal = TRUE) paired = TRUE Abhängige Beobachtungen/Stichproben: Oft Messwiederholung: within-subjects paired = FALSE Unabhängige Beobachtungen/Stichproben: between-subjects Voraussetzungen für die Anwendung des indepedent t-tests: Unabhängige Variable sollte aus zwei kategorialverteilten Gruppen bestehen Die Beobachtungen sollten unabhängig sein Die abhängige Variable sollte innerhalb der beiden Gruppen ungefähr normalverteilt sein Varianzhomogenität Interlude: Warum Normalverteilung? Interlude: Warum Varianzhomogenität/Homoskedastizität? Übung III Lese die Daten, die in “student-mat.csv” enthalten, sind in R ein. Die relevanten Spalten, die wir uns im Folgenden genauer angucken wollen, sind “sex” und “G3”. Erstelle ein entsprechendes Subset der Daten und verschaffe dir einen Überblick. Überprüfe/Begründe, ob die folgenden Voraussetzungen gelten: Unabhängige Variable sollte aus zwei kategorialverteilten Gruppen bestehen Die Beobachtungen sollten unabhängig sein Die abhängige Variable sollte innerhalb der beiden Gruppen ungefähr normalverteilt sein Varianzhomogenität Lösungen III Lese die Daten, die in “student-mat.csv” enthalten, sind in R ein, erstelle ein Subset und verschaffe dir einen Überblick. d &lt;- read.csv(&quot;docs/data/tut7/student-mat.csv&quot;) dsub &lt;- subset(d, select = c(&quot;sex&quot;, &quot;G3&quot;)) head(dsub) ## sex G3 ## 1 F 6 ## 2 F 6 ## 3 F 10 ## 4 F 15 ## 5 F 10 ## 6 M 15 summary(dsub) ## sex G3 ## F:208 Min. : 0.00 ## M:187 1st Qu.: 8.00 ## Median :11.00 ## Mean :10.42 ## 3rd Qu.:14.00 ## Max. :20.00 Lösungen III Überprüfe/Begründe, ob die folgenden Voraussetzungen gelten: Unabhängige Variable sollte aus zwei kategorialverteilten Gruppen bestehen length(levels(dsub$sex)) == 2 ## [1] TRUE Die Beobachtungen sollten unabhängig sein Sex, so nehmen wir zumindest für die Zwecke dieser Übung an, ist binär- und disjunktverteilt. Die Anforderung ist also erfüllt, weil kein Schüler sowohl der Männer- als auch der Frauengruppe angehört. Lösungen III Die abhängige Variable sollte innerhalb der beiden Gruppen ungefähr normalverteilt sein Histogramm hist(dsub$G3, breaks = 20) Lösungen III zu viele 0-Ergebnisse. Die sollten wir rausnehmen dsub &lt;- dsub[dsub$G3 != 0, ] hist(dsub$G3, breaks = 20) Lösungen III Varianzhomogenität install.packages(&quot;car&quot;) library(car) leveneTest(G3 ~ sex, data = dsub) ## Levene&#39;s Test for Homogeneity of Variance (center = median) ## Df F value Pr(&gt;F) ## group 1 0.6145 0.4336 ## 355 Übung IV Berechnet den \\(t\\)-test Lösung IV Berechnet den \\(t\\)-test t.test(dsub$G3[dsub$sex == &quot;F&quot;], dsub$G3[dsub$sex == &quot;M&quot;], paired = F, var.equal = TRUE) Alternative Syntax: t.test(G3 ~ sex, data = dsub, paired = F, var.equal = TRUE) ## ## Two Sample t-test ## ## data: G3 by sex ## t = -1.9405, df = 355, p-value = 0.05311 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -1.330667531 0.008920202 ## sample estimates: ## mean in group F mean in group M ## 11.20541 11.86628 Übung V Lest die Datei “sleep.txt” ein und verschafft euch einen Überblick. Es geht hierbei um Folgendes: “Data which show the effect of two soporific drugs (increase in hours of sleep compared to control) on 10 patients.” Berechnet den entsprechenden \\(t\\)-Test Lösungen V Lest die Datei “sleep.txt” ein und verschafft euch einen Überblick. d &lt;- read.table(&quot;docs/data/tut7/sleep.txt&quot;, sep = &quot;|&quot;, header = T) head(d) ## extra group ID ## 1 0.7 1 1 ## 2 -1.6 1 2 ## 3 -0.2 1 3 ## 4 -1.2 1 4 ## 5 -0.1 1 5 ## 6 3.4 1 6 str(d) ## &#39;data.frame&#39;: 20 obs. of 3 variables: ## $ extra: num 0.7 -1.6 -0.2 -1.2 -0.1 3.4 3.7 0.8 0 2 ... ## $ group: int 1 1 1 1 1 1 1 1 1 1 ... ## $ ID : int 1 2 3 4 5 6 7 8 9 10 ... Lösungen V Berechnet den entsprechenden \\(t\\)-Test t.test(d$extra[d$group == &quot;1&quot;], d$extra[d$group == &quot;2&quot;], paired = T, var.equal = TRUE) t.test(extra ~ group, data = d, paired = T, var.equal = TRUE) ## ## Paired t-test ## ## data: extra by group ## t = -4.0621, df = 9, p-value = 0.002833 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -2.4598858 -0.7001142 ## sample estimates: ## mean of the differences ## -1.58 Lösungen V Berechnet den entsprechenden \\(t\\)-Test Hier habe ich die Alternativhypothese explizit angegeben. Erinnert ihr euch noch daran, was Thomas euch dazu erzählt hat? t.test(d$extra[d$group == &quot;1&quot;], d$extra[d$group == &quot;2&quot;], paired = T, alternative = &quot;less&quot;, var.equal = TRUE) ## ## Paired t-test ## ## data: d$extra[d$group == &quot;1&quot;] and d$extra[d$group == &quot;2&quot;] ## t = -4.0621, df = 9, p-value = 0.001416 ## alternative hypothesis: true difference in means is less than 0 ## 95 percent confidence interval: ## -Inf -0.8669947 ## sample estimates: ## mean of the differences ## -1.58 alternative = less boxplot(d$extra ~ as.factor(d$group)) Übungen VI Lest die Datei “grammar.dat” ein und verschafft euch einen Überblick. Es handelt sich hierbei um (ausdachte) Daten eines (ebenfalls ausgedachten) Experiments, in dem 10 Studierende zweimal an einem Grammatiktest teilgenommen haben. Einmal ohne Training und einmal, zu einem späteren Zeitpunkt, mit vorangestelltem Grammatikunterricht. Wie müsste der \\(t\\)-Test aussehen, der die Ergebnisse dieses Experiments inferenzstatistisch überprüfen soll? Lösungen VI Lest die Datei “grammar.dat” ein und verschafft euch einen Überblick. d &lt;- read.table(&quot;docs/data/tut7/grammar.dat&quot;, sep = &quot;;&quot;, header = T) boxplot(d[, 1:2]) Lösungen VI Wie müsste der \\(t\\)-Test aussehen, der die Ergebnisse dieses Experiments inferenzstatistisch überprüfen soll? t.test(d$before, d$after, alternative = &quot;less&quot;, paired = T, var.equal = TRUE) ## ## Paired t-test ## ## data: d$before and d$after ## t = -4.4853, df = 9, p-value = 0.0007604 ## alternative hypothesis: true difference in means is less than 0 ## 95 percent confidence interval: ## -Inf -4.493909 ## sample estimates: ## mean of the differences ## -7.6 Nur Kurz: Plots Ein Plot zum Abschluss library(ggplot2) library(reshape2) dlong &lt;- melt(d, id.vars = &quot;id&quot;) dlong$id &lt;- factor(dlong$id) p &lt;- ggplot(dlong, aes(y = value, x = variable, group = id)) + geom_line() + geom_point() + stat_summary(fun.y = mean, geom = &quot;line&quot;, color = &quot;red&quot;, linetype = &quot;dashed&quot;, mapping = aes(x = variable, y = value, group = 1)) + stat_summary(fun.y = mean, geom = &quot;point&quot;, color = &quot;red&quot;, shape = 18, size = 3, mapping = aes(x = variable, y = value, group = 1)) + labs(y = &quot;Scores&quot;, x = &quot;Test&quot;) + cleanup Ein Plot zum Abschluss # mittelwert in rot und gestrichelt p F*ck die Uni Schönes Wochenende! "],
["sitzung-8.html", "Sitzung 8 Wiederholung Base R: ANOVA Alternative ANOVA-Befehle Nur Kurz: Plots", " Sitzung 8 Wiederholung Das Wichtigste vom letzten Mal \\(\\chi^2\\)-Test chisq.test(x, y) t-Test t.test(x, y, paired = FALSE, var.equal = T, data = d) t.test(x, y, paired = TRUE, var.equal = T, data = d) t.test(x, y, paired = TRUE, var.equal = T, alternative = &quot;less&quot;, data = d) # &quot;greater&quot;, &quot;two.sided&quot; Alternative Syntax (lineares Modell): t.test(AV ~ Gruppe, paired = FALSE / TRUE, var.equal = T, data = d) Base R: ANOVA Meme My Stats Einleitendes Die Varianzanalyse in R kann auf viele verschiedene Arten durchgeführt werden. Einige davon sind in base R enthalten, andere (idR komfortablere) werden durch externe Pakete zur Verfügung gestellt. Ich zeige euch heute drei verschiedene davon: aov() aov_ez() ezAnova() Interlude: between und within Beantwortet die unteren Fragen und gebt jeweils Beispiele (egal ob linguistischer Art oder nicht) Was bedeutet es, wenn ein Faktor within-subjects ist? Was ist mit between-subjects? Wie steht’s mit within-items? Und mit between-items? Interlude: between und within Beantwortet die unteren Fragen und gebt jeweils Beispiele (egal ob linguistischer Art oder nicht) Was bedeutet es, wenn ein Faktor within-subjects ist? Messung von Gedächtnisleistung: Jede Person einmal ausgeruht und einmal (an einem anderen Termin) nach einer durchzechten Nacht (außerdem Messwiederholung) Was ist mit between-subjects? Geschlecht; Placebo vs richtiges Medikament Wie steht’s mit within-items? Item immer in zwei Ausführungen: Einmal mit Scrambling, einmal ohne Und mit between-items? Itemset ist geteilt in Äußerungen mit Implikatur und Äußerungen ohne aov() Between-Design summary(aov(AV ~ UVbetween, d)) Within-Design summary(aov(AV ~ UVwithin + Error(ID - Variable / UVwithin), d)) Faktorinteraktion mit \"*\" summary(aov(AV ~ UV1 * UV2, d)) Messwiederholung (within-Design) summary(aov(AV ~ UV + Error(ID - Variable / UVwithin), d)) Messwiederholung (between-Design) summary(aov(AV ~ UV + Error(ID - Variable), d)) aov() Mixed-Design mit Messwiederholung summary(aov(AV ~ UVwithin * UVbetween + Error(ID - Variable / UVwithin), d)) Mixed-Design mit Messwiederholung und Interaktion summary(aov(AV ~ UVwithin1 * UVwithin2 * UVbetween + Error(ID - Variable / UVwithin1 * UVwithin2), d)) Übungen II Stellt euch vor, ihr werdet beauftragt folgende Volksweisheit zu testen: “Bier auf Wein, das lass sein”. Was wäre ein geeignetes Experimentdesign und wie müsste die entprechende ANOVA aussehen? Das erste Experiment liefert keine richtigen Ergebnisse. Euch kommt aber in den Sinn, dass der Spruch vielleicht von einem Mann stammt und der Rat daher vielleicht nur auf Männer zutrifft. Wie würdet ihr das testen und wie würde eine dazugehörige ANOVA aussehen? Lösungen II Stellt euch vor, ihr werdet beauftragt folgende Volksweisheit zu testen: “Bier auf Wein, das lass sein”. Was wäre ein geeignetes Experimentdesign und wie müsste die entprechende ANOVA aussehen? aov(AV ~ Reihenfolge + Error(ID - Variable / Reihenfolge), d) Das erste Experiment liefert keine richtigen Ergebnisse. Euch kommt aber in den Sinn, dass der Spruch vielleicht von einem Mann stammt und der Rat daher vielleicht nur auf Männer zutrifft. Wie würdet ihr das testen und wie würde eine dazugehörige ANOVA aussehen? aov(AV ~ Reihenfolge * Geschlecht + Error(ID - Variable / Reihenfolge), d) Übungen III Lest die Datei „LungCapData.csv“ in R ein und verschafft euch einen Überblick über die Daten. Gibt es NAs? Berechnet, ob der Faktor einen signifikaten Einfluss auf die Lungenkapazität hat. Berechnet,ob der Faktor einen signifikaten Einfluss auf die Lungenkapazität hat. Berechnet, ob es eine Interaktion zwischen den Faktoren , für die über 18-Jährigen gibt. Berechnet, ob es eine Interaktion zwischen den Faktoren , und für die über 18-Jährigen gibt. Lösungen III Lest die Datei „LungCapData.csv“ in R ein und verschafft euch einen Überblick über die Daten. d &lt;- read.csv(&quot;docs/data/tut8/LungCapData.csv&quot;, sep = &quot;;&quot;) head(d) ## LungCap Age Height Smoke Gender Caesarean ## 1 6.475 6 62.1 no male no ## 2 10.125 18 74.7 yes female no ## 3 9.550 16 69.7 no female yes ## 4 11.125 14 71.0 no male no ## 5 4.800 5 56.9 no male no ## 6 6.225 11 58.7 no female no summary(d) ## LungCap Age Height Smoke Gender ## Min. : 0.507 Min. : 3.00 Min. :45.30 no :648 female:358 ## 1st Qu.: 6.150 1st Qu.: 9.00 1st Qu.:59.90 yes: 77 male :367 ## Median : 8.000 Median :13.00 Median :65.40 ## Mean : 7.863 Mean :12.33 Mean :64.84 ## 3rd Qu.: 9.800 3rd Qu.:15.00 3rd Qu.:70.30 ## Max. :14.675 Max. :19.00 Max. :81.80 ## Caesarean ## no :561 ## yes:164 ## ## ## ## Lösungen III Gibt es NAs? # prozent NAs pro spalte (apply(is.na(d), 2, sum) / length(d[, 1])) * 100 ## LungCap Age Height Smoke Gender Caesarean ## 0 0 0 0 0 0 # alle zeilen mit NAs d[!complete.cases(d), ] ## [1] LungCap Age Height Smoke Gender Caesarean ## &lt;0 rows&gt; (or 0-length row.names) Lösungen III Berechnet, ob der Faktor einen signifikaten Einfluss auf die Lungenkapazität hat. summary(aov(LungCap ~ Gender, d)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Gender 1 148 147.96 21.47 0.00000426 *** ## Residuals 723 4983 6.89 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Berechnet,ob der Faktor einen signifikaten Einfluss auf die Lungenkapazität hat. summary(aov(LungCap ~ Caesarean, d)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Caesarean 1 2 2.331 0.329 0.567 ## Residuals 723 5128 7.093 Lösungen III Berechnet, ob es eine Interaktion zwischen den Faktoren , für die über 18-Jährigen gibt. dsub &lt;- subset(d, Age &gt;= 18) summary(aov(LungCap ~ Caesarean * Smoke, dsub)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Caesarean 1 0.01 0.010 0.005 0.947 ## Smoke 1 3.77 3.768 1.638 0.205 ## Caesarean:Smoke 1 1.84 1.845 0.802 0.373 ## Residuals 76 174.83 2.300 Berechnet, ob es eine Interaktion zwischen den Faktoren , und für die über 18-Jährigen gibt. summary(aov(LungCap ~ Caesarean * Smoke * Gender, dsub)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Caesarean 1 0.01 0.01 0.005 0.941257 ## Smoke 1 3.77 3.77 1.981 0.163565 ## Gender 1 31.78 31.78 16.710 0.000112 *** ## Caesarean:Smoke 1 1.98 1.98 1.039 0.311355 ## Caesarean:Gender 1 2.73 2.73 1.437 0.234571 ## Smoke:Gender 1 3.04 3.04 1.598 0.210287 ## Caesarean:Smoke:Gender 1 0.23 0.23 0.120 0.730253 ## Residuals 72 136.92 1.90 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Alternative ANOVA-Befehle afex-ANOVA Zur Erinnerung: Die Reihenfolge der positionalen Argumente könnt ihr nur dann ignorieren, wenn ihr sie benennt. Wenn ihr sie nicht benennt, müsst ihr die untere Abfolge einhalten. Allerdings ist euer Code besser nachvollziehbar (für Kollegen, Reviewer, etc.), wenn ihr alle Argumente per Schlüsselwort angegebt (und nicht bloß positional vorgeht). install.packages(&quot;afex&quot;) library(afex) aov_ez(id = &quot;ID-Variable&quot;, dv = &quot;Abhängige Variable&quot;, data = d, within = c(&quot;UVwithin1&quot;, &quot;UVwithin2&quot;), between = c(&quot;UVbetween1&quot;, &quot;UVbetween2&quot;) ) ez-ANOVA install.packages(&quot;ez&quot;) library(ez) ezANOVA(data = d, dv = Abhängige - Variable, wid = ID - Variable, within = .(UVwithin1, UVwithin2), between = .(UVbetween1, UVbetween2)) Übungen IV Jetzt beschäftigen wir uns mit einem Experiment, das im psycholinguistischen Experimentalpraktikum entstanden ist. Das Design ist wie folgt: *: 24 Items, 32 Proband*innen; jede*r Proband*in sah ein Item genau einmal unter einer Bedingung, und alle Bedingungen gleich oft. Zuordnung nach lateinischem Quadrat. Der Klempner wurde ins Gefängnis begleitet, weil er seine Mutter erschlagen hatte. Der Klempner wurde ins Gefängnis begleitet, weil ein Abwasserrohr geplatzt war. Der Klempner wurde zum Gefängnis begleitet, weil er seine Mutter erschlagen hatte. Der Klempner wurde zum Gefängnis begleitet, weil ein Abwasserrohr geplatzt war. Lest die Datei “dzi.dat” ein, verschafft euch einen Überblick und berechnet die deskriptive Statistik und die richtige ANOVA in drei Ausführungen: base R, afex und ez. Lösungen IV Datei einlesen d &lt;- read.table(&quot;docs/data/tut8/psych.dat&quot;, sep = &quot;\\t&quot;, header = T) Überblick verschaffen head(d) ## gender age fach subject variant experiment item condition ## 1 m 19 LADe-EngPhilologie 1 1 1 6 b ## 2 m 19 LADe-EngPhilologie 1 1 1 14 b ## 3 m 19 LADe-EngPhilologie 1 1 1 13 a ## 4 m 19 LADe-EngPhilologie 1 1 1 12 d ## 5 m 19 LADe-EngPhilologie 1 1 1 15 b ## 6 m 19 LADe-EngPhilologie 1 1 1 7 c ## judgement verb gen prep ## 1 7 passive episodic ins ## 2 7 passive episodic ins ## 3 7 passive generic ins ## 4 7 modal episodic zum ## 5 7 passive episodic ins ## 6 7 modal generic zum Lösungen IV summary(d) ## gender age fach subject variant ## m:168 Min. :18.0 De-Gesch :114 Min. : 1.0 Min. :1.00 ## w:426 1st Qu.:19.0 WiPaed : 95 1st Qu.: 8.0 1st Qu.:2.00 ## Median :20.0 LADe-Sport : 54 Median :16.0 Median :4.00 ## Mean :21.4 De : 38 Mean :16.4 Mean :4.47 ## 3rd Qu.:21.0 LADe-Gesch : 27 3rd Qu.:25.0 3rd Qu.:7.00 ## Max. :33.0 De-AllgSprachw: 19 Max. :32.0 Max. :8.00 ## (Other) :247 ## experiment item condition judgement verb ## Min. :1 Min. : 1.00 a:148 Min. :1.000 modal :314 ## 1st Qu.:1 1st Qu.: 5.00 b:150 1st Qu.:5.000 passive:280 ## Median :1 Median :12.00 c:146 Median :7.000 ## Mean :1 Mean :11.54 d:150 Mean :5.944 ## 3rd Qu.:1 3rd Qu.:18.00 3rd Qu.:7.000 ## Max. :1 Max. :23.00 Max. :7.000 ## ## gen prep ## episodic:300 ins:298 ## generic :294 zum:296 ## ## ## ## ## Lösungen IV d[!complete.cases(d), ] ## [1] gender age fach subject variant experiment ## [7] item condition judgement verb gen prep ## &lt;0 rows&gt; (or 0-length row.names) str(d) ## &#39;data.frame&#39;: 594 obs. of 12 variables: ## $ gender : Factor w/ 2 levels &quot;m&quot;,&quot;w&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ age : int 19 19 19 19 19 19 19 19 19 19 ... ## $ fach : Factor w/ 19 levels &quot;De&quot;,&quot;De-AllgSprachw&quot;,..: 11 11 11 11 11 11 11 11 11 11 ... ## $ subject : int 1 1 1 1 1 1 1 1 1 1 ... ## $ variant : int 1 1 1 1 1 1 1 1 1 1 ... ## $ experiment: int 1 1 1 1 1 1 1 1 1 1 ... ## $ item : int 6 14 13 12 15 7 22 11 16 4 ... ## $ condition : Factor w/ 4 levels &quot;a&quot;,&quot;b&quot;,&quot;c&quot;,&quot;d&quot;: 2 2 1 4 2 3 3 3 3 4 ... ## $ judgement : int 7 7 7 7 7 7 7 7 7 7 ... ## $ verb : Factor w/ 2 levels &quot;modal&quot;,&quot;passive&quot;: 2 2 2 1 2 1 2 1 2 1 ... ## $ gen : Factor w/ 2 levels &quot;episodic&quot;,&quot;generic&quot;: 1 1 2 1 1 2 2 2 2 1 ... ## $ prep : Factor w/ 2 levels &quot;ins&quot;,&quot;zum&quot;: 1 1 1 2 1 2 2 2 2 2 ... Faktoren erstellen d$subject &lt;- factor(d$subject) d$item &lt;- factor(d$item) Lösungen IV Deskriptive Statistik library(psych) describeBy(d$judgement, list(d$gen, d$prep), mat = T, digits = 2) ## item group1 group2 vars n mean sd median trimmed mad min max range ## X11 1 episodic ins 1 150 5.55 1.85 6 5.87 1.48 1 7 6 ## X12 2 generic ins 1 148 6.11 1.26 7 6.35 0.00 2 7 5 ## X13 3 episodic zum 1 150 6.08 1.34 7 6.35 0.00 1 7 6 ## X14 4 generic zum 1 146 6.04 1.39 7 6.32 0.00 1 7 6 ## skew kurtosis se ## X11 -1.11 -0.03 0.15 ## X12 -1.42 1.07 0.10 ## X13 -1.61 2.20 0.11 ## X14 -1.53 1.64 0.11 Lösungen IV Base R aovsub &lt;- summary(aov(judgement ~ prep * gen + Error(subject / (prep * gen)), d)) aovitem &lt;- summary(aov(judgement ~ (prep * gen) + Error(item / (prep * gen)), d)) Lösungen IV Base R aovsub ## ## Error: subject ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## prep 1 5.4 5.428 0.412 0.526 ## gen 1 12.8 12.754 0.967 0.334 ## prep:gen 1 2.7 2.707 0.205 0.654 ## Residuals 28 369.2 13.187 ## ## Error: subject:prep ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## prep 1 6.79 6.795 3.630 0.0667 . ## gen 1 0.30 0.302 0.161 0.6908 ## prep:gen 1 0.00 0.002 0.001 0.9758 ## Residuals 29 54.29 1.872 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Error: subject:gen ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## gen 1 11.56 11.555 8.426 0.00687 ** ## prep:gen 1 12.72 12.715 9.272 0.00481 ** ## Residuals 30 41.14 1.371 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Error: subject:prep:gen ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## prep:gen 1 10.93 10.929 4.563 0.0407 * ## Residuals 31 74.24 2.395 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Error: Within ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Residuals 466 721.1 1.547 aovitem ## ## Error: item ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## prep 1 2.55 2.554 0.638 0.4369 ## gen 1 5.47 5.471 1.367 0.2607 ## prep:gen 1 22.72 22.718 5.674 0.0309 * ## Residuals 15 60.06 4.004 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Error: item:prep ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## prep 1 7.77 7.767 3.603 0.0759 . ## gen 1 10.85 10.854 5.035 0.0393 * ## prep:gen 1 1.99 1.986 0.921 0.3514 ## Residuals 16 34.49 2.156 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Error: item:gen ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## gen 1 10.33 10.329 2.462 0.135 ## prep:gen 1 0.87 0.872 0.208 0.654 ## Residuals 17 71.32 4.195 ## ## Error: item:prep:gen ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## prep:gen 1 13.12 13.118 8.933 0.00787 ** ## Residuals 18 26.43 1.468 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Error: Within ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Residuals 518 1055 2.037 Lösungen IV afex library(afex) aov_ez(id = &quot;subject&quot;, dv = &quot;judgement&quot;, within = c(&quot;gen&quot;, &quot;prep&quot;), data = d) ## Anova Table (Type 3 tests) ## ## Response: judgement ## Effect df MSE F ges p.value ## 1 gen 1, 31 0.30 9.07 ** .02 .005 ## 2 prep 1, 31 0.37 4.64 * .01 .04 ## 3 gen:prep 1, 31 0.52 4.85 * .02 .04 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 aov_ez(id = &quot;item&quot;, dv = &quot;judgement&quot;, within = c(&quot;gen&quot;, &quot;prep&quot;), data = d) ## Anova Table (Type 3 tests) ## ## Response: judgement ## Effect df MSE F ges p.value ## 1 gen 1, 18 0.52 2.57 .04 .13 ## 2 prep 1, 18 0.34 2.69 .03 .12 ## 3 gen:prep 1, 18 0.19 9.09 ** .05 .007 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 Lösungen IV ez library(ez) ezANOVA(data = d, wid = subject, dv = judgement, within = .(prep, gen)) ## $ANOVA ## Effect DFn DFd F p p&lt;.05 ges ## 2 prep 1 31 4.641385 0.039094646 * 0.01370960 ## 3 gen 1 31 9.073004 0.005128325 * 0.02213167 ## 4 prep:gen 1 31 4.852868 0.035162122 * 0.02039948 ezANOVA(data = d, wid = item, dv = judgement, within = .(prep, gen)) ## $ANOVA ## Effect DFn DFd F p p&lt;.05 ges ## 2 prep 1 18 2.694220 0.118069072 0.02947056 ## 3 gen 1 18 2.574507 0.125999408 0.04226753 ## 4 prep:gen 1 18 9.087690 0.007445464 * 0.05379394 Nur Kurz: Plots Ein Plot zum Abschluss library(ggplot2) p &lt;- ggplot(data = d, aes(y = judgement, x = prep, shape = gen, color = gen, group = gen)) + stat_summary(fun.y = mean, geom = &quot;line&quot;) + stat_summary(fun.y = mean, geom = &quot;point&quot;) + stat_summary(fun.data = mean_se, geom = &quot;errorbar&quot;, width = 0.2, alpha = .5, linetype = 1) + labs(y = &quot;Mean Rating [1..7]&quot;, x = &quot;Preposition&quot;, shape = &quot;Reading&quot;, color = &quot;Reading&quot;) Ein Plot zum Abschluss p Zum Nachdenken Wie könnte man das folgende Meme in ein Experiment übersetzen? Wie sähe der Versuchsaufbau und wie die afex-ANOVA dazu aus? Der Glühwein ruft Beste Grüße und bis nächste Woche! "],
["sitzung-9.html", "Sitzung 9 Experimentanalyse Daten einlesen und aufbereiten Deskriptive Statistik Plots Inferenzstatistik Kontraste Nochmal Plots", " Sitzung 9 Experimentanalyse Der Plan Heute will ich mit euch so tun, als würden wir gemeinsam ein komplettes Experiment analysieren. Das dient einerseits der Vorbereitung auf die Prüfungsleistung, wo ihr genau das machen müsst, andererseits ist es als kleine Wiederholungssitzung gedacht. Ihr könnt also überprüfen, welche Bereiche aus dem R-Handwerkszeug, das hier bereits Stoff war, schon bei euch sitzen und welche ihr euch vielleicht nochmal angucken solltet. Genauer werden wir Folgendes machen: kurze Einführung in das Experimentdesign Rohdaten einlesen Daten für die Analyse aufbereiten überflüssige Spalten entfernen unabhängige Variablen codieren Spalten zu Faktoren umwandeln Deskriptive Statistik (Visualisierung) (ein kleines bisschen) Inferenzstatistik Einführung ins Experiment Yuqiu Chen, Mailin Antomo und ich haben ein Experiment zu folgender Fragestellung durchgeführt: Warum gibt es einen Unterschied zwischen den beiden Satzpaaren: Did somebody steal a car? # It was the duck \\([_{PSP}\\) who stole a car\\(]\\).-Cleft Did somebody steal a car? I know \\([_{PSP}\\) that the duck stole a car\\(]\\). Scheinbar verhalten sich nicht alle präsuppositionsauslösenden Ausdücke gleich in Bezug auf at-issueness. Können wir diesen intuitiven Unterschied empirisch festigen und auf die soft-hard-Dichotomie von Präsuppositionsauslösern zurückführen? Einführung ins Experiment Faktoren (unabhängige Variablen): : Hard Trigger (-Clefts und Adverbien) vs Soft Trigger (faktive Verben) vs Appositiver Relativsatz : At-Issue vs Non-At-Issue : Erwachsene vs Kinder Methode (abhängige Variable): Akzeptabilitätsrating (1 bis 5) des b-Satzes in Bezug auf die Frage in a Daten einlesen und aufbereiten Daten einlesen und Überblick verschaffen d &lt;- read.csv(&quot;docs/data/tut9/datapre.csv&quot;, sep = &quot;;&quot;) head(d) ## V1 V2 V3 V4 V5 V6 V7 ## 1 id 3536fed0235c7b34a33ddf06fb91b390 ip ::1 time(s) 1558796407.625 data ## 2 id 3536fed0235c7b34a33ddf06fb91b390 ip ::1 time(s) 1558796437.5646 data ## 3 id 3536fed0235c7b34a33ddf06fb91b390 ip ::1 time(s) 1558796471.0036 data ## 4 id 3536fed0235c7b34a33ddf06fb91b390 ip ::1 time(s) 1558796501.9807 data ## 5 id 3536fed0235c7b34a33ddf06fb91b390 ip ::1 time(s) 1558796538.352 data ## 6 id 3536fed0235c7b34a33ddf06fb91b390 ip ::1 time(s) 1558796576.8658 data ## V8 V9 V10 V11 V12 V13 V14 V15 V16 V17 age months gender ## 1 schaffen4_at 5 NA 0 0 NA NA 0 mailin NA 5 1 w ## 2 schaffen1_non 5 NA 0 0 NA NA 0 mailin NA 5 1 w ## 3 cleft2_non 5 NA 0 0 NA NA 0 mailin NA 5 1 w ## 4 appRel3_non 5 NA 0 0 NA NA 0 mailin NA 5 1 w ## 5 appRel9_non 4 NA 0 0 NA NA 0 mailin NA 5 1 w ## 6 gewinnen2_non 4 NA 0 0 NA NA 0 mailin NA 5 1 w Überblick II summary(d) ## V1 V2 V3 V4 ## id:1181 0up71acsusa12ruvaf3llod4h6 : 30 ip:1181 ::1:1181 ## 28af308e70043f33346261c3fe3a79b8: 30 ## 2oocalgtfmpphi3ro9s68fu990 : 30 ## 3536fed0235c7b34a33ddf06fb91b390: 30 ## 3gkj6ts9s53vtfhk8v1r4d07r1 : 30 ## 3jtpb1nfmpi5qb2fe95qktc8j5 : 30 ## (Other) :1001 ## V5 V6 V7 V8 ## time(s):1181 15.431.601.928.574: 1 data:1181 auch3_at : 39 ## 15.431.602.323.662: 1 appRel10_at : 26 ## 15.431.603.044.837: 1 appRel3_non : 26 ## 15.431.603.431.064: 1 appRel9_non : 26 ## 15.431.603.785.808: 1 gewinnen2_non: 26 ## 15.431.604.161.861: 1 schaffen2_at : 26 ## (Other) :1175 (Other) :1012 ## V9 V10 V11 V12 V13 ## Min. :1.000 Mode:logical Min. :0 Min. :0 Mode:logical ## 1st Qu.:2.000 NA&#39;s:1181 1st Qu.:0 1st Qu.:0 NA&#39;s:1181 ## Median :4.000 Median :0 Median :0 ## Mean :3.466 Mean :0 Mean :0 ## 3rd Qu.:5.000 3rd Qu.:0 3rd Qu.:0 ## Max. :5.000 Max. :0 Max. :0 ## ## V14 V15 V16 V17 age ## Mode:logical Min. :0 Imke :180 Mode:logical Min. : 4.00 ## NA&#39;s:1181 1st Qu.:0 Linda :176 NA&#39;s:1181 1st Qu.: 5.00 ## Median :0 mailin :120 Median :18.00 ## Mean :0 Mailin : 88 Mean :16.23 ## 3rd Qu.:0 Susanne:152 3rd Qu.:24.00 ## Max. :0 Y :420 Max. :65.00 ## yuqiu : 45 ## months gender ## Min. : 0.000 m:292 ## 1st Qu.: 0.000 w:889 ## Median : 0.000 ## Mean : 2.767 ## 3rd Qu.: 6.000 ## Max. :11.000 ## Was ist zu tun? leere und überflüssige Spalten entfernen welche sind das? Faktoren codieren V8 enthält alle Informationen für die Faktoren und die Spalte age enthält die Informationen für Spalten umbenennen Leere und überflüssige Spalten entfernen Spalten auswählen, die wir behalten wollen: (apply(is.na(d), 2, sum) / length(d[, 1])) * 100 ## V1 V2 V3 V4 V5 V6 V7 V8 V9 V10 V11 ## 0 0 0 0 0 0 0 0 0 100 0 ## V12 V13 V14 V15 V16 V17 age months gender ## 0 100 100 0 0 100 0 0 0 d &lt;- d[c(2, 8, 9, 16, 18, 19, 20)] head(d) ## V2 V8 V9 V16 age months gender ## 1 3536fed0235c7b34a33ddf06fb91b390 schaffen4_at 5 mailin 5 1 w ## 2 3536fed0235c7b34a33ddf06fb91b390 schaffen1_non 5 mailin 5 1 w ## 3 3536fed0235c7b34a33ddf06fb91b390 cleft2_non 5 mailin 5 1 w ## 4 3536fed0235c7b34a33ddf06fb91b390 appRel3_non 5 mailin 5 1 w ## 5 3536fed0235c7b34a33ddf06fb91b390 appRel9_non 4 mailin 5 1 w ## 6 3536fed0235c7b34a33ddf06fb91b390 gewinnen2_non 4 mailin 5 1 w Leere und überflüssige Spalten entfernen Alternative mit subset-Fuktion: d &lt;- subset(d, select = c(V2, V8, V9, V16, age, months, gender)) Faktor Was wir machen müssen, ist nach Mustern in der Itembenennung zu schauen, die wir verwenden können. Dazu bieten sich “_at” und “_non” an. Eine Funktion, die wir uns zunutze machen können, ist im Paket “stringr” enthalten. install.packages(&quot;stringr&quot;) library(stringr) Sie heißt str_detect und funktioniert so: str_detect(String, Muster) Ein Beispiel: fruit &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;pear&quot;, &quot;pineapple&quot;) str_detect(fruit, &quot;na&quot;) ## [1] FALSE TRUE FALSE FALSE Übung Benutzt str_detect um eine neue Spalte “issueness” anzulegen, die “at-issue” enthält, wenn das Item den String “_at” enthält, und “non-at-issue” bei “_non” Lösung Benutzt str_detect um eine neue Spalte “issueness” anzulegen, die “at-issue” enthält, wenn das Item den String “_at” enthält, und “non-at-issue” bei “_non” d$issueness[str_detect(d$V8, &quot;_at&quot;)] &lt;- &quot;at-issue&quot; d$issueness[str_detect(d$V8, &quot;_non&quot;)] &lt;- &quot;non-at-issue&quot; head(d) ## V2 V8 V9 V16 age months gender ## 1 3536fed0235c7b34a33ddf06fb91b390 schaffen4_at 5 mailin 5 1 w ## 2 3536fed0235c7b34a33ddf06fb91b390 schaffen1_non 5 mailin 5 1 w ## 3 3536fed0235c7b34a33ddf06fb91b390 cleft2_non 5 mailin 5 1 w ## 4 3536fed0235c7b34a33ddf06fb91b390 appRel3_non 5 mailin 5 1 w ## 5 3536fed0235c7b34a33ddf06fb91b390 appRel9_non 4 mailin 5 1 w ## 6 3536fed0235c7b34a33ddf06fb91b390 gewinnen2_non 4 mailin 5 1 w ## issueness ## 1 at-issue ## 2 non-at-issue ## 3 non-at-issue ## 4 non-at-issue ## 5 non-at-issue ## 6 non-at-issue Faktor Die Liste von Triggern sieht wie folgt aus: Soft: schaffen, entdecken, gewinnen Hard: cleft, auch, wieder Appositiver Relativsatz: appRel Übung II Benutzt dieselbe Methode, um den Faktor zu codieren. Tipp: str_detect erlaubt auch Listen von Mustern, deren Elemente durch “|” (read: oder) innerhalb eines Strings getrennt sind: fruit &lt;- c(&quot;apple&quot;, &quot;banana&quot;, &quot;pear&quot;, &quot;pineapple&quot;) str_detect(fruit, &quot;na|ar&quot;) # TRUE bei &quot;na&quot; und &quot;ar&quot; ## [1] FALSE TRUE TRUE FALSE fruit[str_detect(fruit, &quot;na|ar&quot;)] ## [1] &quot;banana&quot; &quot;pear&quot; Lösung II Benutzt dieselbe Methode, um den Faktor zu codieren d$trigger[str_detect(d$V8, &quot;schaffen|entdecken|gewinnen&quot;)] &lt;- &quot;soft&quot; d$trigger[str_detect(d$V8, &quot;cleft|auch|wieder&quot;)] &lt;- &quot;hard&quot; d$trigger[str_detect(d$V8, &quot;appRel&quot;)] &lt;- &quot;appRel&quot; head(d) ## V2 V8 V9 V16 age months gender ## 1 3536fed0235c7b34a33ddf06fb91b390 schaffen4_at 5 mailin 5 1 w ## 2 3536fed0235c7b34a33ddf06fb91b390 schaffen1_non 5 mailin 5 1 w ## 3 3536fed0235c7b34a33ddf06fb91b390 cleft2_non 5 mailin 5 1 w ## 4 3536fed0235c7b34a33ddf06fb91b390 appRel3_non 5 mailin 5 1 w ## 5 3536fed0235c7b34a33ddf06fb91b390 appRel9_non 4 mailin 5 1 w ## 6 3536fed0235c7b34a33ddf06fb91b390 gewinnen2_non 4 mailin 5 1 w ## issueness trigger ## 1 at-issue soft ## 2 non-at-issue soft ## 3 non-at-issue hard ## 4 non-at-issue appRel ## 5 non-at-issue appRel ## 6 non-at-issue soft Faktor Jetzt müssen wir noch die letzte unabhängige Variable kodieren und der schwerste Teil ist geschafft Übung: Erstellt den Faktor “stage” im Datenblatt mit den Stufen “adult” und “child” Lösung III d$stage[d$age &gt;= 18] &lt;- &quot;adult&quot; d$stage[d$age &lt;= 6] &lt;- &quot;child&quot; head(d) ## V2 V8 V9 V16 age months gender ## 1 3536fed0235c7b34a33ddf06fb91b390 schaffen4_at 5 mailin 5 1 w ## 2 3536fed0235c7b34a33ddf06fb91b390 schaffen1_non 5 mailin 5 1 w ## 3 3536fed0235c7b34a33ddf06fb91b390 cleft2_non 5 mailin 5 1 w ## 4 3536fed0235c7b34a33ddf06fb91b390 appRel3_non 5 mailin 5 1 w ## 5 3536fed0235c7b34a33ddf06fb91b390 appRel9_non 4 mailin 5 1 w ## 6 3536fed0235c7b34a33ddf06fb91b390 gewinnen2_non 4 mailin 5 1 w ## issueness trigger stage ## 1 at-issue soft child ## 2 non-at-issue soft child ## 3 non-at-issue hard child ## 4 non-at-issue appRel child ## 5 non-at-issue appRel child ## 6 non-at-issue soft child Letzte Schritte Wir haben es fast geschafft! Es bleiben noch zwei Dinge zu tun. Übung: Benennt die Spalten sinnvoll um Überprüft, ob die wichtigen Spalten Faktoren sind und holt das ggf nach Lösung IV Benennt die Spalten sinnvoll um names(d) &lt;- c(&quot;id&quot;, &quot;item&quot;, &quot;judgment&quot;, &quot;tester&quot;, &quot;years&quot;, &quot;months&quot;, &quot;gender&quot;, &quot;issueness&quot;, &quot;trigger&quot;, &quot;stage&quot;) Lösung IV Überprüft, ob die wichtigen Spalten Faktoren sind und holt das ggf nach str(d) ## &#39;data.frame&#39;: 1181 obs. of 10 variables: ## $ id : Factor w/ 41 levels &quot;071362c17b307af0925ac2d94d697527&quot;,..: 6 6 6 6 6 6 6 6 6 6 ... ## $ item : Factor w/ 59 levels &quot;appRel1_at&quot;,&quot;appRel1_non&quot;,..: 52 47 29 8 20 43 57 23 33 2 ... ## $ judgment : int 5 5 5 5 4 4 3 4 5 4 ... ## $ tester : Factor w/ 7 levels &quot;Imke&quot;,&quot;Linda&quot;,..: 3 3 3 3 3 3 3 3 3 3 ... ## $ years : int 5 5 5 5 5 5 5 5 5 5 ... ## $ months : int 1 1 1 1 1 1 1 1 1 1 ... ## $ gender : Factor w/ 2 levels &quot;m&quot;,&quot;w&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ issueness: chr &quot;at-issue&quot; &quot;non-at-issue&quot; &quot;non-at-issue&quot; &quot;non-at-issue&quot; ... ## $ trigger : chr &quot;soft&quot; &quot;soft&quot; &quot;hard&quot; &quot;appRel&quot; ... ## $ stage : chr &quot;child&quot; &quot;child&quot; &quot;child&quot; &quot;child&quot; ... d$issueness &lt;- factor(d$issueness) d$trigger &lt;- factor(d$trigger) d$stage &lt;- factor(d$stage) str(d[, 8:10]) ## &#39;data.frame&#39;: 1181 obs. of 3 variables: ## $ issueness: Factor w/ 2 levels &quot;at-issue&quot;,&quot;non-at-issue&quot;: 1 2 2 2 2 2 2 1 2 2 ... ## $ trigger : Factor w/ 3 levels &quot;appRel&quot;,&quot;hard&quot;,..: 3 3 2 1 1 3 2 2 2 1 ... ## $ stage : Factor w/ 2 levels &quot;adult&quot;,&quot;child&quot;: 2 2 2 2 2 2 2 2 2 2 ... (Optionale Kür) Um die spätere Analyse ein bisschen einfacher (und die by-item-Plots aufschlussreicher) zu machen, codieren wir im Folgenden noch die Spalte mit den Itemids (also ohne die At-Issueness-Information) und die lexikalischen Trigger (Wert in der Itemspalte ohne Zahl und ohne At-Issueness) # generate a column that holds the item ids without at-issuess # replace strings like &quot;_at&quot; with nothing d$itemid &lt;- factor(gsub(&quot;_.*$&quot;, &quot;&quot;, d$item)) # code lexical triggers # replace strings like &quot;5_non&quot; with nothing d$lex &lt;- factor(gsub(&quot;[[:digit:]]_.*$&quot;, &quot;&quot;, d$item)) head(d, 4) ## id item judgment tester years months ## 1 3536fed0235c7b34a33ddf06fb91b390 schaffen4_at 5 mailin 5 1 ## 2 3536fed0235c7b34a33ddf06fb91b390 schaffen1_non 5 mailin 5 1 ## 3 3536fed0235c7b34a33ddf06fb91b390 cleft2_non 5 mailin 5 1 ## 4 3536fed0235c7b34a33ddf06fb91b390 appRel3_non 5 mailin 5 1 ## gender issueness trigger stage itemid lex ## 1 w at-issue soft child schaffen4 schaffen ## 2 w non-at-issue soft child schaffen1 schaffen ## 3 w non-at-issue hard child cleft2 cleft ## 4 w non-at-issue appRel child appRel3 appRel Deskriptive Statistik describe()/describeBy() Für die descriptive Statistik können wir der Einfachheit halber den describeBy-Befehl verwenden Paket laden: library(psych) Statistik rechnen lassen: describe(...) describeBy(..., ...) Lösungen V describeBy(d$judgment, list(d$stage, d$issueness, d$trigger), mat = T, digits = 2) ## item group1 group2 group3 vars n mean sd median trimmed mad min ## X11 1 adult at-issue appRel 1 99 1.85 0.83 2 1.77 1.48 1 ## X12 2 child at-issue appRel 1 99 3.36 1.45 4 3.44 1.48 1 ## X13 3 adult non-at-issue appRel 1 100 3.99 0.95 4 4.12 1.48 1 ## X14 4 child non-at-issue appRel 1 100 3.71 1.32 4 3.86 1.48 1 ## X15 5 adult at-issue hard 1 100 1.99 1.01 2 1.85 1.48 1 ## X16 6 child at-issue hard 1 94 3.39 1.48 4 3.49 1.48 1 ## X17 7 adult non-at-issue hard 1 97 4.33 1.01 5 4.53 0.00 1 ## X18 8 child non-at-issue hard 1 98 3.80 1.32 4 3.96 1.48 1 ## X19 9 adult at-issue soft 1 100 3.48 1.18 4 3.51 1.48 1 ## X110 10 child at-issue soft 1 97 3.70 1.35 4 3.85 1.48 1 ## X111 11 adult non-at-issue soft 1 100 4.18 1.01 4 4.38 1.48 1 ## X112 12 child non-at-issue soft 1 97 3.84 1.34 4 4.01 1.48 1 ## max range skew kurtosis se ## X11 4 3 0.71 -0.13 0.08 ## X12 5 4 -0.28 -1.34 0.15 ## X13 5 4 -0.97 0.44 0.09 ## X14 5 4 -0.64 -0.84 0.13 ## X15 5 4 0.95 0.08 0.10 ## X16 5 4 -0.27 -1.44 0.15 ## X17 5 4 -1.65 2.24 0.10 ## X18 5 4 -0.79 -0.66 0.13 ## X19 5 4 -0.28 -1.15 0.12 ## X110 5 4 -0.62 -0.94 0.14 ## X111 5 4 -1.53 2.15 0.10 ## X112 5 4 -0.82 -0.66 0.14 Plots Plan Im Folgenden zeige ich euch zwei Boxplots der Daten, die dieselben Informationen zeigen. Nämlich wie sich die unterschiedlichen Faktoren (bzw deren Stufen) auf die Bewertungen auswirken Zuerst kommt der boxplot-Befehl von R. Der Plot wird wenig übersichtlich sein. Danach zeige ich euch einen Plot mit ggplot2, der wesentlich besser verständlich ist. Dazu brauchen wir Folgendens: install.packages(&quot;ggplot2&quot;) library(ggplot2) Boxplot boxplot(data = d, judgment ~ trigger:issueness:stage) Boxplot II ggplot(data = d, aes(x = trigger, y = judgment, color = issueness)) + geom_boxplot() + facet_wrap(~stage) Inferenzstatistik Für einen Test entscheiden Welchen Test müssen wir rechnen? Wir müssen folgende Faktoren in unserer Analyse unterbringen und schauen, ob und wie sie sich auf die abhängige Variable (“judgment” – Akzeptabilitätsurteile) auswirkgen: Welcher Test bietet sich an? Lösung Die Varianzanalyse! nächste Übung: schreibt einen Befehl für die Varianzanlyse mit dem afex-Paket. Falls ihr Hilfe braucht, schaut gerne in die vorherigen Folien oder gebt ?aov_ez ein, nachdem ihr das Paket geladen habt. library(afex) aov_ez(id = &quot;ID-Variable&quot;, dv = &quot;Abhängige Variable&quot;, data = d, within = c(&quot;UVwithin1&quot;, &quot;UVwithin2&quot;), between = c(&quot;UVbetween1&quot;, &quot;UVbetween2&quot;) ) Lösung # subjects anovasub &lt;- aov_ez(&quot;id&quot;, &quot;judgment&quot;, d, within = c(&quot;issueness&quot;, &quot;trigger&quot;), between = &quot;stage&quot;) # items anovaitem &lt;- aov_ez(&quot;itemid&quot;, &quot;judgment&quot;, d, within = c(&quot;issueness&quot;, &quot;stage&quot;), between = &quot;trigger&quot;) Lösung # subjects anovasub ## Anova Table (Type 3 tests) ## ## Response: judgment ## Effect df MSE F ges p.value ## 1 stage 1, 39 1.77 3.56 + .04 .07 ## 2 issueness 1, 39 0.33 190.63 *** .31 &lt;.0001 ## 3 stage:issueness 1, 39 0.33 96.53 *** .19 &lt;.0001 ## 4 trigger 2, 78 0.39 17.35 *** .09 &lt;.0001 ## 5 stage:trigger 2, 78 0.39 7.36 ** .04 .001 ## 6 issueness:trigger 2, 78 0.34 15.51 *** .07 &lt;.0001 ## 7 stage:issueness:trigger 2, 78 0.34 9.22 *** .04 .0003 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 Lösung # items anovaitem ## Anova Table (Type 3 tests) ## ## Response: judgment ## Effect df MSE F ges p.value ## 1 trigger 2, 26 0.22 15.80 *** .30 &lt;.0001 ## 2 issueness 1, 26 0.21 145.05 *** .64 &lt;.0001 ## 3 trigger:issueness 2, 26 0.21 12.64 *** .24 .0001 ## 4 stage 1, 26 0.05 89.51 *** .21 &lt;.0001 ## 5 trigger:stage 2, 26 0.05 21.52 *** .12 &lt;.0001 ## 6 issueness:stage 1, 26 0.16 84.99 *** .45 &lt;.0001 ## 7 trigger:issueness:stage 2, 26 0.16 7.65 ** .13 .002 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 Für ausführlichere Wiedergabe der ANOVA summary(anovasub) ## ## Univariate Type III Repeated-Measures ANOVA Assuming Sphericity ## ## Sum Sq num Df Error SS den Df F value Pr(&gt;F) ## (Intercept) 2950.73 1 69.078 39 1665.9223 &lt; 2.2e-16 *** ## stage 6.31 1 69.078 39 3.5631 0.0665353 . ## issueness 63.03 1 12.896 39 190.6296 &lt; 2.2e-16 *** ## stage:issueness 31.92 1 12.896 39 96.5296 4.209e-12 *** ## trigger 13.53 2 30.407 78 17.3550 5.823e-07 *** ## stage:trigger 5.74 2 30.407 78 7.3608 0.0011793 ** ## issueness:trigger 10.60 2 26.661 78 15.5099 2.133e-06 *** ## stage:issueness:trigger 6.30 2 26.661 78 9.2192 0.0002546 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## ## Mauchly Tests for Sphericity ## ## Test statistic p-value ## trigger 0.93521 0.28007 ## stage:trigger 0.93521 0.28007 ## issueness:trigger 0.91718 0.19347 ## stage:issueness:trigger 0.91718 0.19347 ## ## ## Greenhouse-Geisser and Huynh-Feldt Corrections ## for Departure from Sphericity ## ## GG eps Pr(&gt;F[GG]) ## trigger 0.93915 1.154e-06 *** ## stage:trigger 0.93915 0.0015242 ** ## issueness:trigger 0.92351 4.587e-06 *** ## stage:issueness:trigger 0.92351 0.0003904 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## HF eps Pr(&gt;F[HF]) ## trigger 0.9850323 6.888931e-07 ## stage:trigger 0.9850323 1.256089e-03 ## issueness:trigger 0.9673658 2.956515e-06 ## stage:issueness:trigger 0.9673658 3.055378e-04 Exkurs: Sphärizitätskorrekturen “In particular, RM ANOVA assumes sphericity. That is, the variances of the differences between all pairs of groups are equal. In repeated measures data, this would imply that the variance of e.g. time 1 and time 2 is the same as the variance of the difference of time 1 and time 3.” Peter Flom (2014) Sphärizität in der afex-ANOVA Die afex-ANOVA korrigiert automatisch die Freiheitsgrade (und damit den p-Wert) bei Sphärizitätsverletzungen. Dies kann wie folgt ausgestellt werden: # subjects without sphericity correction anovasub_nosphere &lt;- aov_ez(&quot;id&quot;, &quot;judgment&quot;, d, within = c(&quot;issueness&quot;, &quot;trigger&quot;), between = &quot;stage&quot;, anova_table = list(correction = &quot;none&quot;)) # &lt;&lt; Vergleich beider Ergebnisse anovasub ## Anova Table (Type 3 tests) ## ## Response: judgment ## Effect df MSE F ges p.value ## 1 stage 1, 39 1.77 3.56 + .04 .07 ## 2 issueness 1, 39 0.33 190.63 *** .31 &lt;.0001 ## 3 stage:issueness 1, 39 0.33 96.53 *** .19 &lt;.0001 ## 4 trigger 1.88, 73.25 0.42 17.35 *** .09 &lt;.0001 ## 5 stage:trigger 1.88, 73.25 0.42 7.36 ** .04 .002 ## 6 issueness:trigger 1.85, 72.03 0.37 15.51 *** .07 &lt;.0001 ## 7 stage:issueness:trigger 1.85, 72.03 0.37 9.22 *** .04 .0004 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 ## ## Sphericity correction method: GG anovasub_nosphere ## Anova Table (Type 3 tests) ## ## Response: judgment ## Effect df MSE F ges p.value ## 1 stage 1, 39 1.77 3.56 + .04 .07 ## 2 issueness 1, 39 0.33 190.63 *** .31 &lt;.0001 ## 3 stage:issueness 1, 39 0.33 96.53 *** .19 &lt;.0001 ## 4 trigger 2, 78 0.39 17.35 *** .09 &lt;.0001 ## 5 stage:trigger 2, 78 0.39 7.36 ** .04 .001 ## 6 issueness:trigger 2, 78 0.34 15.51 *** .07 &lt;.0001 ## 7 stage:issueness:trigger 2, 78 0.34 9.22 *** .04 .0003 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 Kontraste Kontraste kodieren, aber wie? install.packages(&quot;emmeans&quot;) library(emmeans) # referenztabelle für kontraste; specs sind die die Faktoren, dessen estimated marginal means (EMM) wir uns angucken wollen (ref_id &lt;- emmeans(anovasub, specs = c(&quot;stage&quot;, &quot;trigger&quot;, &quot;issueness&quot;))) ## stage trigger issueness emmean SE df lower.CL upper.CL ## adult appRel at.issue 1.85 0.171 133 1.51 2.19 ## child appRel at.issue 3.34 0.169 129 3.01 3.68 ## adult hard at.issue 1.99 0.171 133 1.66 2.33 ## child hard at.issue 3.46 0.169 129 3.12 3.79 ## adult soft at.issue 3.48 0.171 133 3.15 3.82 ## child soft at.issue 3.65 0.169 129 3.31 3.98 ## adult appRel non.at.issue 3.99 0.171 133 3.66 4.33 ## child appRel non.at.issue 3.67 0.169 129 3.34 4.01 ## adult hard non.at.issue 4.35 0.171 133 4.01 4.69 ## child hard non.at.issue 3.84 0.169 129 3.51 4.18 ## adult soft non.at.issue 4.18 0.171 133 3.85 4.52 ## child soft non.at.issue 3.81 0.169 129 3.47 4.14 ## ## Warning: EMMs are biased unless design is perfectly balanced ## Confidence level used: 0.95 Exkurs: Estimated Marginal Means # let&#39;s generate a table with values d &lt;- tibble( reads = c(&quot;reads&quot;, &quot;doesnt_read&quot;), studies = c(1, 2), doesnt_study = c(3, 5) ) d ## # A tibble: 2 x 3 ## reads studies doesnt_study ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 reads 1 3 ## 2 doesnt_read 2 5 # then lets transform the data into a tidy one (one row = one observation) d &lt;- d %&gt;% pivot_longer(c(&quot;studies&quot;, &quot;doesnt_study&quot;), names_to = &quot;prepares&quot;, values_to = &quot;grade&quot;) d ## # A tibble: 4 x 3 ## reads prepares grade ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 reads studies 1 ## 2 reads doesnt_study 3 ## 3 doesnt_read studies 2 ## 4 doesnt_read doesnt_study 5 Exkurs cont’d # then lets fit a model fit &lt;- aov(grade ~ reads * prepares, data = d) # and compute estimated marginal means # in a balanced design, EMMs just represent the cell means (here, because we only have one observation per factor combination, means = observations) # in unbalanced design EMMs are adjusted so that all cell means are given equal weight (ref &lt;- emmeans(fit, c(&quot;reads&quot;, &quot;prepares&quot;))) ## Warning in qt((1 - level)/adiv, df): NaNs produced ## reads prepares emmean SE df lower.CL upper.CL ## doesnt_read doesnt_study 5 NaN 0 NaN NaN ## reads doesnt_study 3 NaN 0 NaN NaN ## doesnt_read studies 2 NaN 0 NaN NaN ## reads studies 1 NaN 0 NaN NaN ## ## Confidence level used: 0.95 Übung Spezifiziert Vektoren für die folgenden Kontraste mithilfe der Referenztabelle: appositive RCs (at-issue) vs Hard Trigger (at-issue) Soft Trigger (at-issue) vs Hard Trigger (at-issue) At-issue, Kinder vs At-issue, Erwachsene Beispiel: um adult_appRel mit child_appRel (und nichts anderes) zu vergleichen, verwenden wir folgende Spezifizierung: # adult_appRel versus child_appRel RCAvsC &lt;- c(-1, 1, 0, 0, 0, 0, -1, 1, 0, 0, 0, 0) Lösung Spezifiziert Vektoren für die folgenden Kontraste mithilfe der Referenztabelle: appositive RCs (at-issue) vs Hard Trigger (at-issue) Soft Trigger (at-issue) vs Hard Trigger (at-issue) At-issue, Kinder vs At-issue, Erwachsene # kontraste spezifizieren test_RCvHTat &lt;- c(1, 1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0) test_STvHTat &lt;- c(0, 0, -1, -1, 1, 1, 0, 0, 0, 0, 0, 0) test_ATcvATa &lt;- c(-1, 1, -1, 1, -1, 1, 0, 0, 0, 0, 0, 0) Kontraste berechnen Bonferronikorrektur nur für post hoc Kontraste absolut nötig. Bei a priori spezifizierten (das heißt von der Hypothese gedeckten) Kontrasten kann diese Korrektur weggelassen werden. # bonferroni-korrigierte tests berechnen summary(contrast(ref_id, list(RCvHTat = test_RCvHTat, STvHTat = test_STvHTat, ATcvATa = test_ATcvATa), adjust = &quot;bonferroni&quot;, # Korrektur für multiple Tests infer = TRUE # zeigt p-Werte und KonfInt )) ## contrast estimate SE df lower.CL upper.CL t.ratio p.value ## RCvHTat -0.259 0.267 155.3 -0.906 0.387 -0.971 0.9997 ## STvHTat 1.680 0.267 155.3 1.034 2.327 6.288 &lt;.0001 ## ATcvATa 3.123 0.555 53.1 1.752 4.495 5.630 &lt;.0001 ## ## Confidence level used: 0.95 ## Conf-level adjustment: bonferroni method for 3 estimates ## P value adjustment: bonferroni method for 3 tests Zum selbst Ausprobieren Warning von Thomas (pc): “D.h. ich wuerde den Leuten sagen, dass der letzte Code-Schnipsel (wo Du dann alle Vergleiche anschaust) eine Mischung aus prae- und post-hoc-Betrachtung ist. Und dass das natuerlich fuer moralisch sproede bzw. vom publication bias und Befristung weichgeklopfte Gemueter gewisse Verlockungen/Gefahren birgt. You see?” # alle paarweisen vergleiche für die beiden level von issueness berechnen ref_big &lt;- emmeans(anovasub, ~ stage * trigger | issueness) ref_big ## issueness = at.issue: ## stage trigger emmean SE df lower.CL upper.CL ## adult appRel 1.85 0.171 133 1.51 2.19 ## child appRel 3.34 0.169 129 3.01 3.68 ## adult hard 1.99 0.171 133 1.66 2.33 ## child hard 3.46 0.169 129 3.12 3.79 ## adult soft 3.48 0.171 133 3.15 3.82 ## child soft 3.65 0.169 129 3.31 3.98 ## ## issueness = non.at.issue: ## stage trigger emmean SE df lower.CL upper.CL ## adult appRel 3.99 0.171 133 3.66 4.33 ## child appRel 3.67 0.169 129 3.34 4.01 ## adult hard 4.35 0.171 133 4.01 4.69 ## child hard 3.84 0.169 129 3.51 4.18 ## adult soft 4.18 0.171 133 3.85 4.52 ## child soft 3.81 0.169 129 3.47 4.14 ## ## Warning: EMMs are biased unless design is perfectly balanced ## Confidence level used: 0.95 Ergebnisse des großen Paarvergleichs summary(contrast(ref_big, method = &quot;pairwise&quot;, adjust = &quot;bonferroni&quot;, infer = TRUE )) ## issueness = at.issue: ## contrast estimate SE df lower.CL upper.CL t.ratio ## adult,appRel - child,appRel -1.4980 0.241 131 -2.218 -0.7779 -6.220 ## adult,appRel - adult,hard -0.1475 0.191 155 -0.718 0.4227 -0.771 ## adult,appRel - child,hard -1.6099 0.241 131 -2.330 -0.8898 -6.684 ## adult,appRel - adult,soft -1.6375 0.191 155 -2.208 -1.0673 -8.561 ## adult,appRel - child,soft -1.8004 0.241 131 -2.520 -1.0802 -7.475 ## child,appRel - adult,hard 1.3505 0.241 131 0.630 2.0706 5.607 ## child,appRel - child,hard -0.1119 0.187 155 -0.668 0.4446 -0.600 ## child,appRel - adult,soft -0.1395 0.241 131 -0.860 0.5806 -0.579 ## child,appRel - child,soft -0.3024 0.187 155 -0.859 0.2541 -1.620 ## adult,hard - child,hard -1.4624 0.241 131 -2.182 -0.7423 -6.072 ## adult,hard - adult,soft -1.4900 0.191 155 -2.060 -0.9198 -7.790 ## adult,hard - child,soft -1.6529 0.241 131 -2.373 -0.9327 -6.863 ## child,hard - adult,soft -0.0276 0.241 131 -0.748 0.6925 -0.115 ## child,hard - child,soft -0.1905 0.187 155 -0.747 0.3660 -1.020 ## adult,soft - child,soft -0.1629 0.241 131 -0.883 0.5573 -0.676 ## p.value ## &lt;.0001 ## 1.0000 ## &lt;.0001 ## &lt;.0001 ## &lt;.0001 ## &lt;.0001 ## 1.0000 ## 1.0000 ## 1.0000 ## &lt;.0001 ## &lt;.0001 ## &lt;.0001 ## 1.0000 ## 1.0000 ## 1.0000 ## ## issueness = non.at.issue: ## contrast estimate SE df lower.CL upper.CL t.ratio ## adult appRel - child appRel 0.3194 0.241 131 -0.401 1.0395 1.326 ## adult appRel - adult hard -0.3525 0.191 155 -0.923 0.2177 -1.843 ## adult appRel - child hard 0.1519 0.241 131 -0.568 0.8720 0.631 ## adult appRel - adult soft -0.1900 0.191 155 -0.760 0.3802 -0.993 ## adult appRel - child soft 0.1868 0.241 131 -0.533 0.9069 0.776 ## child appRel - adult hard -0.6719 0.241 131 -1.392 0.0482 -2.790 ## child appRel - child hard -0.1675 0.187 155 -0.724 0.3890 -0.897 ## child appRel - adult soft -0.5094 0.241 131 -1.229 0.2107 -2.115 ## child appRel - child soft -0.1325 0.187 155 -0.689 0.4239 -0.710 ## adult hard - child hard 0.5044 0.241 131 -0.216 1.2245 2.094 ## adult hard - adult soft 0.1625 0.191 155 -0.408 0.7327 0.850 ## adult hard - child soft 0.5393 0.241 131 -0.181 1.2594 2.239 ## child hard - adult soft -0.3419 0.241 131 -1.062 0.3782 -1.420 ## child hard - child soft 0.0349 0.187 155 -0.522 0.5914 0.187 ## adult soft - child soft 0.3768 0.241 131 -0.343 1.0969 1.565 ## p.value ## 1.0000 ## 1.0000 ## 1.0000 ## 1.0000 ## 1.0000 ## 0.0910 ## 1.0000 ## 0.5449 ## 1.0000 ## 0.5723 ## 1.0000 ## 0.4023 ## 1.0000 ## 1.0000 ## 1.0000 ## ## Confidence level used: 0.95 ## Conf-level adjustment: bonferroni method for 15 estimates ## P value adjustment: bonferroni method for 15 tests Nochmal Plots by subject subs &lt;- ggplot(d, aes(y = judgment, x = trigger, color = issueness, shape = issueness, linetype = issueness, group = issueness)) + stat_summary(fun.y = mean, geom = &quot;line&quot;) + stat_summary(fun.y = mean, geom = &quot;point&quot;) + stat_summary(fun.data = mean_se, geom = &quot;errorbar&quot;, width = 0.25, alpha = .5, linetype = 1) + labs(y = &quot;Mean Judgments&quot;, x = &quot;Trigger Type&quot;) + scale_color_manual(values = c(&quot;#19278e&quot;, &quot;#BC0D0D&quot;)) + scale_x_discrete(labels = c(&#39;appositive RC&#39;, &#39;Hard&#39;, &#39;Soft&#39;)) + facet_wrap(~id) + cleanup by subject subs by item items &lt;- ggplot(d, aes(y = judgment, x = issueness, color = issueness, shape = issueness)) + stat_summary(fun.y = mean, geom = &quot;line&quot;, aes(group = 1)) + stat_summary(fun.y = mean, geom = &quot;point&quot;, aes(group = 1)) + stat_summary(fun.data = mean_se, geom = &quot;errorbar&quot;, width = 0.25, alpha = .5, linetype = 1) + labs(y = &quot;Mean Judgments&quot;, x = &quot;Issueness&quot;) + scale_color_manual(values = c(&quot;#19278e&quot;, &quot;#BC0D0D&quot;)) + facet_wrap(~itemid) + cleanup by item items Off to the rave Bis zum nächsten Mal! "],
["ggplot2.html", "ggplot2 Warum Plots: Anscombes Quartett Beispiele Einfache Plots Plots erweitern Plots verschönern Plot speichern Plots verbinden stat_summary() Fehlerbalken Diagnostische Plots Normalverteilung Boxplot verbessern", " ggplot2 Warum Plots: Anscombes Quartett Scheinbar gleiche Daten Diese vier Bedingungen sehen auf den ersten Blick ziemlich gleich aus: ## # A tibble: 4 x 9 ## set mean_x mean_y sd_x sd_y cor_xy intercept slope r_squared ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 A 9 7.50 3.32 2.03 0.816 3 0.5 0.667 ## 2 B 9 7.50 3.32 2.03 0.816 3.00 0.5 0.666 ## 3 C 9 7.5 3.32 2.03 0.816 3.00 0.5 0.666 ## 4 D 9 7.50 3.32 2.03 0.817 3.00 0.5 0.667 Aber! Beispiele Beispiel 1 Beispiel 2 Beispiel 3 Einfache Plots Daten für den Basisplot Hier schonmal der Cheat Sheet zu ggplot2, eine weitere Hilfeseite und ein großartiges Tutorial. Paket laden und Daten präparieren (ich verwende hier “nur” 5000 Beobachtungen, weil es sonst in manchen Plots ziemlich voll wird; das kommt aber ganz auf die Art des Plots an): library(ggplot2) set.seed(1) d &lt;- as.data.frame(diamonds) d &lt;- d[sample(1:nrow(d), 5000), ] head(d) ## carat cut color clarity depth table price x y z ## 24388 0.41 Very Good D SI2 62.3 61 638 4.72 4.75 2.95 ## 43307 0.50 Very Good F VS2 62.8 57 1402 5.05 5.08 3.18 ## 4050 1.03 Fair I SI2 65.2 56 3530 6.42 6.35 4.16 ## 11571 1.10 Ideal I SI1 62.1 57 5037 6.60 6.64 4.11 ## 25173 1.51 Very Good E VS2 63.3 61 13757 7.24 7.17 4.56 ## 32618 0.30 Ideal H VS2 62.1 55 457 4.30 4.33 2.68 Zu den Daten selbst: “A dataset containing the prices and other attributes of almost 54,000 diamonds.” ggplot() Grundgerüst ist der ggplot-Befehl mit Datensatz- und aes-Angaben: Werte auf der \\(x\\)- und \\(y\\)-Achse. Dabei sollte die abhängige Variable, hier der Verkaufspreis, die \\(y\\)-Achse bilden, während die (erste) unabhängige Variable, hier das Gewicht der Diamanten, auf der \\(x\\)-Achse verortet ist. Ich speichere diese Angaben unter dem Namen p ab, damit ich später weniger Tipparbeit habe und euch einfach verschiedene Variationen desselben Plots zeigen kann. Nötig ist das aber nicht unbedingt. p &lt;- ggplot(d, aes(x = carat, y = price)) ggplot() Lassen wir uns dieses Objekt anzeigen, sehen wir allerdings nur die Achsen. p Geometrien Das liegt daran, dass wir ggplot noch nicht gesagt haben, was für eine Art Plot wir haben wollen. Dies machen wir mithilfe sogenannter Geometrien, die wir einfach per + unserem Objekt hinzufügen können. Gängig sind vor allem Scatterplots (geom_point), Linienplots (geom_line) und Balkendiagramme (geom_bar/geom_histogramm) Geometrien Hier die Punktwolke (also der Scatterplot): p + geom_point() Geometrien Falls sich Punkte überlagern: geom_jitter rüttelt ein wenig an den Punkten, damit sie sich nicht länger überschneiden ggplot(d, aes(x = cut, y = price)) + geom_point() Geometrien ggplot(d, aes(x = cut, y = price)) + geom_jitter() Geometrien Hier ein Linienplot: p + geom_line() Geometrien Hier eine Trendlinie (LM bzw Korrelation): p + geom_smooth(method = &quot;lm&quot;) Geometrien Linie + Punkte p + geom_point() + geom_smooth(method = &quot;lm&quot;) Mehr Spalten Was ist, wenn ihr mehr als nur zwei Spalten unterbringen will? \\(\\rightarrow\\) aes erweitern. Weitere Möglichkeiten: - fill - linetype (kurz: lty) - size - group - alpha p &lt;- ggplot(d, aes(x = carat, y = price, color = cut)) Mehr Spalten Was ist, wenn ihr mehr als nur zwei Spalten unterbringen will? \\(\\rightarrow\\) aes erweitern: p + geom_line() Mehr Spalten p + geom_point() Mehr Spalten p + geom_point() + geom_smooth(method = &quot;lm&quot;) Mehr Spalten Oder als Boxplot: p + geom_boxplot() Häufigkeiten fill statt color weil wir nicht bloß die Umrandung der Balken einfärben wollen d$cut &lt;- factor(d$cut) ggplot(d, aes(x = cut, fill = cut)) + geom_bar() Häufigkeiten Zum Unterschied zwischen Histogrammen und Barplots: Ohne weitere Argumente sind Barplots für diskrete Daten und Histogramme für kontinuierliche Daten ausgelegt (ich zeige euch aber gleich, dass man es auch anders machen kann) ggplot(d, aes(x = carat)) + geom_histogram() Häufigkeiten diskret ggplot(d, aes(x = cut)) + geom_histogram(stat = &quot;count&quot;) Prozente group = 1 damit Prozente nicht innerhalb der Schliffkategorien berechnet werden (dann hätten wir überall 100%!) ggplot(d, aes(x = cut, y = ..prop.., group = 1)) + geom_histogram(stat = &quot;count&quot;) Prozente mit Label label ändert die Beschriftung ggplot(diamonds, aes(x = cut, y = ..prop.., group = 1)) + geom_histogram(stat = &quot;count&quot;) + geom_text(aes(label = scales::percent(..prop..)), stat = &quot;count&quot;) Häufigkeiten ggplot(d, aes(x = carat)) + geom_density() Plots erweitern Beschriftungen p &lt;- ggplot(d, aes(x = carat, y = price, color = cut)) + geom_point() + geom_smooth(method = &quot;lm&quot;) Beschriftungen p + labs(x = &quot;Diamantengewicht in Karat&quot;, y = &quot;Verkaufspreis&quot;, color = &quot;Schliff&quot;) Beschriftungen p + labs(x = &quot;Diamantengewicht in Karat&quot;, y = &quot;Verkaufspreis&quot;) + guides(color = F) Beschriftungen library(ggpubr) p + clean_theme() Beschriftungen p + scale_y_continuous(name = &quot;Verkaufspreis&quot;, labels = scales::dollar) Achsen beschränken p + ylim(0, 20000) Achsen beschränken p + xlim(0, 2.5) Achsen beschränken p &lt;- ggplot(d, aes(x = carat, y = price, fill = cut)) + geom_boxplot() p Achsen beschränken Merkt ihr was? Die Daten verändern sich! p + ylim(1, 4000) Achsen beschränken Besser: p + coord_cartesian(ylim = c(1, 4000)) Facetten p + facet_wrap(~color) Facetten p + facet_wrap(~color, ncol = 4) Plots verschönern Daten für den Basisplot library(ggplot2) d &lt;- as.data.frame(msleep) # from ggplot2 package head(d) ## name genus vore order conservation ## 1 Cheetah Acinonyx carni Carnivora lc ## 2 Owl monkey Aotus omni Primates &lt;NA&gt; ## 3 Mountain beaver Aplodontia herbi Rodentia nt ## 4 Greater short-tailed shrew Blarina omni Soricomorpha lc ## 5 Cow Bos herbi Artiodactyla domesticated ## 6 Three-toed sloth Bradypus herbi Pilosa &lt;NA&gt; ## sleep_total sleep_rem sleep_cycle awake brainwt bodywt ## 1 12.1 NA NA 11.9 NA 50.000 ## 2 17.0 1.8 NA 7.0 0.01550 0.480 ## 3 14.4 2.4 NA 9.6 NA 1.350 ## 4 14.9 2.3 0.1333333 9.1 0.00029 0.019 ## 5 4.0 0.7 0.6666667 20.0 0.42300 600.000 ## 6 14.4 2.2 0.7666667 9.6 NA 3.850 Basisplot p &lt;- ggplot(d, aes(y = sleep_total, x = vore, color = vore)) + geom_boxplot() p Möglichkeit 1 cleanup = theme(axis.line.x = element_line(color = &quot;black&quot;), axis.line.y = element_line(color = &quot;black&quot;), panel.background = element_rect(fill = &quot;transparent&quot;, colour = NA), panel.grid.major = element_line(size = .2, color = &quot;gray&quot;), panel.grid.major.x = element_blank(), legend.background = element_rect(fill = &quot;transparent&quot;), axis.title.x = element_text(vjust = -1), axis.title.y = element_text(vjust = 2), plot.margin = unit(c(.5, .5, .5, .5), &quot;cm&quot;), strip.background = element_rect(colour = &quot;black&quot;, fill = &quot;transparent&quot;), legend.key = element_blank() ) cleanup in action p + cleanup Möglichkeit 2 install.packages(&quot;ggpubr&quot;) library(ggpubr) +theme_pubr() + clean_theme() theme_pubr() p + theme_pubr() clean_theme() p + clean_theme() theme_pubr() + clean_theme() p + theme_pubr() + clean_theme() ggthemes library(ggthemes) +theme_base() # looks like basic R plot +theme_tufte() +theme_minimal() +theme_gray() +theme_few() +theme_wsj() +theme_economist() +theme_fivethirtyeight() +theme_stata() +theme_hc() +theme_solarized() +theme_pander() +theme_void() +theme_linedraw() # to change the font or its size in a theme, use +theme_bw(base_family = &quot;Times&quot;, base_size = &quot;10&quot;) Schriftarten +theme_bw(base_family = &quot;Times&quot;, base_size = &quot;10&quot;) &quot;Courier&quot; &quot;Times&quot; &quot;Palatino&quot; &quot;AvantGarde&quot; &quot;Helvetica&quot; Legende verändern p + theme(legend.position = c(0.7, 0.12), legend.background = element_rect(linetype = &quot;solid&quot;, color = &quot;black&quot;), legend.direction = &quot;horizontal&quot;) Achsenbeschriftung p + theme(axis.text.x = element_text(angle = 60, hjust = 1, colour = &quot;darkred&quot;, size = 12, family = &quot;serif&quot;, face = &quot;bold&quot;)) Farbreferenz Mögliche Farben: http://sape.inf.usi.ch/quick-reference/ggplot2/colour Vordefinierte Farbpaletten: https://github.com/EmilHvitfeldt/r-color-palettes/blob/master/README.md Mein Favorit: https://www.data-imaginist.com/2018/scico-and-the-colour-conundrum/ scico # install.packages(&quot;scico&quot;) library(scico) scico_palette_show() Palettenvorschau # palette erstellen (6 farben aus palette &quot;berlin&quot;) COL &lt;- scico(6, palette = &quot;berlin&quot;) # und plotten ggplot() + aes(x = factor(COL, levels = COL), fill = factor(COL, levels = COL)) + geom_bar() + scale_fill_manual(values = COL) + guides(fill = F) + labs(x = NULL, y = NULL) + cleanup scico in action ggplot(d, aes(y = sleep_total, x = vore, fill = vore)) + geom_boxplot() + scale_fill_scico_d(palette = &quot;tokyo&quot;) + cleanup Plot speichern Plots speichern Ohne Angabe des zu speichernden Objekts speichert ggsave den zuletzt angezeigten Plot Ohne “path” wird die Datei Arbeitsverzeichnis (getwd) gespeichert ggsave(&quot;boxplot.pdf&quot;, device = &quot;pdf&quot;, width = 8, height = 5, units = &quot;cn&quot;, path = getwd()) Plots verbinden Mehrere Plots verbinden library(gridExtra) grid.arrange(p, p) nrow/ncol grid.arrange(p, p, nrow = 1) stat_summary() Line Plot d &lt;- subset(d, d$conservation %in% c(&quot;domesticated&quot;, &quot;lc&quot;)) p &lt;- ggplot(d, aes(x = vore, y = sleep_total, color = conservation, group = conservation)) + stat_summary(fun.y = mean, geom = &quot;line&quot;) + stat_summary(fun.y = mean, geom = &quot;point&quot;) + labs(x = &quot;Trophische Ebene&quot;, y = &quot;Mittlere Schlafenszeit in Stunden&quot;, title = &quot;Ein Linienplot&quot;) Line Plot p + cleanup Line Plot p &lt;- p + cleanup + scale_color_manual(values = c(&quot;#19278e&quot;, &quot;#BC0D0D&quot;)) p Fehlerbalken Fehlerbalken Standardfehler p + stat_summary(fun.data = mean_se, geom = &quot;errorbar&quot;, width = 0.25, alpha = .5, linetype = 1) Andere Fehlerbalken Standardfehler vs Standardabweichung p1 &lt;- p + stat_summary(fun.data = mean_se, geom = &quot;errorbar&quot;, width = 0.25, alpha = .5, linetype = 1) p2 &lt;- p + stat_summary(fun.data = mean_sdl, geom = &quot;errorbar&quot;, width = 0.25, alpha = .5, linetype = 1) Standardfehler vs Standardabweichung grid.arrange(p1, p2, nrow = 2) Andere Fehlerbalken 95% Konfidenzintervall p3 &lt;- p + stat_summary(fun.data = mean_cl_normal, geom = &quot;errorbar&quot;, width = 0.25, alpha = .5, linetype = 1) + coord_cartesian(ylim = c(5, 20)) Standardfehler vs 95% Konfidenzintervall grid.arrange(p1, p3, nrow = 2) Diagnostische Plots Diagnostikfunktion library(qqplotr) # stat_qq_band diagPlot &lt;- function(model) { p1 &lt;- ggplot(model, aes(.fitted, .resid)) + geom_point() p1 &lt;- p1 + stat_smooth(method = &quot;loess&quot;) p1 &lt;- p1 + geom_hline(yintercept = 0, col = &quot;red&quot;, linetype = &quot;dashed&quot;) p1 &lt;- p1 + labs(x = &quot;Fitted values&quot;, y = &quot;Residuals&quot;, title = &quot;Residual vs Fitted Plot&quot;) p2 &lt;- ggplot(model, aes(qqnorm(.stdresid)[[1]], .stdresid)) p2 &lt;- p2 + stat_qq_band(mapping = aes(sample = qqnorm(.stdresid)[[1]]), alpha = .6) p2 &lt;- p2 + geom_point(na.rm = TRUE) + geom_abline() p2 &lt;- p2 + labs(x = &quot;Theoretical Quantiles&quot;, y = &quot;Standardized Residuals&quot;, title = &quot;Normal Q-Q&quot;) return(list(rvfPlot = p1, qqPlot = p2)) } Plots diagPlts &lt;- diagPlot(lm(cty ~ displ * hwy, data = mpg)) plot &lt;- grid.arrange(grobs = diagPlts, ncol = 2) Alternative: Base R par(mfrow = c(1, 2)) plot(lm(cty ~ displ * hwy, data = mpg)) Weitere Alternative Von mir nicht getestet! Erstellt die bekannten diagnostischen Plots. install.packages(&quot;broom&quot;) library(broom) plot_residuals(lm(cty ~ displ * hwy, data = mpg)) Ganz simpel: Residuenhistogramm mod &lt;- lm(cty ~ displ * hwy, data = mpg) # alternativer plot: hist(resid(mod)); test: shapiro.test(resid(mod)) ggplot(mod, aes(x = .resid)) + geom_histogram(aes(y = ..density..)) + cleanup Normalverteilung Normalverteilung: Daten # sicherstellen, dass ihr die gleichen &quot;zufällig&quot; # erstellten daten bekommt set.seed(1) # normalverteile daten generieren a &lt;- rnorm(250, mean = 2, sd = 1) b &lt;- rnorm(250, mean = 4.5, sd = 1) # datenblatt erstellen d &lt;- data.frame(a, b) # daten kombinieren und neues datenblatt ab &lt;- c(a, b) d2 &lt;- data.frame(ab) shapiro.test shapiro.test(a) ## ## Shapiro-Wilk normality test ## ## data: a ## W = 0.9964, p-value = 0.8398 shapiro.test(b) ## ## Shapiro-Wilk normality test ## ## data: b ## W = 0.9968, p-value = 0.897 shapiro.test(ab) ## ## Shapiro-Wilk normality test ## ## data: ab ## W = 0.98647, p-value = 0.0001351 Normalverteilung: Daten # beide Verteilungen getrennt p1 &lt;- ggplot(d) + geom_histogram(mapping = aes(x = a, y = ..density..), fill = &quot;#19278e&quot;, alpha = .5) + stat_function(fun = dnorm, args = list(mean = mean(d$a), sd = sd(d$a)), color = &quot;#19278e&quot;, geom = &quot;line&quot;) + geom_histogram(mapping = aes(x = b, y = ..density..), fill = &quot;#BC0D0D&quot;, alpha = .5) + stat_function(fun = dnorm, args = list(mean = mean(d$b), sd = sd(d$b)), color = &quot;#BC0D0D&quot;, geom = &quot;line&quot;) + cleanup # beide Verteilungen kombiniert p2 &lt;- ggplot(d2) + geom_histogram(mapping = aes(x = ab, y = ..density..), fill = &quot;#621b54&quot;, alpha = .5) + stat_density(aes(x = ab, y = ..density..), color = &quot;#BC0D0D&quot;, adjust = 1, geom = &quot;line&quot;) + stat_function(fun = dnorm, args = list(mean = mean(d2$ab), sd = sd(d2$ab)), color = &quot;#19278e&quot;, geom = &quot;line&quot;) + cleanup Normalplots grid.arrange(p1, p2, ncol = 2) Boxplot verbessern Mittelwerte in Boxplots p &lt;- ggplot(mtcars, aes(x = factor(cyl), y = drat, fill = factor(cyl))) + geom_boxplot(alpha = .25, outlier.alpha = 0) + geom_jitter(color = &quot;grey&quot;) + stat_summary(fun.y = mean, geom = &quot;point&quot;, shape = 10, size = 4, position = position_dodge(0.75)) + cleanup Mittelwerte in Boxplots p Mittelwerte in Boxplots p + geom_hline(yintercept = mean(mtcars$drat), lty = 2) Tufte-Boxplots library(ggthemes) +geom_tufteboxplot() p &lt;- ggplot(mtcars, aes(x = factor(cyl), y = drat)) + geom_tufteboxplot() + stat_summary(fun.y = mean, geom = &quot;point&quot;, shape = 10, size = 4, position = position_dodge(0.75)) + cleanup Tufte-Boxplots p Übung Baut den folgenden Plot (“class_cl.csv”) nach. Lösung Baut den folgenden Plot mit “class_cl.csv” nach. d &lt;- read.csv(&quot;docs/data/tutgg/class_cl.csv&quot;) d &lt;- d[d$truth == &quot;f&quot;, ] p &lt;- ggplot(d, aes(x = delivery, fill = judgment)) + geom_histogram(stat = &quot;count&quot;, position = &quot;dodge&quot;) + labs(title = &quot;Lie 3AFC Data&quot;, y = &quot;Absolute Frequency&quot;, x = &quot;Delivery&quot;) + facet_wrap(~id) + cleanup Übung II Baut den folgenden Plot mit “complex.csv” nach. Lösung II Baut den folgenden Plot mit “complex.csv” nach. d &lt;- read.csv(&quot;docs/data/tutgg/complex.csv&quot;) p &lt;- ggplot(d, aes(x = structure, y = judgment, color = match, group = match)) + stat_summary(fun.y = mean, geom = &quot;line&quot;) + stat_summary(fun.y = mean, geom = &quot;point&quot;) + stat_summary(fun.data = mean_se, geom = &quot;errorbar&quot;, width = 0.25, alpha = .5, linetype = 1) + facet_wrap(~gender) + cleanup Bonus: Pimp My Boxplot Paper, die ihr lesen solltet: Weissgerber et al. (2015): Beyond Bar and Line Graphs: Time for a New Data Presentation Paradigm Cumming et al. (2007): Error bars in experimental biology Arbeitsstreik Wir sehen uns nächste Woche! "],
["gifs-in-r.html", "Gifs in R Normalverteilung P-Wert-Verteilung Korrelation Würfel Zeitverlauf Central Limit Theorem Interaktive Plots", " Gifs in R Cheat Sheet zu gganimate (und anderen Paketen) hier erhältlich: https://www.rstudio.com/resources/cheatsheets/ Normalverteilung library(tidyverse) library(gganimate) # more frames for the animations later options(gganimate.nframes = 200) # alternative theme library(hrbrthemes) theme_maik &lt;- function() { theme_ipsum_rc() %+replace% theme( panel.grid.major.x = element_blank(), panel.grid.minor.x = element_blank() ) } # dispersion of normal distribution cols &lt;- scico(5, palette = &quot;devon&quot;, end = .5) ggplot(data = data.frame(x = c(-7, 7)), aes(x)) + geom_curve(aes(x = 5.5, xend = 5.5, y = .4, yend = 0.05), arrow = arrow(length = unit(0.08, &quot;inch&quot;)), size = 0.5, color = cols[1], curvature = 0) + annotate(&quot;text&quot;, x = 5.8, y = .3, size = 3.2, color = cols[1], angle = 270, label = &quot;Higher Standard Deviation&quot;) + stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1), color = cols[1], size = 1) + stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1.5), color = cols[2], size = 1) + stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 2), color = cols[3], size = 1) + stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 2.5), color = cols[4], size = 1) + stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 3), color = cols[5], size = 1) + scale_y_continuous(breaks = NULL) + theme_maik() + labs(x = &quot;&quot;, y = &quot;&quot;) + transition_layers(from_blank = F) + enter_grow() # location of normal distribution cols &lt;- scico(5, palette = &quot;devon&quot;, end = .5) ggplot(data = data.frame(x = c(-4, 16)), aes(x)) + geom_curve(aes(x = 0, xend = 12, y = .43, yend = .43), arrow = arrow(length = unit(0.08, &quot;inch&quot;)), size = 0.5, color = cols[1], curvature = 0) + annotate(&quot;text&quot;, x = 6, y = .45, size = 3.2, color = cols[1], label = &quot;Higher Mean&quot;) + stat_function(fun = dnorm, n = 101, args = list(mean = 0, sd = 1), color = cols[1], size = 1) + stat_function(fun = dnorm, n = 101, args = list(mean = 3, sd = 1), color = cols[2], size = 1) + stat_function(fun = dnorm, n = 101, args = list(mean = 6, sd = 1), color = cols[3], size = 1) + stat_function(fun = dnorm, n = 101, args = list(mean = 9, sd = 1), color = cols[4], size = 1) + stat_function(fun = dnorm, n = 101, args = list(mean = 12, sd = 1), color = cols[5], size = 1) + scale_y_continuous(breaks = NULL) + theme_maik() + labs(x = &quot;&quot;, y = &quot;&quot;) + transition_layers(from_blank = F, keep_layers = c(Inf, Inf, 0, 0, 0, 0, 0)) + exit_drift(x_mod = 3, y_mod = 0) P-Wert-Verteilung # DOES NOT WORK AT THE MOMENT # static plots from: https://gist.github.com/dgrtwo/1337b9d5c80adde1bc7a72048d9e4cc6 library(broom) # generate data t_tests &lt;- crossing(pi0 = .75, effect_size = .25, trial = 1:10000, sample_size = c(10, 30, 100, 300)) %&gt;% mutate(h1 = runif(n()) &gt; pi0) %&gt;% unnest(map(sample_size, seq_len)) %&gt;% # something is wrong with map mutate(x = rnorm(n(), h1 * effect_size, 1)) %&gt;% group_by(pi0, effect_size, sample_size, trial, h1) %&gt;% summarize(test = list(t.test(x))) %&gt;% unnest(map(test, tidy)) %&gt;% ungroup() head(t_tests) # SEE COMMENT ABOVE # plot animation t_tests %&gt;% ggplot(aes(p.value, fill = ifelse(h1, &quot;alternative&quot;, &quot;null&quot;))) + geom_histogram(binwidth = .05, boundary = 0) + labs(fill = &quot;H1&quot;, x = &quot;P-value&quot;, title = &quot;P-value Distribution with Sample Size = {closest_state}&quot;, subtitle = &quot;One-sample t-test, pi0 = .75, alternative mean = .25, m = 10000&quot;) + theme_maik() + transition_states(sample_size) Korrelation from: https://crumplab.github.io/statistics/gifs.html Correlation between random deviates from normal distribution across four sample sizes # data all_df &lt;- data.frame() for (sim in 1:10) { for (n in c(10, 50, 100, 1000)) { North_pole &lt;- rnorm(n, 0, 1) South_pole &lt;- rnorm(n, 0, 1) t_df &lt;- data.frame(nsize = rep(n, n), simulation = rep(sim, n), North_pole, South_pole) all_df &lt;- rbind(all_df, t_df) } } head(all_df) ## nsize simulation North_pole South_pole ## 1 10 1 -1.4879262 1.1612893 ## 2 10 1 -1.1619376 0.3983627 ## 3 10 1 -1.5890897 0.3623522 ## 4 10 1 0.4195830 -0.8525257 ## 5 10 1 -0.9929283 1.9536679 ## 6 10 1 -2.1645471 -0.1642708 Beachte die hohe Variabilität des Korrelationskoeffizienten bei kleinen Stichproben über die 10 Simulationen: cor_by &lt;- by(all_df, list(all_df$simulation, all_df$nsize), function(x) cor(x$North_pole, x$South_pole)) rbind(cor_by) ## 10 50 100 1000 ## 1 -0.5918917 -0.092977251 0.03787743 0.039855996 ## 2 0.1667258 -0.077223258 -0.07990132 0.027070831 ## 3 -0.1707260 -0.045979624 0.07474168 0.033863227 ## 4 -0.1684336 -0.088069191 0.09824673 0.034628231 ## 5 -0.1105484 0.114470224 -0.17458249 0.004448221 ## 6 -0.4793814 0.301845167 -0.21520614 0.014994305 ## 7 0.2114297 0.007627771 0.08877511 -0.013933219 ## 8 -0.1645379 -0.234096228 0.22155066 0.017613001 ## 9 -0.2087030 -0.082574886 0.16194774 0.022472051 ## 10 0.3066167 0.118468209 0.09536719 0.016152547 # generate the animation ggplot(all_df, aes(x = North_pole, y = South_pole)) + geom_point() + geom_smooth(method = lm, se = FALSE) + theme_maik() + facet_wrap(~nsize) + transition_states( simulation, transition_length = 2, state_length = 1 ) + enter_fade() + exit_shrink() + ease_aes(&#39;sine-in-out&#39;) Correlation between X and Y variables that have a true correlation library(MASS) proportional_permute &lt;- function(x, prop) { indices &lt;- seq(1:length(x)) s_indices &lt;- sample(indices) n_shuffle &lt;- round(length(x) * prop) switch &lt;- sample(indices) x[s_indices[1:n_shuffle]] &lt;- x[switch[1:n_shuffle]] return(x) } # data all_df &lt;- data.frame() for (sim in 1:10) { for (samples in c(10, 50, 100, 1000)) { North_pole &lt;- runif(samples, 1, 10) South_pole &lt;- proportional_permute(North_pole, .5) + runif(samples, -5, 5) t_df &lt;- data.frame(nsize = rep(samples, samples), simulation = rep(sim, samples), North_pole, South_pole) all_df &lt;- rbind(all_df, t_df) } } head(all_df) ## nsize simulation North_pole South_pole ## 1 10 1 2.703927 9.96016940 ## 2 10 1 2.291694 -1.49980938 ## 3 10 1 6.972790 -0.09346151 ## 4 10 1 8.806318 10.56235905 ## 5 10 1 6.753495 8.58355443 ## 6 10 1 1.755240 -2.75460252 Beachte die hohe Variabilität des Korrelationskoeffizienten bei kleinen Stichproben über die 10 Simulationen: cor_by &lt;- by(all_df, list(all_df$simulation, all_df$nsize), function(x) cor(x$North_pole, x$South_pole)) rbind(cor_by) ## 10 50 100 1000 ## 1 0.46341286 0.2327006 0.2163476 0.3087383 ## 2 0.66209758 0.1981913 0.3887388 0.3047594 ## 3 0.08603569 0.2222401 0.1873166 0.3573672 ## 4 0.13663875 0.4500101 0.3493339 0.3366337 ## 5 -0.05130151 0.3778502 0.3159541 0.3538828 ## 6 0.04094742 0.3176962 0.2435782 0.3413892 ## 7 -0.35403247 0.2123489 0.1713233 0.3478543 ## 8 0.51061515 0.4002461 0.3474626 0.3041247 ## 9 0.52129439 0.3047629 0.2825007 0.3906099 ## 10 0.54610635 0.3258739 0.2551699 0.3190099 library(MASS) proportional_permute &lt;- function(x, prop) { indices &lt;- seq(1:length(x)) s_indices &lt;- sample(indices) n_shuffle &lt;- round(length(x) * prop) switch &lt;- sample(indices) x[s_indices[1:n_shuffle]] &lt;- x[switch[1:n_shuffle]] return(x) } # data all_df &lt;- data.frame() for (sim in 1:10) { for (samples in c(10, 50, 100, 1000)) { North_pole &lt;- runif(samples, 1, 10) South_pole &lt;- proportional_permute(North_pole, .05) + runif(samples, -5, 5) t_df &lt;- data.frame(nsize = rep(samples, samples), simulation = rep(sim, samples), North_pole, South_pole) all_df &lt;- rbind(all_df, t_df) } } ggplot(all_df, aes(x = North_pole, y = South_pole)) + geom_point() + geom_smooth(method = lm, se = FALSE) + theme_maik() + facet_wrap(~nsize) + transition_states( simulation, transition_length = 2, state_length = 1) + enter_fade() + exit_shrink() + ease_aes(&#39;sine-in-out&#39;) Würfel Daten # from: https://gist.github.com/dgrtwo/d590078aae86e24418a7cf5a9fb31ae3 # generate data # 1. numbers from 1 to 2000 # 2. generate just as many dice roll results # 3. define the frames for the animation and the amount of aggregation (data will explode) # 4. exclude rows such that only the dice rolls within the aggregated number stay # 5. count up the dice rolls for each of those intervalls simulation &lt;- tibble(roll = 1:2000) %&gt;% mutate(result = sample(6, n(), replace = TRUE)) %&gt;% crossing(nrolls = seq(10, 2000, 5)) %&gt;% filter(roll &lt;= nrolls) %&gt;% count(nrolls, result) # general overview head(simulation) ## # A tibble: 6 x 3 ## nrolls result n ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 10 1 1 ## 2 10 2 1 ## 3 10 3 2 ## 4 10 4 2 ## 5 10 5 3 ## 6 10 6 1 Plots # plot it with gganimate p1 &lt;- ggplot(simulation, aes(x = result, y = n, fill = factor(result))) + geom_col(position = &quot;identity&quot;) + transition_manual(nrolls) + view_follow() + scale_x_continuous(breaks = 1:6) + scale_fill_manual(values = viridis::viridis(6)) + guides(fill = F) + labs(y = &quot;# of rolls with this result&quot;, title = &quot;Distribution of Results of a Six-Sided Die\\nAfter { current_frame } Rolls:&quot;) + theme_maik() # animate it animate(p1) # again, but with different view options p2 &lt;- ggplot(simulation, aes(x = result, y = n, fill = factor(result))) + geom_col(position = &quot;identity&quot;) + transition_manual(nrolls) + view_static() + scale_x_continuous(breaks = 1:6) + scale_fill_manual(values = viridis::viridis(6)) + guides(fill = F) + labs(y = &quot;# of rolls with this result&quot;, title = &quot;Distribution of results after { current_frame } rolls \\nof a six-sided die&quot;) + theme_maik() # animate it animate(p2) Zeitverlauf # from: https://github.com/thomasp85/gganimate/wiki/Temperature-time-series airq &lt;- airquality airq$Month &lt;- format(ISOdate(2004, 1:12, 1), &quot;%B&quot;)[airq$Month] head(airq) ## Ozone Solar.R Wind Temp Month Day ## 1 41 190 7.4 67 May 1 ## 2 36 118 8.0 72 May 2 ## 3 12 149 12.6 74 May 3 ## 4 18 313 11.5 62 May 4 ## 5 NA NA 14.3 56 May 5 ## 6 28 NA 14.9 66 May 6 ggplot(airq, aes(Day, Temp, group = Month)) + geom_line() + geom_segment(aes(xend = 31, yend = Temp), linetype = 2, colour = &#39;grey&#39;) + geom_point(size = 2) + geom_text(aes(x = 31.1, label = Month), hjust = 0) + transition_reveal(Day) + coord_cartesian(clip = &#39;off&#39;) + labs(title = &#39;Temperature in New York&#39;, y = &#39;Temperature (°F)&#39;) + theme_maik() + theme(plot.margin = margin(5.5, 40, 5.5, 5.5)) Central Limit Theorem Daten # sample from non-normal distribution set.seed(1990) means &lt;- rep(NA, 1000) for (i in c(1:1000)) { samp100 &lt;- rexp(100) means[i] &lt;- round(mean(samp100), digits = 2) } # generate data tibble(sample = 1:1000, result = means) %&gt;% crossing(nsample = seq(5, 1000, 5)) %&gt;% filter(sample &lt;= nsample) %&gt;% count(nsample, result) -&gt; d head(d) ## # A tibble: 6 x 3 ## nsample result n ## &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 5 0.94 1 ## 2 5 1.01 1 ## 3 5 1.02 1 ## 4 5 1.14 1 ## 5 5 1.16 1 ## 6 10 0.86 1 Plots # plot the exponential distribution # non-normal distribution population &lt;- rexp(1000) nnormaldraw &lt;- ggplot() + aes(population) + geom_histogram(binwidth = .4, fill = viridis::cividis(1)) + labs(title = &quot;Exponential Distribution&quot;) + theme_maik() nnormaldraw # plot the sampling gif ggplot(d, aes(x = result, y = n, fill = factor(result))) + geom_col(position = &quot;identity&quot;) + transition_manual(nsample) + view_step(pause_length = 2, step_length = 1, nsteps = 5, wrap = F) + scale_fill_viridis_d() + guides(fill = F) + labs(y = &quot;# of samples with this result&quot;, title = &quot;Distribution of mean results after { current_frame } samples\\nfrom Exponential Distribution&quot;) + theme_maik() Interaktive Plots economics library(ggplot2) library(plotly) head(economics) ## # A tibble: 6 x 6 ## date pce pop psavert uempmed unemploy ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1967-07-01 507. 198712 12.6 4.5 2944 ## 2 1967-08-01 510. 198911 12.6 4.7 2945 ## 3 1967-09-01 516. 199113 11.9 4.6 2958 ## 4 1967-10-01 512. 199311 12.9 4.9 3143 ## 5 1967-11-01 517. 199498 12.8 4.7 3066 ## 6 1967-12-01 525. 199657 11.8 4.8 3018 # create static ggplot2 p &lt;- ggplot(economics, aes(date, psavert, size = psavert, alpha = 1 / psavert)) + geom_jitter() + geom_smooth(method = &quot;lm&quot;) + guides(size = F, alpha = F) + theme_maik() p # convert normal plot to interactive plot ggplotly(p, width = 700, height = 500) diamonds # keep sampled data the same set.seed(100) # sample data d &lt;- diamonds[sample(nrow(diamonds), 1000), ] head(d) ## carat cut color clarity depth table price x y z ## 16887 1.26 Ideal G SI1 59.6 57 6738 7.08 7.04 4.21 ## 3696 0.70 Ideal D VS2 62.7 57 3448 5.65 5.67 3.55 ## 31705 0.36 Ideal F SI1 62.0 56 770 4.59 4.54 2.83 ## 24270 2.10 Premium J SI2 59.1 58 12494 8.46 8.40 4.98 ## 11159 1.21 Premium D SI2 59.7 58 4946 7.06 6.96 4.19 ## 26116 2.00 Good E SI2 64.7 57 15393 7.75 7.86 5.05 # create plot p &lt;- ggplot(data = d, aes(x = carat, y = price)) + geom_point(size = 4) + geom_smooth(aes(colour = cut, fill = cut)) + facet_wrap(~cut) + theme_maik() # initialize interactive version ggplotly(p, width = 700, height = 500) "],
["eine-shiny-app.html", "Eine Shiny App", " Eine Shiny App Zum Ausprobieren einfach den Code in ein neues Skript kopieren, erforderliche Pakete installieren und in RStudio auf Run App klicken – mehr Infos gibts hier. library(shiny) library(shinythemes) library(psych) library(ggplot2) library(ggridges) # geom_density_ridges library(ggthemes) library(ggpubr) # theme_pubr(margin = F) + labs_pubr() library(plotly) # for plots with zoom functions and much more library(reshape2) # for melting and casting data frames library(DT) # for better tables # plot theme variations economist = theme_economist() # requires library(ggthemes) grey = theme_grey() # requires library(ggthemes) pubr = theme_pubr(margin = F) + labs_pubr() # requires library(pubr) clean = theme_tufte() + labs_pubr() + clean_theme() # requires library(pubr) and library(ggthemes) ## %######################################################%## #### Data Set for Central Limit Theorem #### ## %######################################################%## # generate 5000 values from an exponential distribution # with rate parameter=0.2 exp_data = rexp(n = 5000, rate = 0.2) # find mean of 5 values from an exponential distribution (repeat 5000 times) exp_5 = array(NA, dim = 5000) for (i in 1:5000) { exp_5[i] = mean(rexp(n = 5, rate = 0.2)) } # find mean of 50 values from an exponential distribution (repeat 5000 times) exp_50 = array(NA, dim = 5000) for (i in 1:5000) { exp_50[i] = mean(rexp(n = 50, rate = 0.2)) } # generate data frame d &lt;- data.frame(exp_data, exp_5, exp_50) # rename columns to make them nicer in the plot&#39;s legend names(d) &lt;- c(&quot;N = 1&quot;, &quot;N = 5&quot;, &quot;N = 50&quot;) # melt data so just one variable can be called to represent the three columns d &lt;- melt(d) ## %######################################################%## #### Shiny App #### ## %######################################################%## # Define UI for random distribution app ---- ui &lt;- fluidPage( # change the theme # to quickly check which one you like, add shinythemes::themeSelector() instead of shinytheme(&quot;paper&quot;) # others: &quot;superhero&quot; theme = shinytheme(&quot;lumen&quot;), # App title ---- titlePanel(&quot;Normal? Distributions&quot;), # Sidebar layout with input and output definitions ---- sidebarLayout( # Sidebar panel for inputs ---- sidebarPanel( # br() element to introduce extra vertical spacing ---- # conditional panel to enable/disable certain features only when they are (not) applicable depending on the numerical value of the tabs conditionalPanel(condition = &quot;input.tabs&lt;=2&quot;, # Input: slider for the number of observations to generate ---- numericInput(&quot;n&quot;, &quot;Number of Observations:&quot;, value = 500, min = 1, max = 12000), # Input: radio button for categorical selection, # alternative: selectInput radioButtons(&quot;dist&quot;, &quot;Distribution&quot;, c(&quot;Normal&quot; = &quot;rnorm&quot;, &quot;Exponential&quot; = &quot;rexp&quot;, &quot;Beta&quot; = &quot;rbeta&quot; # &quot;LogNormal&quot; = &quot;rlnorm&quot; ), inline = T), # checkbox to indicate whether a # transformation is to be applied selectInput(&quot;z&quot;, &quot;Transformation:&quot;, c(&quot;None&quot; = &quot;none&quot;, &quot;z (mean = 0, sd = 1)&quot; = &quot;scale&quot;, &quot;Logarithm&quot; = &quot;log&quot;, &quot;Square Root&quot; = &quot;sqrt&quot;) ), # conditional sidebar elements depending # on the choice of distribution uiOutput(&quot;slidermean&quot;), uiOutput(&quot;slidersd&quot;), uiOutput(&quot;sliderrate&quot;), uiOutput(&quot;slidershape1&quot;), uiOutput(&quot;slidershape2&quot;) ), # conditional panel to enable/disable certain features only when they are (not) applicable depending on the numerical value of the tabs conditionalPanel(condition = &quot;input.tabs&lt;=3&amp;input.tabs!=1&quot;, sliderInput(&quot;bin&quot;, &quot;Bar Width/Point Size:&quot;, value = 1.55, min = .01, max = 10), sliderInput(&quot;alpha&quot;, &quot;Opacity&quot;, value = .6, min = .1, max = 1), # Input: radio # button for # categorical # selection, # alternative: # selectInput # ---- radioButtons(&quot;col&quot;, &quot;Color:&quot;, c(&quot;Pink&quot; = &quot;deeppink1&quot;, &quot;Black&quot; = &quot;black&quot;, &quot;Blue&quot; = &quot;navyblue&quot;, &quot;Green&quot; = &quot;darkolivegreen&quot;, &quot;Magenta&quot; = &quot;darkmagenta&quot;), inline = T ), # drop down list selectInput(&quot;theme&quot;, &quot;Theme:&quot;, c(&quot;Publication&quot; = &quot;pubr&quot;, &quot;Economist&quot; = &quot;economist&quot;, &quot;Grey&quot; = &quot;grey&quot;, &quot;Clean&quot; = &quot;clean&quot;) ) ), # adjust width of side panel width = 3 ), # Main panel for displaying outputs ---- mainPanel( # Output: Tabset w/ plots, summary, and table ---- # id is used to refer back to the individual panels for the # condiditional side bar elements depending on the value tabsetPanel(id = &quot;tabs&quot;, type = &quot;tabs&quot;, # histogram with mean, uses ncount tabPanel(&quot;nCount Plot&quot;, value = 2, plotlyOutput(&quot;plot1&quot;, height = &quot;550px&quot;), fluidRow( column(9, offset = 0, h6(&quot;Test&quot;) ) ) ), # histogram with mean, +/- sd and t-significant areas # uses density statistic tabPanel(&quot;Density Plot&quot;, value = 2, plotlyOutput(&quot;plot2&quot;, height = &quot;550px&quot;)), # qq plot tabPanel(&quot;QQ Plot&quot;, value = 2, plotlyOutput(&quot;plot3&quot;, height = &quot;550px&quot;)), # boxplot with underlayed individual data points tabPanel(&quot;Boxplot&quot;, value = 2, plotlyOutput(&quot;plot4&quot;, height = &quot;550px&quot;)), # descriptive stats of generated data set tabPanel(&quot;Descriptive Stats&quot;, value = 1, fluidRow( h3(&quot;Describe&quot;), dataTableOutput(&quot;table&quot;), h3(&quot;Head&quot;), dataTableOutput(&quot;table4&quot;) ) ), # plot illustrating the cental limit # theorem with exponential distribution tabPanel(&quot;CLT1&quot;, value = 3, plotOutput(&quot;plot5&quot;, height = &quot;550px&quot;)), # classical plots with # means, sd error and # lines conntecting the means tabPanel(&quot;CLT2&quot;, value = 3, plotOutput(&quot;plot6&quot;, height = &quot;550px&quot;)), # descriptive stats of exponential data tabPanel(&quot;CLT Data&quot;, value = 4, # multiple outputs in one panel via fluidRow fluidRow( # Title element h3(&quot;DescribeBy&quot;), dataTableOutput(&quot;table2&quot;), h3(&quot;Head&quot;), dataTableOutput(&quot;table3&quot;) )) ), # adjust width of main panel width = 9) ) ) # Define server logic for random distribution app ---- server &lt;- function(input, output) { # Reactive expression to generate the requested data set according to n, mean and sd ---- # This is called whenever the inputs change. The output functions # defined below then use the value computed from this expression # generates the same dataset for all the manipulatable plots and tables # this allows the user to jump from numeric representations to # different types of visualization x &lt;- reactive({ # react to the checkbox for z-transformation: # if FALSE, do the usual if (input$z == &quot;none&quot;) { if (input$dist == &quot;rnorm&quot;) {x &lt;- rnorm(input$n, mean = input$mean, sd = input$sd)} # else if(input$dist == &quot;rlnorm&quot;) # {x &lt;- rlnorm(input$n, meanlog = input$mean, sdlog=input$sd)} else if (input$dist == &quot;rexp&quot;) { x &lt;- rexp(input$n, rate = input$rate) } else { x &lt;- rbeta(input$n, input$shape1, input$shape2) } } # if TRUE, apply the transformation specified by the input # either scale, log or sqrt else { if (input$dist == &quot;rnorm&quot;) {x &lt;- get(input$z)(rnorm(input$n, mean = input$mean, sd = input$sd))} # else if(input$dist == &quot;rlnorm&quot;) # {x &lt;- rlnorm(input$n, meanlog = input$mean, sdlog=input$sd)} else if (input$dist == &quot;rexp&quot;) { x &lt;- get(input$z)(rexp(input$n, rate = input$rate)) } else { x &lt;- get(input$z)(rbeta(input$n, input$shape1, input$shape2)) } } }) # Generate a plot of the data ---- # Also uses the inputs to build the plot label. Note that the # dependencies on the inputs and the data reactive expression are # both tracked, and all expressions are called in the sequence # implied by the dependency graph. # histogram with mean # uses ncount statistic output$plot1 &lt;- renderPlotly({ # dist &lt;- input$dist # compute descriptive stats, will use skew and kurtosis later s &lt;- describe(x()) # extract mean and sd mean &lt;- round(s[3], digits = 4) sd &lt;- round(s[4], digits = 4) # extract skew skew &lt;- round(s[11], digits = 4) # extract kurtosis kurtosis &lt;- round(s[12], digits = 4) # add kolmogorov-smirnov normality test # mean and sd have the same values as in the sampling distribution ks &lt;- ks.test(x(), &quot;pnorm&quot;, mean = mean(x()), sd = sd(x())) ksp &lt;- round(as.data.frame(ks[2])[1, 1], digits = 4) text &lt;- paste(&quot; Mean: &quot;, mean, &quot;\\n Std. Dev: &quot;, sd, &quot;\\n Skew: &quot;, skew, &quot;\\n Kurtosis: &quot;, kurtosis, &quot;\\n Kolmogorov-Smirnov p: &quot;, ksp) ggplot() + aes(x()) + geom_histogram(aes(y = ..ncount..), binwidth = input$bin, fill = input$col, alpha = input$alpha) + labs(x = paste(&quot;Number of Observations: &quot;, input$n), y = &quot;&quot;) + # annotate(&quot;text&quot;, -Inf, Inf, label = text, size=4.5, hjust = 0, vjust=1) + get(input$theme) + # scale_x_continuous(limits = c(input$mean-(5*input$sd), input$mean+(5*input$sd))) + geom_vline(aes(xintercept = mean(x(), na.rm = T)), color = &quot;black&quot;, linetype = &quot;dotted&quot;, size = 1) }) # histogram with mean, +/- sd and t-significant areas # uses density statistic output$plot2 &lt;- renderPlotly({ # dist &lt;- input$dist # compute descriptive stats, will use skew and kurtosis later s &lt;- describe(x()) # extract mean and sd mean &lt;- round(s[3], digits = 4) sd &lt;- round(s[4], digits = 4) # extract skew skew &lt;- round(s[11], digits = 4) # extract kurtosis kurtosis &lt;- round(s[12], digits = 4) # add kolmogorov-smirnov normality test # mean and sd have the same values as in the sampling distribution ks &lt;- ks.test(x(), &quot;pnorm&quot;, mean = mean(x()), sd = sd(x())) ksp &lt;- round(as.data.frame(ks[2])[1, 1], digits = 4) text &lt;- paste(&quot; Mean: &quot;, mean, &quot;\\n Std. Dev: &quot;, sd, &quot;\\n Skew: &quot;, skew, &quot;\\n Kurtosis: &quot;, kurtosis, &quot;\\n Kolmogorov-Smirnov p: &quot;, ksp) # thresholds for shaded areas; # modelled after two-tailed # t-distribution thresholdup &lt;- quantile(x(), prob = 0.975)[[1]] thresholddown &lt;- quantile(x(), prob = 0.025)[[1]] ggplot() + aes(x()) + geom_histogram(aes(y = ..density..), binwidth = input$bin, fill = input$col, alpha = input$alpha) + stat_function(fun = dnorm, args = list(mean = mean(x()), sd = sd(x())), xlim = c(mean(x()), mean(x()) + sd(x())), geom = &quot;area&quot;, color = &quot;black&quot;, fill = &quot;transparent&quot;, size = .7, linetype = &quot;dotted&quot;) + stat_function(fun = dnorm, args = list(mean = mean(x()), sd = sd(x())), xlim = c(mean(x()) - sd(x()), mean(x())), geom = &quot;area&quot;, color = &quot;black&quot;, fill = &quot;transparent&quot;, size = .7, linetype = &quot;dotted&quot;) + stat_function(fun = dnorm, args = list(mean = mean(x()), sd = sd(x())), xlim = c(thresholdup, max(x())), geom = &quot;area&quot;, fill = &quot;black&quot;, alpha = .4) + stat_function(fun = dnorm, args = list(mean = mean(x()), sd = sd(x())), xlim = c(min(x()), thresholddown), geom = &quot;area&quot;, fill = &quot;black&quot;, alpha = .4) + stat_function(fun = dnorm, args = list(mean = mean(x()), sd = sd(x())), color = &quot;black&quot;, size = .7, alpha = .5) + labs(x = paste(&quot;Number of Observations: &quot;, input$n), y = &quot;&quot;) + # annotate(&quot;text&quot;, -Inf, Inf, label = text, size=4.5, hjust = 0, vjust=1) + # scale_x_continuous(limits = c(input$mean-(5*input$sd), input$mean+(5*input$sd))) + get(input$theme) }) # qq plot output$plot3 &lt;- renderPlotly({ # compute descriptive stats, will use skew and kurtosis later s &lt;- describe(x()) # extract mean and sd mean &lt;- round(s[3], digits = 4) sd &lt;- round(s[4], digits = 4) # extract skew skew &lt;- round(s[11], digits = 4) # extract kurtosis kurtosis &lt;- round(s[12], digits = 4) # add kolmogorov-smirnov normality test # mean and sd have the same values as in the sampling distribution ks &lt;- ks.test(x(), &quot;pnorm&quot;, mean = mean(x()), sd = sd(x())) ksp &lt;- round(as.data.frame(ks[2])[1, 1], digits = 4) text &lt;- paste(&quot; Mean: &quot;, mean, &quot;\\n Std. Dev: &quot;, sd, &quot;\\n Skew: &quot;, skew, &quot;\\n Kurtosis: &quot;, kurtosis, &quot;\\n Kolmogorov-Smirnov p: &quot;, ksp) ggplot() + aes(sample = x()) + geom_qq(size = input$bin, color = input$col, alpha = input$alpha) + get(input$theme) + geom_abline(intercept = mean(x()), slope = sd(x()), color = &quot;black&quot;, size = 1, alpha = 0.8) + labs(x = &quot;&quot;, y = &quot;&quot;) # + annotate(&quot;text&quot;, -Inf, Inf, label = text, size=4.5, hjust = 0, vjust=1) }) # boxplot with underlayed individual data points output$plot4 &lt;- renderPlotly({ # compute descriptive stats, will use skew and kurtosis later s &lt;- psych::describe(x()) # extract mean and sd mean &lt;- round(s[3], digits = 4) sd &lt;- round(s[4], digits = 4) # extract skew skew &lt;- round(s[11], digits = 4) # extract kurtosis kurtosis &lt;- round(s[12], digits = 4) # add kolmogorov-smirnov normality test # mean and sd have the same values as in the sampling distribution ks &lt;- ks.test(x(), &quot;pnorm&quot;, mean = mean(x()), sd = sd(x())) ksp &lt;- round(as.data.frame(ks[2])[1, 1], digits = 4) text &lt;- paste(&quot; Mean: &quot;, mean, &quot;\\n Std. Dev: &quot;, sd, &quot;\\n Skew: &quot;, skew, &quot;\\n Kurtosis: &quot;, kurtosis, &quot;\\n Kolmogorov-Smirnov p: &quot;, ksp) ggplot() + aes(y = x()) + geom_jitter(aes(x = 0), color = input$col, alpha = input$alpha / 3, size = input$bin, width = 0.37) + geom_boxplot(outlier.shape = NA, size = .7, outlier.color = NA) + get(input$theme) + labs(x = &quot;&quot;, y = &quot;&quot;) + theme(axis.text.x = element_blank(), axis.ticks.x = element_blank()) # + annotate(&quot;text&quot;, -Inf, Inf, label = text, size=4.5, hjust = 0, vjust=1) }) # describe function including IQR and quantiles output$table &lt;- renderDataTable({ # describe the data, including IQR and Quantiles, requires library(psych) statrandom &lt;- describe(x(), IQR = T, quant = c(.25, .75)) # rename the columns names(statrandom) &lt;- c(&quot;Row&quot;, &quot;Observations&quot;, &quot;Mean&quot;, &quot;Standard Deviation&quot;, &quot;Median&quot;, &quot;Trimmed&quot;, &quot;MAD&quot;, &quot;Min&quot;, &quot;Max&quot;, &quot;Range&quot;, &quot;Skew&quot;, &quot;Kurtosis&quot;, &quot;Standard Error&quot;, &quot;IQR&quot;, &quot;Q 0.25&quot;, &quot;Q 0.75&quot;) # to reduce the number of digits, embedd in print command print(statrandom[2:length(statrandom)], digits = 3) # do not display the uninformative row names }, rownames = FALSE) # plots detailing the central limit # theorem by drawing from exponential # distribution with increasing number # of means, see above output$plot5 &lt;- renderPlot({ # newer version, requires library(ggridges) ggplot(d, aes(x = value, y = variable)) + geom_density_ridges(aes(fill = variable, color = variable), scale = input$bin, alpha = input$alpha) + get(input$theme) + scale_x_continuous(limits = c(-1, 15)) + labs(x = &quot;Mean of n = 1, 5 and 50 Data Points from Exponential Distribution taken 5000 times&quot;, y = &quot;&quot;, fill = &quot;Number of Observations for Mean&quot;, color = &quot;Number of Observations for Mean&quot;) + scale_fill_manual(values = c(&quot;deeppink1&quot;, &quot;darkmagenta&quot;, &quot;navyblue&quot;)) + scale_color_manual(values = c(&quot;deeppink1&quot;, &quot;darkmagenta&quot;, &quot;navyblue&quot;)) }) # classical plots with means, sd error and lines conntecting them output$plot6 &lt;- renderPlot({ ggplot(d, aes(x = variable, y = value, color = variable, shape = variable, group = variable)) + stat_summary(fun.y = mean, geom = &quot;line&quot;, group = 1, color = input$col) + stat_summary(fun.y = mean, geom = &quot;point&quot;, size = input$bin) + get(input$theme) + scale_fill_manual(values = c(&quot;deeppink1&quot;, &quot;darkmagenta&quot;, &quot;navyblue&quot;)) + scale_color_manual(values = c(&quot;deeppink1&quot;, &quot;darkmagenta&quot;, &quot;navyblue&quot;)) + stat_summary(fun.data = mean_se, geom = &quot;errorbar&quot;, width = input$bin / 10, color = input$col) + labs(x = &quot;Mean of n = 1, 5 and 50 Data Points from Exponential Distribution taken 5000 times&quot;, y = &quot;&quot;, fill = &quot;Number of Observations for Mean&quot;, color = &quot;Number of Observations for Mean&quot;, shape = &quot;Number of Observations for Mean&quot;, group = &quot;Number of Observations for Mean&quot;) }) # describe CLT data by factor levels output$table2 &lt;- renderDataTable({ # describe the data, requires library(psych) stat &lt;- describeBy(d$value, d$variable, mat = T, digits = 3) # rename the columns names(stat) &lt;- c(&quot;Row&quot;, &quot;Group&quot;, &quot;Vars&quot;, &quot;Observations&quot;, &quot;Mean&quot;, &quot;Standard Deviation&quot;, &quot;Median&quot;, &quot;Trimmed&quot;, &quot;MAD&quot;, &quot;Min&quot;, &quot;Max&quot;, &quot;Range&quot;, &quot;Skew&quot;, &quot;Kurtosis&quot;, &quot;Standard Error&quot;) stat[2:length(stat)] # do not display the uninformative row names }, rownames = FALSE) # show CLT data output$table3 &lt;- renderDataTable({ # format function to display less decimal numbers format(head(d, n = length(d$variable)), digits = 2) # do not display the uninformative row names }, rownames = FALSE) # show dynamic data output$table4 &lt;- renderDataTable({ # format function to display less decimal numbers data &lt;- as.data.frame(x()) names(data) &lt;- &quot;Data&quot; format(head(data, n = input$n), digits = 2) # do not display the uninformative row names }, rownames = FALSE) # CONditionally apparent UI elements ---- output$slidermean &lt;- renderUI({ if (input$dist %in% c(&quot;rnorm&quot;, &quot;rlnorm&quot;)) { sliderInput(&quot;mean&quot;, &quot;Mean&quot;, value = 150, min = 125, max = 175) } }) output$slidersd &lt;- renderUI({ if (input$dist %in% c(&quot;rnorm&quot;, &quot;rlnorm&quot;)) { sliderInput(&quot;sd&quot;, &quot;SD&quot;, value = 15, min = 0.1, max = 30) } }) output$sliderrate &lt;- renderUI({ if (input$dist == &quot;rexp&quot;) { sliderInput(&quot;rate&quot;, &quot;Rate&quot;, value = 0.2, min = 0.01, max = 0.9) } }) output$slidershape1 &lt;- renderUI({ if (input$dist == &quot;rbeta&quot;) { sliderInput(&quot;shape1&quot;, &quot;Shape 1&quot;, value = 5, min = 0.01, max = 10) } }) output$slidershape2 &lt;- renderUI({ if (input$dist == &quot;rbeta&quot;) { sliderInput(&quot;shape2&quot;, &quot;Shape 2&quot;, value = 2, min = 0.01, max = 10) } }) # END OF: server &lt;- function(input, output) { } # Create Shiny app ---- shinyApp(ui, server) "],
["extrasitzung.html", "Extrasitzung Einfach Textdateien speichern Funktionen aus dem tidyverse For-Loops Funktionen, die das Coden erleichtern Janitor-Paket Automatische Tabellen mit gtsummary Automatische Berichte Bildverarbeitung (und -bearbeitung) in R Popularity Contest Anhang: Die wichtigsten Funktionen", " Extrasitzung Einfach Textdateien speichern Daten library(ggplot2) d &lt;- as.data.frame(msleep) # from ggplot2 package head(d) ## name genus vore order conservation ## 1 Cheetah Acinonyx carni Carnivora lc ## 2 Owl monkey Aotus omni Primates &lt;NA&gt; ## 3 Mountain beaver Aplodontia herbi Rodentia nt ## 4 Greater short-tailed shrew Blarina omni Soricomorpha lc ## 5 Cow Bos herbi Artiodactyla domesticated ## 6 Three-toed sloth Bradypus herbi Pilosa &lt;NA&gt; ## sleep_total sleep_rem sleep_cycle awake brainwt bodywt ## 1 12.1 NA NA 11.9 NA 50.000 ## 2 17.0 1.8 NA 7.0 0.01550 0.480 ## 3 14.4 2.4 NA 9.6 NA 1.350 ## 4 14.9 2.3 0.1333333 9.1 0.00029 0.019 ## 5 4.0 0.7 0.6666667 20.0 0.42300 600.000 ## 6 14.4 2.2 0.7666667 9.6 NA 3.850 sink Auch im Arbeitsverzeichnis (\\(\\rightarrow\\) getwd) cat erzeugt den Textstring in seinem Argument auch in der Textdatei, nützlich für Titel, wenn mehrere Berechnungen in dieselbe Datei sollen \\n erzeugt einen Zeilenumbruch Mit with erspart man sich ein kleines bisschen Schreibarbeit, weil man den Datensatz selbst nicht immer wieder innerhalb der Funktion eingeben muss Wichtig: sink immer mit sink beenden, wenn die Textdatei alle Infos enthält, die man abspeichern wollte library(psych) sink(&quot;descriptive_stats.txt&quot;) cat(&quot;\\n\\n###################################################################&quot;) with(d, describeBy(price, cut)) cat(&quot;\\n\\n###################################################################&quot;) sink() Funktionen aus dem tidyverse Tidyverse https://www.tidyverse.org install.packages(&quot;tidyverse&quot;) library(tidyverse) Funktionen, die man kennen sollte mutate() # spalten erstellen oder verändern summarise() # mehrere werte auf einen reduzieren (zB mean) group_by() # datenblatt gruppieren (wie by() ) %&gt;% # &quot;pipe&quot; Pipe %&gt;% https://twitter.com/andrewheiss/status/1173743447171354624 Beispiele # Spalte für Faktor Issueness anlegen d &lt;- mutate(d, issue = case_when( str_detect(d$V8, &quot;_at&quot;) ~ &quot;at-issue&quot;, str_detect(d$V8, &quot;_non&quot;) ~ &quot;non-at-issue&quot;)) # Spalten zu Faktoren konvertieren d &lt;- mutate_at(d, c(&quot;issue&quot;, &quot;trigger&quot;, &quot;itemid&quot;, &quot;lex&quot;), funs(factor(.))) # neue Spalte und Faktoren mit Pipes d &lt;- d %&gt;% mutate(issue = case_when( str_detect(V8, &quot;_at&quot;) ~ &quot;at-issue&quot;, str_detect(V8, &quot;_non&quot;) ~ &quot;non-at-issue&quot;)) %&gt;% mutate_at(c(&quot;issue&quot;, &quot;trigger&quot;, &quot;itemid&quot;, &quot;lex&quot;), funs(factor(.))) Weitere Beispiele Mit dem altbekannten Diamantendatenset: d &lt;- diamonds head(d) ## carat cut color clarity depth table price x y z ## 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 ## 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 ## 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 ## 4 0.29 Premium I VS2 62.4 58 334 4.20 4.23 2.63 ## 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 ## 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 Vorkommen zählen # reihen zählen und nach color und cut aufspalten d %&gt;% count(color, cut) %&gt;% head ## # A tibble: 6 x 3 ## color cut n ## &lt;ord&gt; &lt;ord&gt; &lt;int&gt; ## 1 D Fair 163 ## 2 D Good 662 ## 3 D Very Good 1513 ## 4 D Premium 1603 ## 5 D Ideal 2834 ## 6 E Fair 224 Zählen und Sortieren d %&gt;% count(color, cut) %&gt;% arrange(n) %&gt;% head() ## # A tibble: 6 x 3 ## color cut n ## &lt;ord&gt; &lt;ord&gt; &lt;int&gt; ## 1 J Fair 119 ## 2 D Fair 163 ## 3 I Fair 175 ## 4 E Fair 224 ## 5 H Fair 303 ## 6 J Good 307 Sortieren und Spalten auswählen d %&gt;% arrange(desc(price)) %&gt;% dplyr::select(price, carat, color, cut) %&gt;% head() ## price carat color cut ## 1 18823 2.29 I Premium ## 2 18818 2.00 G Very Good ## 3 18806 1.51 G Ideal ## 4 18804 2.07 G Ideal ## 5 18803 2.00 H Very Good ## 6 18797 2.29 I Premium Gruppieren und rechnen d %&gt;% group_by(color) %&gt;% summarise(mprice = mean(price), medprice = median(price)) ## # A tibble: 7 x 3 ## color mprice medprice ## &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 D 3170. 1838 ## 2 E 3077. 1739 ## 3 F 3725. 2344. ## 4 G 3999. 2242 ## 5 H 4487. 3460 ## 6 I 5092. 3730 ## 7 J 5324. 4234 Gruppieren, berechnen, filtern # nach schliff gruppieren # spalte mit mittlerem preis hinzufügen # und maximale verkaufspreise anzeigen lassen d %&gt;% group_by(cut) %&gt;% mutate(mprice = mean(price)) %&gt;% filter(price == max(price)) ## # A tibble: 5 x 11 ## # Groups: cut [5] ## carat cut color clarity depth table price x y z mprice ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2.01 Fair G SI1 70.6 64 18574 7.43 6.64 4.69 4359. ## 2 2.8 Good G SI2 63.8 58 18788 8.9 8.85 0 3929. ## 3 1.51 Ideal G IF 61.7 55 18806 7.37 7.41 4.56 3458. ## 4 2 Very Good G SI1 63.5 56 18818 7.9 7.97 5.04 3982. ## 5 2.29 Premium I VS2 60.8 60 18823 8.5 8.47 5.16 4584. Gruppenkorrelationen plotten # nach farbe gruppieren # und korrelation zwischen preis und gewicht berechnen # und plotten d %&gt;% group_by(color) %&gt;% summarise(price_carat_cor = cor(price, carat)) %&gt;% ggplot(aes(color, price_carat_cor, group = 1)) + geom_path() Filtern und plotten # nur zeilen mit preis &gt; 15000 und schliff == Fair # im plot darstellen d %&gt;% filter(price &gt; 15000, cut == &quot;Fair&quot;) %&gt;% ggplot(aes(carat, price)) + geom_point() Ein Tutorial vignette(&quot;dplyr&quot;) Speed-Up Bei größeren Datensätzen (Korpusdaten, EGG, Eye-Tracking) kann es vorkommen, dass die inhärenten Funktionen von R und dem tidyverse zu langsam sind. Wem eine solche Situation häufiger unterkommt, sollte es sich überlegen, die unteren beiden Pakete in seinen Workflow zu integrieren. Beide bieten tidyverse-Syntax mit der Geschwindigkeit von data.table, einem Paket, das zwar wesentlich schneller als andere Lösungen ist, aber einiges an Komplexität mit sich bringt. https://github.com/TysonStanley/tidyfast https://github.com/markfairbanks/tidytable For-Loops Loops # ausgabe aller geraden zahlen zwischen 1 und 15 for (i in 1:15) { # 1:15 = zahlenreihe 1 bis 15 if (i %% 2) { # wenn die jeweilige zahl durch 2 teilbar ist, ... next} # dann gehe zum nächsten Schritt und print(i) # gib sie aus } ## [1] 2 ## [1] 4 ## [1] 6 ## [1] 8 ## [1] 10 ## [1] 12 ## [1] 14 Loops # summe: x_1 + x_2 + ... + x_n my.sum &lt;- function(vector) { sum &lt;- 0 # ist nötig, damit wir immer wieder neu bei # null beginnen, wenn wir die funktion nochmal benutzen wollen for (i in vector) {sum &lt;- sum + i } sum } a &lt;- c(1, 2, 3, 4, 5, 6) my.sum(a) ## [1] 21 sum(a) ## [1] 21 my.sum(c(1, 2, 3, 4, 5, 6)) ## [1] 21 Übung Schreibe eine Funktion “my.mean”, die den Mittelwert eines Vektors berechnet Schreibe eine Funktion “my.var”, die die Varianz eines Vektors berechnet Lösungen Schreibe eine Funktion “my.mean”, die den Mittelwert eines Vektors berechnet # mittelwert: (x_1 + x_2 + ... + x_n)/n my.mean &lt;- function(vector) { sum &lt;- 0 # wie oben for (i in vector) { sum &lt;- sum + i } sum / length(vector) } a &lt;- c(1, 2, 3, 4, 5, 6) my.mean(a) == mean(a) ## [1] TRUE my.mean(a) ## [1] 3.5 Lösungen Schreibe eine Funktion “my.var”, die die Varianz eines Vektors berechnet # varianz: # ((x_1-x_mittelwert)+(x_n-x_mittelwert) ... + (x_n-x_mittelwert))/n my.var &lt;- function(vector) { sum &lt;- 0 meanie &lt;- my.mean(vector) for (i in vector) { sum &lt;- sum + (i - meanie)^2 } sum / (length(vector) - 1) } my.var(a) == var(a) ## [1] TRUE my.var(a) ## [1] 3.5 Funktionen, die das Coden erleichtern Daten d &lt;- InsectSprays head(d) ## count spray ## 1 10 A ## 2 7 A ## 3 20 A ## 4 14 A ## 5 14 A ## 6 12 A str(d) ## &#39;data.frame&#39;: 72 obs. of 2 variables: ## $ count: num 10 7 20 14 14 12 10 23 17 20 ... ## $ spray: Factor w/ 6 levels &quot;A&quot;,&quot;B&quot;,&quot;C&quot;,&quot;D&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... by() by(d$count, d$spray, mean) ## d$spray: A ## [1] 14.5 ## ------------------------------------------------------------ ## d$spray: B ## [1] 15.33333 ## ------------------------------------------------------------ ## d$spray: C ## [1] 2.083333 ## ------------------------------------------------------------ ## d$spray: D ## [1] 4.916667 ## ------------------------------------------------------------ ## d$spray: E ## [1] 3.5 ## ------------------------------------------------------------ ## d$spray: F ## [1] 16.66667 with() Struktur: with(Variable, Befehl/Funktion) Der Befehl/die Funktion innerhalb von within wird mit der angegebenen Variable durchgeführt with() Hier die Daten, mit denen ich die Funktionsweise von with illustrieren werde: head(ToothGrowth) ## len supp dose ## 1 4.2 VC 0.5 ## 2 11.5 VC 0.5 ## 3 7.3 VC 0.5 ## 4 5.8 VC 0.5 ## 5 6.4 VC 0.5 ## 6 10.0 VC 0.5 str(ToothGrowth) ## &#39;data.frame&#39;: 60 obs. of 3 variables: ## $ len : num 4.2 11.5 7.3 5.8 6.4 10 11.2 11.2 5.2 7 ... ## $ supp: Factor w/ 2 levels &quot;OJ&quot;,&quot;VC&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ dose: num 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 ... with() with(ToothGrowth, { boxplot(len ~ dose, boxwex = 0.25, at = 1:3 - 0.2, subset = (supp == &quot;VC&quot;), col = &quot;yellow&quot;, main = &quot;Guinea Pigs&#39; Tooth Growth&quot;, xlab = &quot;Vitamin C dose mg&quot;, ylab = &quot;tooth length&quot;, ylim = c(0, 35)) boxplot(len ~ dose, add = TRUE, boxwex = 0.25, at = 1:3 + 0.2, subset = supp == &quot;OJ&quot;, col = &quot;orange&quot;) legend(2, 9, c(&quot;Ascorbic acid&quot;, &quot;Orange juice&quot;), fill = c(&quot;yellow&quot;, &quot;orange&quot;)) }) attach() Alle folgenden Befehle werden mit der angegebenen Variable ausgeführt Eignet sich, wenn mit wenigen Tabellen/Variablen gleichzeitig gearbeitet wird Eine Variable, die mit attach ausgewählt wurde, kann mit detach wieder entfernt werden, wenn beispielsweise eine neue Variable verwendet werden soll attach(d) mean(count) ## [1] 9.5 sd(count) ## [1] 7.203286 median(count) ## [1] 7 mad(count, constant = 1) ## [1] 5 detach(d) apply()-Familie tapply(Vektorvariable/Spaltenvariable, INDEX = Vektorvariable/Spalte, Befehl) für Tabellen/Data-frames Als zweites Argument muss eine Spalte angegeben werden, nach deren Inhalt die Funktion ausgeführt wird Es werden quasi Subsets nach dem Inhalt der angegebenen Spalte erstellt, über die dann einzeln der Befehl angewandt wird. Alternative Befehle: sapply, lapply apply()-Familie # Berechnet Mittelwert und Median der Insektenanzahl # für den jeweiligen Typ von Insektenspray tapply(d$count, d$spray, mean) ## A B C D E F ## 14.500000 15.333333 2.083333 4.916667 3.500000 16.666667 tapply(d$count, d$spray, median) ## A B C D E F ## 14.0 16.5 1.5 5.0 3.0 15.0 Unterschied by() und tapply() Janitor-Paket Installieren und laden install.packages(&quot;janitor&quot;) library(janitor) Bessere Häufigkeitstabellen Alternative zu table Gibt Ergebnisse als data frame aus d &lt;- diamonds tab &lt;- tabyl(d, color, cut) tab # absolute Häufigkeiten ## color Fair Good Very Good Premium Ideal ## D 163 662 1513 1603 2834 ## E 224 933 2400 2337 3903 ## F 312 909 2164 2331 3826 ## G 314 871 2299 2924 4884 ## H 303 702 1824 2360 3115 ## I 175 522 1204 1428 2093 ## J 119 307 678 808 896 adorn_percentages(tab, denominator = &quot;row&quot;) # relative Häufigkeiten ## color Fair Good Very Good Premium Ideal ## D 0.02405904 0.09771218 0.2233210 0.2366052 0.4183026 ## E 0.02286414 0.09523323 0.2449730 0.2385424 0.3983873 ## F 0.03269755 0.09526305 0.2267868 0.2442884 0.4009642 ## G 0.02780730 0.07713425 0.2035955 0.2589444 0.4325186 ## H 0.03648844 0.08453757 0.2196532 0.2842004 0.3751204 ## I 0.03227591 0.09627444 0.2220583 0.2633714 0.3860199 ## J 0.04237892 0.10933048 0.2414530 0.2877493 0.3190883 Bessere Häufigkeitstabellen denominator-Argument verändert, wie Häufigkeiten berechnet werden adorn_pct_formatting: *100 plus % zusätzlich möglich: adorn_totals adorn_pct_formatting(adorn_percentages(tab, denominator = &quot;col&quot;)) ## color Fair Good Very Good Premium Ideal ## D 10.1% 13.5% 12.5% 11.6% 13.2% ## E 13.9% 19.0% 19.9% 16.9% 18.1% ## F 19.4% 18.5% 17.9% 16.9% 17.8% ## G 19.5% 17.8% 19.0% 21.2% 22.7% ## H 18.8% 14.3% 15.1% 17.1% 14.5% ## I 10.9% 10.6% 10.0% 10.4% 9.7% ## J 7.4% 6.3% 5.6% 5.9% 4.2% adorn_pct_formatting(adorn_percentages(tab, denominator = &quot;all&quot;)) ## color Fair Good Very Good Premium Ideal ## D 0.3% 1.2% 2.8% 3.0% 5.3% ## E 0.4% 1.7% 4.4% 4.3% 7.2% ## F 0.6% 1.7% 4.0% 4.3% 7.1% ## G 0.6% 1.6% 4.3% 5.4% 9.1% ## H 0.6% 1.3% 3.4% 4.4% 5.8% ## I 0.3% 1.0% 2.2% 2.6% 3.9% ## J 0.2% 0.6% 1.3% 1.5% 1.7% Leere Zeilen und Spalten entfernen q &lt;- data.frame(v1 = c(1, NA, 3), v2 = c(NA, NA, NA), v3 = c(&quot;a&quot;, NA, &quot;b&quot;)) q ## v1 v2 v3 ## 1 1 NA a ## 2 NA NA &lt;NA&gt; ## 3 3 NA b remove_empty(q, c(&quot;rows&quot;, &quot;cols&quot;)) ## v1 v3 ## 1 1 a ## 3 3 b Spalten mit konstanten Werten entfernen (a &lt;- data.frame(good = 1:3, boring = &quot;the same&quot;)) ## good boring ## 1 1 the same ## 2 2 the same ## 3 3 the same remove_constant(a) ## good ## 1 1 ## 2 2 ## 3 3 clean_names() # Create a data.frame with dirty names test_df &lt;- as.data.frame(matrix(ncol = 6)) names(test_df) &lt;- c(&quot;firstName&quot;, &quot;ábc@!*&quot;, &quot;% successful (2009)&quot;, &quot;REPEAT VALUE&quot;, &quot;REPEAT VALUE&quot;, &quot;&quot;) test_df ## firstName ábc@!* % successful (2009) REPEAT VALUE REPEAT VALUE ## 1 NA NA NA NA NA NA clean_names(test_df) ## first_name abc percent_successful_2009 repeat_value repeat_value_2 x ## 1 NA NA NA NA NA NA Automatische Tabellen mit gtsummary Automatische Tabellen mit gtsummary library(gtsummary) options( gtsummary.add_p.test.continuous_by2 = &quot;t.test&quot;, gtsummary.add_p.test.continuous = &quot;aov&quot; ) d &lt;- diamonds sum_tbl &lt;- d %&gt;% select(price, cut, carat) %&gt;% mutate(cut = factor(cut, ordered = F)) %&gt;% filter(cut %in% c(&quot;Premium&quot;, &quot;Fair&quot;, &quot;Good&quot;)) %&gt;% droplevels() %&gt;% tbl_summary(by = cut, statistic = list(all_continuous() ~ &quot;{mean} ({sd})&quot;), label = list(carat ~ &quot;Carat&quot;, price ~ &quot;Price&quot;)) %&gt;% add_p() Automatische Tabellen sum_tbl html { font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Helvetica Neue', 'Fira Sans', 'Droid Sans', Arial, sans-serif; } #lvpgiypvtz .gt_table { display: table; border-collapse: collapse; margin-left: auto; /* table.margin.left */ margin-right: auto; /* table.margin.right */ color: #333333; font-size: 16px; /* table.font.size */ background-color: #FFFFFF; /* table.background.color */ width: auto; /* table.width */ border-top-style: solid; /* table.border.top.style */ border-top-width: 2px; /* table.border.top.width */ border-top-color: #A8A8A8; /* table.border.top.color */ border-bottom-style: solid; /* table.border.bottom.style */ border-bottom-width: 2px; /* table.border.bottom.width */ border-bottom-color: #A8A8A8; /* table.border.bottom.color */ } #lvpgiypvtz .gt_heading { background-color: #FFFFFF; /* heading.background.color */ border-bottom-color: #FFFFFF; /* table.background.color */ border-left-style: hidden; /* heading.border.lr.style */ border-left-width: 1px; /* heading.border.lr.width */ border-left-color: #D3D3D3; /* heading.border.lr.color */ border-right-style: hidden; /* heading.border.lr.style */ border-right-width: 1px; /* heading.border.lr.width */ border-right-color: #D3D3D3; /* heading.border.lr.color */ } #lvpgiypvtz .gt_title { color: #333333; font-size: 125%; /* heading.title.font.size */ font-weight: initial; /* heading.title.font.weight */ padding-top: 4px; /* heading.top.padding - not yet used */ padding-bottom: 4px; border-bottom-color: #FFFFFF; /* table.background.color */ border-bottom-width: 0; } #lvpgiypvtz .gt_subtitle { color: #333333; font-size: 85%; /* heading.subtitle.font.size */ font-weight: initial; /* heading.subtitle.font.weight */ padding-top: 0; padding-bottom: 4px; /* heading.bottom.padding - not yet used */ border-top-color: #FFFFFF; /* table.background.color */ border-top-width: 0; } #lvpgiypvtz .gt_bottom_border { border-bottom-style: solid; /* heading.border.bottom.style */ border-bottom-width: 2px; /* heading.border.bottom.width */ border-bottom-color: #D3D3D3; /* heading.border.bottom.color */ } #lvpgiypvtz .gt_column_spanner { border-bottom-style: solid; border-bottom-width: 2px; border-bottom-color: #D3D3D3; padding-top: 4px; padding-bottom: 4px; } #lvpgiypvtz .gt_col_headings { border-top-style: solid; /* column_labels.border.top.style */ border-top-width: 2px; /* column_labels.border.top.width */ border-top-color: #D3D3D3; /* column_labels.border.top.color */ border-bottom-style: solid; /* column_labels.border.bottom.style */ border-bottom-width: 2px; /* column_labels.border.bottom.width */ border-bottom-color: #D3D3D3; /* column_labels.border.bottom.color */ border-left-style: none; /* column_labels.border.lr.style */ border-left-width: 1px; /* column_labels.border.lr.width */ border-left-color: #D3D3D3; /* column_labels.border.lr.color */ border-right-style: none; /* column_labels.border.lr.style */ border-right-width: 1px; /* column_labels.border.lr.width */ border-right-color: #D3D3D3; /* column_labels.border.lr.color */ } #lvpgiypvtz .gt_col_heading { color: #333333; background-color: #FFFFFF; /* column_labels.background.color */ font-size: 100%; /* column_labels.font.size */ font-weight: normal; /* column_labels.font.weight */ text-transform: inherit; /* column_labels.text_transform */ vertical-align: middle; padding: 5px; margin: 10px; overflow-x: hidden; } #lvpgiypvtz .gt_sep_right { border-right: 5px solid #FFFFFF; } #lvpgiypvtz .gt_group_heading { padding: 8px; /* row_group.padding */ color: #333333; background-color: #FFFFFF; /* row_group.background.color */ font-size: 100%; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ text-transform: inherit; /* row_group.text_transform */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ border-left-style: none; /* row_group.border.left.style */ border-left-width: 1px; /* row_group.border.left.width */ border-left-color: #D3D3D3; /* row_group.border.left.color */ border-right-style: none; /* row_group.border.right.style */ border-right-width: 1px; /* row_group.border.right.width */ border-right-color: #D3D3D3; /* row_group.border.right.color */ vertical-align: middle; } #lvpgiypvtz .gt_empty_group_heading { padding: 0.5px; color: #333333; background-color: #FFFFFF; /* row_group.background.color */ font-size: 100%; /* row_group.font.size */ font-weight: initial; /* row_group.font.weight */ border-top-style: solid; /* row_group.border.top.style */ border-top-width: 2px; /* row_group.border.top.width */ border-top-color: #D3D3D3; /* row_group.border.top.color */ border-bottom-style: solid; /* row_group.border.bottom.style */ border-bottom-width: 2px; /* row_group.border.bottom.width */ border-bottom-color: #D3D3D3; /* row_group.border.bottom.color */ vertical-align: middle; } #lvpgiypvtz .gt_striped { background-color: rgba(128, 128, 128, 0.05); /* row.striping.background_color */ } #lvpgiypvtz .gt_from_md > :first-child { margin-top: 0; } #lvpgiypvtz .gt_from_md > :last-child { margin-bottom: 0; } #lvpgiypvtz .gt_row { padding-top: 8px; /* data_row.padding */ padding-bottom: 8px; /* data_row.padding */ padding-left: 5px; padding-right: 5px; margin: 10px; border-top-style: solid; /* table_body.hlines.style */ border-top-width: 1px; /* table_body.hlines.width */ border-top-color: #D3D3D3; /* table_body.hlines.color */ border-left-style: none; /* table_body.vlines.style */ border-left-width: 1px; /* table_body.vlines.width */ border-left-color: #D3D3D3; /* table_body.vlines.color */ border-right-style: none; /* table_body.vlines.style */ border-right-width: 1px; /* table_body.vlines.width */ border-right-color: #D3D3D3; /* table_body.vlines.color */ vertical-align: middle; overflow-x: hidden; } #lvpgiypvtz .gt_stub { color: #333333; background-color: #FFFFFF; /* stub.background.color */ font-weight: initial; /* stub.font.weight */ text-transform: inherit; /* stub.text_transform */ border-right-style: solid; /* stub.border.style */ border-right-width: 2px; /* stub.border.width */ border-right-color: #D3D3D3; /* stub.border.color */ padding-left: 12px; } #lvpgiypvtz .gt_summary_row { color: #333333; background-color: #FFFFFF; /* summary_row.background.color */ text-transform: inherit; /* summary_row.text_transform */ padding-top: 8px; /* summary_row.padding */ padding-bottom: 8px; /* summary_row.padding */ padding-left: 5px; padding-right: 5px; } #lvpgiypvtz .gt_first_summary_row { padding-top: 8px; /* summary_row.padding */ padding-bottom: 8px; /* summary_row.padding */ padding-left: 5px; padding-right: 5px; border-top-style: solid; /* summary_row.border.style */ border-top-width: 2px; /* summary_row.border.width */ border-top-color: #D3D3D3; /* summary_row.border.color */ } #lvpgiypvtz .gt_grand_summary_row { color: #333333; background-color: #FFFFFF; /* grand_summary_row.background.color */ text-transform: inherit; /* grand_summary_row.text_transform */ padding-top: 8px; /* grand_summary_row.padding */ padding-bottom: 8px; /* grand_summary_row.padding */ padding-left: 5px; padding-right: 5px; } #lvpgiypvtz .gt_first_grand_summary_row { padding-top: 8px; /* grand_summary_row.padding */ padding-bottom: 8px; /* grand_summary_row.padding */ padding-left: 5px; padding-right: 5px; border-top-style: double; /* grand_summary_row.border.style */ border-top-width: 6px; /* grand_summary_row.border.width */ border-top-color: #D3D3D3; /* grand_summary_row.border.color */ } #lvpgiypvtz .gt_table_body { border-top-style: solid; /* table_body.border.top.style */ border-top-width: 2px; /* table_body.border.top.width */ border-top-color: #D3D3D3; /* table_body.border.top.color */ border-bottom-style: solid; /* table_body.border.bottom.style */ border-bottom-width: 2px; /* table_body.border.bottom.width */ border-bottom-color: #D3D3D3; /* table_body.border.bottom.color */ } #lvpgiypvtz .gt_footnotes { color: #333333; background-color: #FFFFFF; /* footnotes.background.color */ border-bottom-style: none; /* footnotes.border.bottom.style */ border-bottom-width: 2px; /* footnotes.border.bottom.width */ border-bottom-color: #D3D3D3; /* footnotes.border.bottom.color */ border-left-style: none; /* footnotes.border.lr.color */ border-left-width: 2px; /* footnotes.border.lr.color */ border-left-color: #D3D3D3; /* footnotes.border.lr.color */ border-right-style: none; /* footnotes.border.lr.color */ border-right-width: 2px; /* footnotes.border.lr.color */ border-right-color: #D3D3D3; /* footnotes.border.lr.color */ } #lvpgiypvtz .gt_footnote { margin: 0px; font-size: 90%; /* footnotes.font.size */ padding: 4px; /* footnotes.padding */ } #lvpgiypvtz .gt_sourcenotes { color: #333333; background-color: #FFFFFF; /* source_notes.background.color */ border-bottom-style: none; /* source_notes.border.bottom.style */ border-bottom-width: 2px; /* source_notes.border.bottom.width */ border-bottom-color: #D3D3D3; /* source_notes.border.bottom.color */ border-left-style: none; /* source_notes.border.lr.style */ border-left-width: 2px; /* source_notes.border.lr.style */ border-left-color: #D3D3D3; /* source_notes.border.lr.style */ border-right-style: none; /* source_notes.border.lr.style */ border-right-width: 2px; /* source_notes.border.lr.style */ border-right-color: #D3D3D3; /* source_notes.border.lr.style */ } #lvpgiypvtz .gt_sourcenote { font-size: 90%; /* source_notes.font.size */ padding: 4px; /* source_notes.padding */ } #lvpgiypvtz .gt_left { text-align: left; } #lvpgiypvtz .gt_center { text-align: center; } #lvpgiypvtz .gt_right { text-align: right; font-variant-numeric: tabular-nums; } #lvpgiypvtz .gt_font_normal { font-weight: normal; } #lvpgiypvtz .gt_font_bold { font-weight: bold; } #lvpgiypvtz .gt_font_italic { font-style: italic; } #lvpgiypvtz .gt_super { font-size: 65%; } #lvpgiypvtz .gt_footnote_marks { font-style: italic; font-size: 65%; } Characteristic Fair, N = 16101 Good, N = 49061 Premium, N = 137911 p-value2 Price 4359 (3560) 3929 (3682) 4584 (4349) Carat 1.05 (0.52) 0.85 (0.45) 0.89 (0.52) 1 Statistics presented: mean (SD) 2 Statistical tests performed: One-way ANOVA correlation-Paket library(correlation) d %&gt;% select(price, carat, depth) %&gt;% correlation() ## Parameter1 | Parameter2 | r | 95% CI | t | df | p | Method | n_Obs ## -------------------------------------------------------------------------------------------- ## price | carat | 0.92 | [ 0.92, 0.92] | 551.41 | 53938 | &lt; .001 | Pearson | 53940 ## price | depth | -0.01 | [-0.02, 0.00] | -2.47 | 53938 | 0.013 | Pearson | 53940 ## carat | depth | 0.03 | [ 0.02, 0.04] | 6.56 | 53938 | &lt; .001 | Pearson | 53940 Automatische Berichte report-Paket Hier zu finden: https://github.com/easystats/report install.packages(&quot;devtools&quot;) devtools::install_github(&quot;easystats/report&quot;) library(report) Dataframes d &lt;- diamonds d %&gt;% select(price, carat, cut) %&gt;% report_sample() ## # Descriptive Statistics ## ## Characteristic | Summary ## ------------------------------------ ## Mean price (SD) | 3932.8 (3989.4) ## Mean carat (SD) | 0.8 (0.5) ## cut [Fair], % | 3.0 ## cut [Good], % | 9.1 ## cut [Very Good], % | 22.4 ## cut [Premium], % | 25.6 ## cut [Ideal], % | 40.0 report_sample(d, group_by = &quot;cut&quot;, select = c(&quot;price&quot;, &quot;carat&quot;)) ## # Descriptive Statistics ## ## Characteristic | Fair (n=1610) | Good (n=4906) | Very Good (n=12082) | Premium (n=13791) | Ideal (n=21551) | Total ## --------------------------------------------------------------------------------------------------------------------------------- ## Mean price (SD) | 4358.8 (3560.4) | 3928.9 (3681.6) | 3981.8 (3935.9) | 4584.3 (4349.2) | 3457.5 (3808.4) | 3932.8 (3989.4) ## Mean carat (SD) | 1.0 (0.5) | 0.8 (0.5) | 0.8 (0.5) | 0.9 (0.5) | 0.7 (0.4) | 0.8 (0.5) t-Test d %&gt;% filter(cut == &quot;Fair&quot; | cut == &quot;Ideal&quot;) %&gt;% droplevels() %&gt;% t.test(carat ~ cut, data = .) %&gt;% report() The Welch Two Sample t-test suggests that the difference of carat by cut (mean in group Fair = 1.05, mean in group Ideal = 0.70) is significant (difference = 0.34, 95% CI [0.32, 0.37], t(1781.99) = 26.00, p &lt; .001) and can be considered as very large (Cohen’s d = 1)., The Welch Two Sample t-test suggests that the difference of carat by cut (mean in group Fair = 1.05, mean in group Ideal = 0.70) is significant (difference = 0.34, 95% CI [0.32, 0.37], t(1781.99) = 26.00, p &lt; .001) and can be considered as very large (Cohen’s d = 1)., The Welch Two Sample t-test suggests that the difference of carat by cut (mean in group Fair = 1.05, mean in group Ideal = 0.70) is significant (difference = 0.34, 95% CI [0.32, 0.37], t(1781.99) = 26.00, p &lt; .001) and can be considered as very large (Cohen’s d = 1). and The Welch Two Sample t-test suggests that the difference of carat by cut (mean in group Fair = 1.05, mean in group Ideal = 0.70) is significant (difference = 0.34, 95% CI [0.32, 0.37], t(1781.99) = 26.00, p &lt; .001) and can be considered as very large (Cohen’s d = 1). ANOVA (base) stdaov &lt;- aov(price ~ color * cut, data = d) stdaov %&gt;% report() The ANOVA suggests that: The main effect of color is significant (F(6, 53905) = 294.07, p &lt; .001) and can be considered as small (partial omega squared = 0.03). The main effect of cut is significant (F(4, 53905) = 159.36, p &lt; .001) and can be considered as small (partial omega squared = 0.01). The interaction between color and cut is significant (F(24, 53905) = 4.53, p &lt; .001) and can be considered as very small (partial omega squared = 0.00). ANOVA (afex) library(afex) d$id &lt;- 1:length(d$carat) afexaov &lt;- aov_ez(&quot;id&quot;, dv = &quot;price&quot;, between = c(&quot;color&quot;, &quot;cut&quot;), data = d) afexaov$aov %&gt;% report() The ANOVA suggests that: The main effect of color is significant (F(6, 53905) = 294.07, p &lt; .001) and can be considered as small (partial omega squared = 0.03). The main effect of cut is significant (F(4, 53905) = 159.36, p &lt; .001) and can be considered as small (partial omega squared = 0.01). The interaction between color and cut is significant (F(24, 53905) = 4.53, p &lt; .001) and can be considered as very small (partial omega squared = 0.00). afexaov$aov %&gt;% report() %&gt;% table_short() ## Parameter | Sum_Squares | df | Mean_Square | F | p | Omega_Sq_partial ## -------------------------------------------------------------------------------- ## color | 2.68491e+10 | 6 | 4.47485e+09 | 294.07 | 0.00 | 0.03 ## cut | 9.69968e+09 | 4 | 2.42492e+09 | 159.36 | 0.00 | 0.01 ## color:cut | 1.65346e+09 | 24 | 6.88940e+07 | 4.53 | 0.00 | 0.00 ## Residuals | 8.20271e+11 | 53905 | 1.52170e+07 | | | LMM d %&gt;% filter(cut == &quot;Fair&quot; | cut == &quot;Ideal&quot;) %&gt;% droplevels() %&gt;% lmer(price ~ carat + cut + (1 | carat), data = .) %&gt;% report() We fitted a linear mixed model (estimated using REML and nloptwrap optimizer) to predict price with carat and cut (formula = price ~ carat + cut). The model included carat as random effects (formula = ~1 | carat). Standardized parameters were obtained by fitting the model on a standardized version of the dataset. Effect sizes were labelled following Funder’s (2019) recommendations.The model’s total explanatory power is substantial (conditional R2 = 0.87) and the part related to the fixed effects alone (marginal R2) is of 0.57. The model’s intercept, corresponding to price = 0, carat = 0 and cut = Fair, is at -1192.60 (SE = 273.06, 95% CI [-1727.79, -657.41], p &lt; .001). Within this model: The effect of carat is positive and can be considered as very large and significant (beta = 6346.99, SE = 165.26, 95% CI [6023.09, 6670.89], std. beta = 1.67, p &lt; .001). The effect of cut.L is positive and can be considered as small and significant (beta = 1129.24, SE = 26.68, 95% CI [1076.95, 1181.54], std. beta = 0.30, p &lt; .001). Bildverarbeitung (und -bearbeitung) in R magick https://cran.r-project.org/web/packages/magick/vignettes/intro.html # install.packages(&quot;magick&quot;) library(magick) Popularity Contest Die populärsten Pakete Hier geklaut: https://github.com/v-kozhevnikov Die populärsten Pakete Hier geklaut: https://github.com/v-kozhevnikov fin Anhang: Die wichtigsten Funktionen Allgemein Hier geklaut: http://www.sr.bham.ac.uk/~ajrs/R/r-function_list.html builtins() # List all built-in functions options() # Set options to control how R computes &amp; displays results ?NA # Help page on handling of missing data values abs(x) # The absolute value of &quot;x&quot; append() # Add elements to a vector c(x) # A generic function which combines its arguments cat(x) # Prints the arguments cbind() # Combine vectors by row/column (cf. &quot;paste&quot; in Unix) diff(x) # Returns suitably lagged and iterated differences gl() # Generate factors with the pattern of their levels grep() # Pattern matching identical() # Test if 2 objects are *exactly* equal jitter() # Add a small amount of noise to a numeric vector julian() # Return Julian date length(x) # Return no. of elements in vector x ls() # List objects in current environment mat.or.vec() # Create a matrix or vector paste(x) # Concatenate vectors after converting to character range(x) # Returns the minimum and maximum of x rep(1,5) # Repeat the number 1 five times rev(x) # List the elements of &quot;x&quot; in reverse order seq(1,10,0.4) # Generate a sequence (1 -&gt; 10, spaced by 0.4) sequence() # Create a vector of sequences sign(x) # Returns the signs of the elements of x sort(x) # Sort the vector x order(x) # list sorted element numbers of x tolower(),toupper() # Convert string to lower/upper case letters unique(x) # Remove duplicate entries from vector system(&quot;cmd&quot;) # Execute &quot;cmd&quot; in operating system (outside of R) vector() # Produces a vector of given length and mode formatC(x) # Format x using &#39;C&#39; style formatting specifications floor(x), ceiling(x), round(x), signif(x), trunc(x) # rounding functions Sys.getenv(x) # Get the value of the environment variable &quot;x&quot; Sys.putenv(x) # Set the value of the environment variable &quot;x&quot; Sys.time() # Return system time Sys.Date() # Return system date getwd() # Return working directory setwd() # Set working directory ?files # Help on low-level interface to file system list.files() # List files in a give directory file.info() # Get information about files # Built-in constants: pi,letters,LETTERS # Pi, lower &amp; uppercase letters, e.g. letters[7] = &quot;g&quot; month.abb,month.name # Abbreviated &amp; full names for months Mathe Hier geklaut: http://www.sr.bham.ac.uk/~ajrs/R/r-function_list.html log(x),logb(),log10(),log2(),exp(),expm1(),log1p(),sqrt() # Fairly obvious cos(),sin(),tan(),acos(),asin(),atan(),atan2() # Usual stuff cosh(),sinh(),tanh(),acosh(),asinh(),atanh() # Hyperbolic functions union(),intersect(),setdiff(),setequal() # Set operations +,-,*,/,^,%%,%/% # Arithmetic operators &lt;,&gt;,&lt;=,&gt;=,==,!= # Comparison operators eigen() # Computes eigenvalues and eigenvectors deriv() # Symbolic and algorithmic derivatives of simple expressions integrate() # Adaptive quadrature over a finite or infinite interval. sqrt(),sum() ?Control # Help on control flow statements (e.g. if, for, while) ?Extract # Help on operators acting to extract or replace subsets of vectors ?Logic # Help on logical operators ?Mod # Help on functions which support complex arithmetic in R ?Paren # Help on parentheses ?regex # Help on regular expressions used in R ?Syntax # Help on R syntax and giving the precedence of operators ?Special # Help on special functions related to beta and gamma functions Statistik Hier geklaut: http://www.sr.bham.ac.uk/~ajrs/R/r-function_list.html help(package=stats) # List all stats functions ?Chisquare # Help on chi-squared distribution functions ?Poisson # Help on Poisson distribution functions help(package=survival) # Survival analysis cor.test() # Perform correlation test cumsum(); cumprod(); cummin(); cummax() # Cumuluative functions for vectors density(x) # Compute kernel density estimates ks.test() # Performs one or two sample Kolmogorov-Smirnov tests loess(), lowess() # Scatter plot smoothing mad() # Calculate median absolute deviation mean(x), weighted.mean(x), median(x), min(x), max(x), quantile(x) rnorm(), runif() # Generate random data with Gaussian/uniform distribution splinefun() # Perform spline interpolation smooth.spline() # Fits a cubic smoothing spline sd() # Calculate standard deviation summary(x) # Returns a summary of x: mean, min, max etc. t.test() # Student&#39;s t-test var() # Calculate variance sample() # Random samples &amp; permutations ecdf() # Empirical Cumulative Distribution Function qqplot() # quantile-quantile plot "]
]
